# Application SDK Common and Decorators Documentation

This document provides comprehensive documentation for the Application SDK's common utilities and decorators.

## Table of Contents
1. [Common Module](#common-module)
   - [Constants](#constants)
   - [AWS Utilities](#aws-utilities)
   - [Logger Adaptors](#logger-adaptors)
   - [General Utilities](#general-utilities)
2. [Decorators Module](#decorators-module)
   - [Core Decorators](#core-decorators)
   - [Helper Functions](#helper-functions)

## Common Module

### Constants

#### ApplicationConstants
Application-wide constants.

**Values:**
- `APPLICATION_NAME`: Application name (default: "default")
- `TENANT_ID`: Tenant ID (default: "default")

### AWS Utilities

#### Functions
- `get_region_name_from_hostname(hostname: str) -> str`: Extract AWS region from RDS endpoint
- `generate_aws_rds_token_with_iam_role(role_arn: str, host: str, user: str, ...) -> str`: Generate RDS auth token using IAM role
- `generate_aws_rds_token_with_iam_user(aws_access_key_id: str, aws_secret_access_key: str, ...) -> str`: Generate RDS auth token using IAM user

### Logger Adaptors

#### AtlanLoggerAdapter
Enhanced logger adapter with OpenTelemetry integration.

**Features:**
- Request context tracking
- Workflow and activity context logging
- OpenTelemetry integration
- Structured logging
- Log level management

**Methods:**
- `__init__(logger_name: str)`: Initialize logger with configuration
- `process(msg: Any, kwargs: Dict[str, Any]) -> Tuple[Any, Dict[str, Any]]`: Process log message
- `debug(msg: str, *args, **kwargs)`: Log debug message
- `info(msg: str, *args, **kwargs)`: Log info message
- `warning(msg: str, *args, **kwargs)`: Log warning message
- `error(msg: str, *args, **kwargs)`: Log error message
- `critical(msg: str, *args, **kwargs)`: Log critical message

**Configuration:**
- Environment variables for OpenTelemetry setup
- Log level configuration
- Resource attributes
- Batch processing settings

### General Utilities

#### Functions
- `prepare_query(query: str, workflow_args: Dict[str, Any], temp_table_regex_sql: str = "") -> Optional[str]`: Prepare SQL query with filters
- `prepare_filters(include_filter_str: str, exclude_filter_str: str) -> Tuple[str, str]`: Prepare SQL filters
- `normalize_filters(filter_dict: Dict[str, List[str] | str], is_include: bool) -> List[str]`: Normalize SQL filters
- `get_workflow_config(config_id: str) -> Dict[str, Any]`: Get workflow configuration
- `update_workflow_config(config_id: str, config: Dict[str, Any]) -> Dict[str, Any]`: Update workflow configuration

## Decorators Module

### Core Decorators

#### @transform
Decorator for data transformation with input/output handling.

**Features:**
- Batch processing support
- Input/Output class integration
- State management
- Error handling

**Usage:**
```python
@transform(
    batch_input=SQLQueryInput(query="SELECT * FROM table"),
    output=JsonOutput(output_path="/path/to/output")
)
async def process_data(batch_input: pd.DataFrame, output: JsonOutput, **kwargs):
    await output.write_batched_dataframe(batch_input)
```

#### @transform_daft
Decorator for Daft DataFrame transformations.

**Features:**
- Daft DataFrame support
- Batch processing
- Input/Output integration

#### @run_sync
Decorator for running synchronous functions in thread pool.

**Usage:**
```python
@run_sync
def heavy_computation():
    # Synchronous code here
    pass
```

### Helper Functions

#### Core Functions
- `to_async(func: Callable[..., Any], *args, **kwargs) -> Iterator[Union[pd.DataFrame, "daft.DataFrame"]]`: Convert sync to async
- `_get_dataframe(input_obj: Input, get_dataframe_fn: Callable[..., Any]) -> Union[pd.DataFrame, "daft.DataFrame"]`: Get DataFrame from input
- `prepare_fn_kwargs(self: Any, state: Optional[ActivitiesState], ...) -> Dict[str, Any]`: Prepare function arguments
- `run_process(fn: Callable[..., Any], ...) -> Any`: Process input data through function

## Usage Examples

### Logger Usage
```python
logger = get_logger(__name__)
logger.info("Processing data", extra={"data_id": "123"})
```

### Query Preparation
```python
query = prepare_query(
    "SELECT * FROM table WHERE name ~ '{normalized_include_regex}'",
    workflow_args={
        "metadata": {
            "include-filter": '{"db1": ["schema1"]}',
            "exclude-filter": '{"db1": ["temp"]}'
        }
    }
)
```

### Transform Decorator
```python
@transform(
    batch_input=SQLQueryInput(query="SELECT * FROM users"),
    output=JsonOutput(output_path="/data/users")
)
async def process_users(batch_input: pd.DataFrame, output: JsonOutput):
    processed_data = batch_input.apply(process_user)
    await output.write_batched_dataframe(processed_data)
```

## Error Handling

### Logger
- OpenTelemetry integration errors
- Context management errors
- Log processing errors

### Utilities
- Query preparation errors
- Filter normalization errors
- Configuration management errors

### Decorators
- Input/Output processing errors
- State management errors
- Batch processing errors

## Best Practices

1. Use appropriate log levels
2. Implement proper error handling
3. Use batch processing for large datasets
4. Configure OpenTelemetry properly
5. Use appropriate filter patterns
6. Handle state management correctly
7. Implement proper resource cleanup
8. Use type hints consistently 