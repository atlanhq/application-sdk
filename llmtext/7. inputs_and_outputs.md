# Inputs and Outputs Documentation

This document provides a comprehensive overview of the Application SDK's input and output modules.

## Base Classes and Utilities

### Base Classes
```python
class Input(ABC):
    """Abstract base class for input data sources.
    
    This class defines the interface for input handlers that can read data
    from various sources in different formats.
    
    Methods:
        re_init: Re-initialize the input class with given keyword arguments
        get_batched_dataframe: Get an iterator of batched pandas DataFrames
        get_dataframe: Get a single pandas DataFrame
        get_batched_daft_dataframe: Get an iterator of batched daft DataFrames
        get_daft_dataframe: Get a single daft DataFrame
    """
    
    @classmethod
    def re_init(cls, **kwargs: Dict[str, Any]):
        """Re-initialize the input class with given keyword arguments.
        
        Args:
            **kwargs: Keyword arguments for re-initialization
            
        Returns:
            Input: An instance of the input class
        """
        return cls(**kwargs)
        
    @abstractmethod
    def get_batched_dataframe(self) -> Iterator[pd.DataFrame]:
        """Get an iterator of batched pandas DataFrames."""
        
    @abstractmethod
    def get_dataframe(self) -> pd.DataFrame:
        """Get a single pandas DataFrame."""
        
    @abstractmethod
    def get_batched_daft_dataframe(self) -> Iterator["daft.DataFrame"]:
        """Get an iterator of batched daft DataFrames."""
        
    @abstractmethod
    def get_daft_dataframe(self) -> "daft.DataFrame":
        """Get a single daft DataFrame."""


class Output(ABC):
    """Abstract base class for output handlers.
    
    This class defines the interface for output handlers that can write data
    to various destinations in different formats.
    
    Attributes:
        output_path (str): Path where the output will be written
        output_prefix (str): Prefix for files when uploading to object store
        total_record_count (int): Total number of records processed
        chunk_count (int): Number of chunks the output was split into
        state (Optional[ActivitiesState]): Current state of the activity
        
    Methods:
        re_init: Re-initialize the output class with given keyword arguments
        write_batched_dataframe: Write batched pandas DataFrames
        write_dataframe: Write a single pandas DataFrame
        write_batched_daft_dataframe: Write batched daft DataFrames
        write_daft_dataframe: Write a single daft DataFrame
        get_statistics: Get statistics about the output operation
        write_statistics: Write statistics to a JSON file
    """
    
    @classmethod
    def re_init(cls, **kwargs: Dict[str, Any]):
        """Re-initialize the output class with given keyword arguments.
        
        Args:
            **kwargs: Keyword arguments for re-initialization
            
        Returns:
            Output: An instance of the output class
        """
        return cls(**kwargs)
        
    async def write_batched_dataframe(
        self,
        batched_dataframe: Union[AsyncGenerator, Generator]
    ):
        """Write batched pandas DataFrames to output."""
        
    @abstractmethod
    async def write_dataframe(self, dataframe: pd.DataFrame):
        """Write a single pandas DataFrame to output."""
        
    async def write_batched_daft_dataframe(
        self,
        batched_dataframe: Union[AsyncGenerator, Generator]
    ):
        """Write batched daft DataFrames to output."""
        
    @abstractmethod
    async def write_daft_dataframe(self, dataframe: "daft.DataFrame"):
        """Write a single daft DataFrame to output."""
        
    async def get_statistics(self, typename: Optional[str] = None) -> ActivityStatistics:
        """Get statistics about the output operation."""
```

### Utility Functions
```python
def is_empty_dataframe(dataframe: Union[pd.DataFrame, "daft.DataFrame"]) -> bool:
    """Helper method to check if the dataframe has any rows.
    
    Args:
        dataframe: A pandas or daft DataFrame
        
    Returns:
        bool: True if the DataFrame is empty, False otherwise
        
    Note:
        For daft DataFrames, this requires the daft module to be installed.
        If daft is not found, the function returns True.
    """
```

## Input Types

### SQL Query Input (`sql_query.py`)
```python
class SQLQueryInput(Input):
    """Input handler for SQL queries with support for:
    - Asynchronous query execution
    - SQLAlchemy engine and connection string support
    - Temporary table handling
    - Chunked data reading
    - ConnectorX integration for supported databases
    """
    
    def __init__(
        self,
        query: str,
        engine: Optional[Union[Engine, str]] = None,
        chunk_size: Optional[int] = 100000,
        temp_table_sql_query: Optional[str] = None,
        **kwargs: Dict[str, Any],
    ):
        """Initialize with SQL query and connection details."""
```

### JSON Input (`json.py`)
```python
class JsonInput(Input):
    """Input handler for JSON files with support for:
    - Multiple file reading
    - Object store integration
    - Chunked data reading
    - Line-delimited JSON support
    """
    
    def __init__(
        self,
        path: str,
        file_names: Optional[List[str]] = None,
        download_file_prefix: Optional[str] = None,
        chunk_size: Optional[int] = None,
        **kwargs: Dict[str, Any],
    ):
        """Initialize with file path and reading configuration."""
```

### Parquet Input (`parquet.py`)
```python
class ParquetInput(Input):
    """Input handler for Parquet files with support for:
    - Single file and directory reading
    - Efficient columnar storage reading
    - Object store integration
    - Native daft chunking support
    """
    
    def __init__(
        self,
        file_path: str,
        chunk_size: Optional[int] = 100000,
        input_prefix: Optional[str] = None,
        **kwargs: Dict[str, Any],
    ):
        """Initialize with file path and reading configuration."""
```

### Iceberg Input (`iceberg.py`)
```python
class IcebergInput(Input):
    """Input handler for Iceberg tables with support for:
    - Table versioning
    - Partition pruning
    - Schema evolution
    - Native daft integration
    """
    
    def __init__(
        self, table: Table, chunk_size: Optional[int] = 100000, **kwargs: Dict[str, Any]
    ):
        """Initialize with Iceberg table and reading configuration."""
```

### Object Store Input (`objectstore.py`)
```python
class ObjectStoreInput:
    """Utility class for object store operations with support for:
    - Dapr bindings integration
    - File downloading
    - Path handling
    - Error management
    """
    
    OBJECT_STORE_NAME = os.getenv("OBJECT_STORE_NAME", "objectstore")
    OBJECT_GET_OPERATION = "get"
```

### State Store Input (`statestore.py`)
```python
class StateStoreInput:
    """State store interface for the application.
    
    This class provides functionality to read state data using Dapr state store bindings.
    It supports retrieving state data and configurations with proper error handling.
    
    Class Attributes:
        STATE_STORE_NAME (str): Name of the state store binding, defaults to "statestore"
    """
    
    STATE_STORE_NAME = os.getenv("STATE_STORE_NAME", "statestore")
    
    @classmethod
    def get_state(cls, key: str) -> Dict[str, Any]:
        """Get state from the store.
        
        Args:
            key: The key to retrieve the state for
            
        Returns:
            Dict[str, Any]: The retrieved state data
            
        Raises:
            ValueError: If no state is found for the given key
            Exception: If there's an error with the Dapr client operations
        """
    
    @classmethod
    def extract_configuration(cls, config_id: str) -> Dict[str, Any]:
        """Extract configuration from the state store using the config ID.
        
        Args:
            config_id: The unique identifier for the configuration
            
        Returns:
            Dict[str, Any]: The configuration if found
            
        Raises:
            ValueError: If the config_id is invalid or configuration is not found
            Exception: If there's an error with the Dapr client operations
        """
```

### Secret Store Input (`secretstore.py`)
```python
class SecretStoreInput:
    """Secret store interface for the application.
    
    This class provides functionality to retrieve credentials and secrets using
    Dapr secret store bindings. It integrates with the state store for credential
    management.
    
    Class Attributes:
        SECRET_STORE_NAME (str): Name of the secret store binding, defaults to "secretstore"
    """
    
    SECRET_STORE_NAME = os.getenv("SECRET_STORE_NAME", "secretstore")
    
    @classmethod
    def extract_credentials(cls, credential_guid: str) -> Dict[str, Any]:
        """Extract credentials from the state store using the credential GUID.
        
        Args:
            credential_guid: The unique identifier for the credentials
            
        Returns:
            Dict[str, Any]: The credentials if found
            
        Raises:
            ValueError: If the credential_guid is invalid or credentials are not found
            Exception: If there's an error with the Dapr client operations
            
        Examples:
            >>> SecretStoreInput.extract_credentials("1234567890")
            {"username": "admin", "password": "password"}
        """
```

## Output Types

### JSON Output (`json.py`)
```python
class JsonOutput(Output):
    """Output handler for JSON files with support for:
    - Chunked writing
    - Buffer management
    - Object store integration
    - Custom path generation
    """
    
    def __init__(
        self,
        output_suffix: str,
        output_path: Optional[str] = None,
        output_prefix: Optional[str] = None,
        typename: Optional[str] = None,
        state: Optional[ActivitiesState] = None,
        chunk_start: Optional[int] = None,
        buffer_size: int = 1024 * 1024 * 10,
        chunk_size: Optional[int] = None,
        total_record_count: int = 0,
        chunk_count: int = 0,
        path_gen: Callable[[int | None, int], str] = path_gen,
        **kwargs: Dict[str, Any],
    ):
        """Initialize with output configuration and buffer settings."""
```

### Parquet Output (`parquet.py`)
```python
class ParquetOutput(Output):
    """Output handler for Parquet files with support for:
    - Efficient columnar storage
    - Snappy compression
    - Object store integration
    - Chunked writing
    """
    
    def __init__(
        self,
        output_path: str,
        output_suffix: str,
        output_prefix: str,
        typename: Optional[str] = None,
        mode: str = "append",
        chunk_size: int = 100000,
        total_record_count: int = 0,
        chunk_count: int = 0,
        state: Optional[ActivitiesState] = None,
        **kwargs: Dict[str, Any],
    ):
        """Initialize with output configuration and write settings."""
```

### Iceberg Output (`iceberg.py`)
```python
class IcebergOutput(Output):
    """Output handler for Iceberg tables with support for:
    - Table versioning
    - Schema evolution
    - Transaction management
    - Native daft integration
    """
    
    def __init__(
        self,
        iceberg_catalog: Catalog,
        iceberg_namespace: str,
        iceberg_table: Union[str, Table],
        mode: str = "append",
        total_record_count: int = 0,
        chunk_count: int = 0,
        **kwargs: Dict[str, Any],
    ):
        """Initialize with Iceberg catalog and table configuration."""
```

### Object Store Output (`objectstore.py`)
```python
class ObjectStoreOutput:
    """Utility class for object store operations with support for:
    - Dapr bindings integration
    - File uploading
    - Directory uploading
    - Path handling
    - Error management
    """
    
    OBJECT_STORE_NAME = os.getenv("OBJECT_STORE_NAME", "objectstore")
    OBJECT_CREATE_OPERATION = "create"
```

### Event Store Output (`eventstore.py`)
```python
class EventStore:
    """Event store for publishing application events.
    
    This class provides functionality to publish events to a pub/sub system using Dapr.
    It supports various event types including workflow, activity, and custom events.
    
    Class Attributes:
        EVENT_STORE_NAME (str): Name of the event store binding, defaults to "eventstore"
        TOPIC_NAME (str): Default topic name for events, defaults to "app_events"
    """
    
    EVENT_STORE_NAME = "eventstore"
    TOPIC_NAME = "app_events"
    
    @classmethod
    def create_event(cls, event: Event, topic_name: str = TOPIC_NAME):
        """Create and publish a new event.
        
        Args:
            event (Event): Event data to publish
            topic_name (str, optional): Topic name to publish to. Defaults to TOPIC_NAME.
            
        Example:
            >>> event = WorkflowStartEvent(
            ...     workflow_name="test_workflow",
            ...     workflow_id="123",
            ...     workflow_run_id="run_1"
            ... )
            >>> EventStore.create_event(event)
        """
```

### Event Types
```python
class Event(BaseModel):
    """Base class for all events.
    
    Attributes:
        event_type (str): Type of the event
    """
    
    event_type: str = Field(init=False)

class ActivityStartEvent(Event):
    """Event emitted when an activity starts.
    
    Attributes:
        event_type (str): Always set to ACTIVITY_START_EVENT
        activity_type (str | None): Type of the activity
        activity_id (str | None): Unique identifier for the activity
    """

class ActivityEndEvent(Event):
    """Event emitted when an activity ends.
    
    Attributes:
        event_type (str): Always set to ACTIVITY_END_EVENT
        activity_type (str | None): Type of the activity
        activity_id (str | None): Unique identifier for the activity
    """

class WorkflowStartEvent(Event):
    """Event emitted when a workflow starts.
    
    Attributes:
        event_type (str): Always set to WORKFLOW_START_EVENT
        workflow_name (str | None): Name of the workflow
        workflow_id (str | None): Unique identifier for the workflow
        workflow_run_id (str | None): Run identifier for the workflow
    """

class WorkflowEndEvent(Event):
    """Event emitted when a workflow ends.
    
    Attributes:
        event_type (str): Always set to WORKFLOW_END_EVENT
        workflow_name (str | None): Name of the workflow
        workflow_id (str | None): Unique identifier for the workflow
        workflow_run_id (str | None): Run identifier for the workflow
        workflow_output (Dict[str, Any]): Output data from the workflow
    """

class CustomEvent(Event):
    """Custom event for application-specific events.
    
    Attributes:
        event_type (str): Always set to CUSTOM_EVENT
        data (Dict[str, Any]): Custom event data
    """
```

## Common Features and Best Practices

### Common Features
1. DataFrame Support
   - Both pandas and daft DataFrame formats
   - Chunked operations for memory efficiency
   - Type hints and validation

2. Storage Integration
   - Object store support
   - Path handling
   - Error management

3. Performance
   - Async/await support
   - Buffer management
   - Configurable chunk sizes

4. Monitoring
   - Progress tracking
   - Statistics collection
   - Error logging

### Best Practices
1. Memory Management
   - Use appropriate chunk sizes
   - Monitor buffer sizes
   - Implement proper cleanup

2. Error Handling
   - Graceful error handling
   - Proper logging
   - Error recovery mechanisms

3. Code Quality
   - Type hints
   - Consistent naming
   - Comprehensive documentation
   - Appropriate file formats

4. Performance
   - Use async/await
   - Implement chunking
   - Monitor resource usage