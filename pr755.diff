diff --git a/.cursor/rules/documentation.mdc b/.cursor/rules/documentation.mdc
index ebdc53fa9..d9f477b32 100644
--- a/.cursor/rules/documentation.mdc
+++ b/.cursor/rules/documentation.mdc
@@ -1,25 +1,20 @@
 ---
-description:
+description: Update conceptual documentation when SDK code changes to keep docs synchronized with implementation
 globs: application_sdk/**/*.py
 alwaysApply: false
 ---
-Update conceptual documentation when SDK code changes.
-  A file within the Application SDK has been modified: `{trigger_file_path}`.
+# Documentation Updates

-  **Action Required:** Please review the changes in this file and update the corresponding conceptual documentation under `docs/docs/concepts/` to ensure it accurately reflects the current code. Keeping documentation synchronized with the code is crucial for maintainability.
+When modifying SDK code, update corresponding conceptual documentation under `docs/docs/concepts/` to reflect changes.

-  **Module to Concept Doc Mapping:**
-  *   `application_sdk/application/**` -> `docs/docs/concepts/application.md` (Covers both simple and specialized workflow applications. The `BaseApplication` class is the generic entry point for all workflow-driven applications.)
-  *   `application_sdk/server/**` -> `docs/docs/concepts/server.md` (Covers server abstractions, FastAPI integration, and API endpoint patterns.)
-  *   `application_sdk/handlers/**` -> `docs/docs/concepts/handlers.md` (Covers handler interfaces and custom handler logic.)
-  *   `application_sdk/clients/**` -> `docs/docs/concepts/clients.md` (Covers client interfaces, SQL clients, and connection management.)
-  *   `application_sdk/activities/**` -> `docs/docs/concepts/activities.md` (Covers activity interfaces and activity patterns.)
-  *   `application_sdk/common/**` or `application_sdk/constants.py` -> `docs/docs/concepts/common.md` (Covers shared utilities, logging, and constants.)
-  *   `application_sdk/inputs/**` -> `docs/docs/concepts/inputs.md` (Covers input abstractions and input handling.)
-  *   `application_sdk/outputs/**` -> `docs/docs/concepts/outputs.md` (Covers output abstractions and output handling.)
-  *   `application_sdk/transformers/**` -> `docs/docs/concepts/transformers.md` (Covers transformer interfaces and entity mapping.)
-  *   `application_sdk/workflows/**` -> `docs/docs/concepts/workflows.md` (Covers workflow interfaces, workflow patterns, and orchestration.)
-  *   `application_sdk/worker.py` -> `docs/docs/concepts/worker.md` (Covers worker orchestration and Temporal worker management.)
-  *   `docs/docs/concepts/application_sql.md` -> Specialized SQL workflow application patterns and examples.
-
-  *(Note: Carefully examine the code changes to determine the exact impact and which documentation file(s) require updates. For application changes, review both simple and specialized application patterns.)*
\ No newline at end of file
+**Module to Concept Doc Mapping:**
+*   `application_sdk/application/**` -> `docs/docs/concepts/application.md`
+*   `application_sdk/server/**` -> `docs/docs/concepts/server.md`
+*   `application_sdk/handlers/**` -> `docs/docs/concepts/handlers.md`
+*   `application_sdk/clients/**` -> `docs/docs/concepts/clients.md`
+*   `application_sdk/activities/**` -> `docs/docs/concepts/activities.md`
+*   `application_sdk/common/**` or `application_sdk/constants.py` -> `docs/docs/concepts/common.md`
+*   `application_sdk/io/**` -> `docs/docs/concepts/inputs.md` (for readers) AND `docs/docs/concepts/outputs.md` (for writers)
+*   `application_sdk/transformers/**` -> `docs/docs/concepts/transformers.md`
+*   `application_sdk/workflows/**` -> `docs/docs/concepts/workflows.md`
+*   `application_sdk/worker.py` -> `docs/docs/concepts/worker.md`
\ No newline at end of file
diff --git a/.cursor/rules/exception-handling.mdc b/.cursor/rules/exception-handling.mdc
index 44e03d5e7..5832aa47b 100644
--- a/.cursor/rules/exception-handling.mdc
+++ b/.cursor/rules/exception-handling.mdc
@@ -146,6 +146,44 @@ except Exception as e:
     # Don't re-raise for non-critical operations
 ```

+## Valid Exception Swallowing in This Codebase
+
+The following patterns are acceptable and used in the codebase:
+
+### **✅ VALID: Signal Handlers and Cleanup Operations**
+```python
+# From observability.py - signal handler cleanup
+except Exception as e:
+    logging.error(f"Error during signal handler flush: {e}")
+    # Don't re-raise - cleanup failures shouldn't crash the application
+```
+
+### **✅ VALID: Observability Operations (Metrics, Traces, Events)**
+```python
+# From observability_decorator.py - trace recording
+try:
+    traces.record_trace(...)
+except Exception as trace_error:
+    logger.error(f"Failed to record trace: {str(trace_error)}")
+    # Don't re-raise - observability failures shouldn't break business logic
+
+# From interceptors/events.py - event publishing
+except Exception as publish_error:
+    logger.warning(f"Failed to publish workflow end event: {publish_error}")
+    # Don't re-raise - event publishing is non-critical
+```
+
+### **✅ VALID: Cleanup in Finally Blocks**
+```python
+# From interceptors/cleanup.py - workflow cleanup
+finally:
+    try:
+        await workflow.execute_activity(cleanup, ...)
+    except Exception as e:
+        logger.warning(f"Failed to cleanup artifacts: {e}")
+        # Don't re-raise - cleanup failures shouldn't fail the workflow
+```
+
 ## Module-Specific Guidelines

 ### **Handlers (`application_sdk/handlers/`)**
@@ -153,15 +191,11 @@ except Exception as e:
 - **Include operation context** in error messages
 - **Use specific exception types** for different error scenarios

-### **Outputs (`application_sdk/outputs/`)**
-- **Critical**: Re-raise exceptions for data writing operations
-- **Non-critical**: May swallow exceptions for metrics/logging operations
-- **Resource cleanup**: Ensure files/connections are properly closed
-
-### **Inputs (`application_sdk/inputs/`)**
-- **Critical**: Re-raise exceptions for data reading operations
+### **I/O Module (`application_sdk/io/`)**
+- **Critical**: Re-raise exceptions for data reading/writing operations
 - **Include file/connection context** in error messages
 - **Handle specific I/O exceptions** appropriately
+- **Resource cleanup**: Ensure files/connections are properly closed

 ### **Observability (`application_sdk/observability/`)**
 - **Non-critical**: May swallow exceptions to prevent cascading failures
diff --git a/.cursor/rules/guidelines.mdc b/.cursor/rules/guidelines.mdc
index 15b784f89..47748dff4 100644
--- a/.cursor/rules/guidelines.mdc
+++ b/.cursor/rules/guidelines.mdc
@@ -1,5 +1,5 @@
 ---
-description:
+description: Meta-guidelines for writing and maintaining cursor rule files with proper structure and formatting
 globs: .cursor/rules/*.mdc
 alwaysApply: false
 ---
diff --git a/.cursor/rules/logging.mdc b/.cursor/rules/logging.mdc
index 606809311..3d54e9e1b 100644
--- a/.cursor/rules/logging.mdc
+++ b/.cursor/rules/logging.mdc
@@ -1,10 +1,14 @@
 ---
-description:
+description: Logging standards and best practices using AtlanLoggerAdapter for structured logging
 globs: application_sdk/**/*.py
 alwaysApply: false
 ---
 # Logging Guidelines

+## Exceptions
+
+- **Observability Module**: The `application_sdk/observability/` module uses the standard `logging` module directly since it implements the `get_logger` wrapper. All other modules must use `get_logger`.
+
 - **Logger Configuration**
     - Use `AtlanLoggerAdapter` for all logging
     - Configure loggers using `get_logger(__name__)`
diff --git a/.cursor/rules/performance.mdc b/.cursor/rules/performance.mdc
index 531aec1db..55e299356 100644
--- a/.cursor/rules/performance.mdc
+++ b/.cursor/rules/performance.mdc
@@ -186,19 +186,25 @@ for query in queries:
 #### **JSON Serialization Optimization**
 - **Rule**: Use efficient serialization libraries and avoid unnecessary conversions
 - **Anti-pattern**: Using default json module for large datasets
-- **Correct pattern**: Use orjson, ujson, or custom serializers
+- **Correct pattern**: Use orjson (already used throughout this codebase)

-**✅ DO: Efficient JSON serialization**
+**✅ DO: Efficient JSON serialization (from objectstore.py)**
 ```python
 import orjson  # Much faster than json

-# Use orjson for better performance
-data = orjson.dumps(large_dict, option=orjson.OPT_APPEND_NEWLINE)
+# Fast JSON parsing from object store response
+file_list = orjson.loads(response_data.decode("utf-8"))

-# Use streaming for large datasets
-def serialize_large_dataset(data_iterator):
-    for item in data_iterator:
-        yield orjson.dumps(item) + b'\n'
+# Efficient JSON encoding
+data = json.dumps({"prefix": prefix}).encode("utf-8") if prefix else ""
+```
+
+**✅ DO: orjson in I/O operations (from io/json.py)**
+```python
+# Fast DataFrame to JSON conversion
+for chunk in df.to_pandas(batch_size=chunk_size):
+    for record in chunk.to_dict(orient="records"):
+        json_line = orjson.dumps(record).decode("utf-8")
 ```

 **❌ DON'T: Inefficient serialization**
@@ -206,9 +212,6 @@ def serialize_large_dataset(data_iterator):
 # REJECT: Using default json for large datasets
 import json
 data = json.dumps(large_dict)  # Slower than orjson
-
-# REJECT: Converting to string unnecessarily
-data = str(orjson.dumps(large_dict))  # Unnecessary conversion
 ```


diff --git a/.cursor/rules/standards.mdc b/.cursor/rules/standards.mdc
index 7071a9c20..ceed6a7ba 100644
--- a/.cursor/rules/standards.mdc
+++ b/.cursor/rules/standards.mdc
@@ -1,6 +1,6 @@
 ---
-description:
-globs:
+description: Core development standards including code formatting, documentation, testing, logging, exception handling, and performance
+globs: application_sdk/**/*.py
 alwaysApply: true
 ---
 # Development Standards
@@ -58,6 +58,12 @@ This document outlines the development standards for the Application SDK, includ
 - **Logging Standards**
     - logging standards are defined in [logging.mdc](mdc:.cursor/rules/logging.mdc)

+- **Exception Handling**
+    - exception handling standards are defined in [exception-handling.mdc](mdc:.cursor/rules/exception-handling.mdc)
+
+- **Performance Standards**
+    - performance standards are defined in [performance.mdc](mdc:.cursor/rules/performance.mdc)
+
 ## Security, Performance, and Resource Management

 - **Security Considerations**
diff --git a/application_sdk/activities/common/sql_utils.py b/application_sdk/activities/common/sql_utils.py
new file mode 100644
index 000000000..c8fedc1d2
--- /dev/null
+++ b/application_sdk/activities/common/sql_utils.py
@@ -0,0 +1,310 @@
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    AsyncIterator,
+    Callable,
+    Dict,
+    Iterator,
+    List,
+    Optional,
+    Tuple,
+    Union,
+)
+
+from application_sdk.activities.common.models import ActivityStatistics
+from application_sdk.clients.sql import BaseSQLClient
+from application_sdk.common.utils import (
+    get_database_names,
+    parse_credentials_extra,
+    prepare_query,
+)
+from application_sdk.io.parquet import ParquetFileWriter
+from application_sdk.observability.logger_adaptor import get_logger
+
+if TYPE_CHECKING:
+    import pandas as pd
+
+logger = get_logger(__name__)
+
+
+async def setup_database_connection(
+    sql_client: BaseSQLClient,
+    database_name: str,
+) -> None:
+    """Setup connection for a specific database.
+
+    Args:
+        sql_client: The SQL client to configure.
+        database_name: The name of the database to connect to.
+    """
+    extra = parse_credentials_extra(sql_client.credentials)
+    extra["database"] = database_name
+    sql_client.credentials["extra"] = extra
+    await sql_client.load(sql_client.credentials)
+
+
+def prepare_database_query(
+    sql_query: str,
+    database_name: Optional[str],
+    workflow_args: Dict[str, Any],
+    temp_table_regex_sql: str = "",
+    use_posix_regex: bool = False,
+) -> str:
+    """Prepare query for database execution with proper substitutions.
+
+    Args:
+        sql_query: The raw SQL query string.
+        database_name: The database name to substitute into the query.
+        workflow_args: Workflow arguments for query preparation.
+        temp_table_regex_sql: SQL regex for temp table exclusion.
+        use_posix_regex: Whether to use POSIX regex syntax.
+
+    Returns:
+        The prepared SQL query string.
+
+    Raises:
+        ValueError: If query preparation fails.
+    """
+    # Replace database name placeholder if provided
+    fetch_sql = sql_query
+    if database_name:
+        fetch_sql = fetch_sql.replace("{database_name}", database_name)
+
+    # Prepare the query
+    prepared_query = prepare_query(
+        query=fetch_sql,
+        workflow_args=workflow_args,
+        temp_table_regex_sql=temp_table_regex_sql,
+        use_posix_regex=use_posix_regex,
+    )
+
+    if prepared_query is None:
+        db_context = f" for database {database_name}" if database_name else ""
+        raise ValueError(f"Failed to prepare query{db_context}")
+
+    return prepared_query
+
+
+async def execute_single_db(
+    sql_client: BaseSQLClient,
+    prepared_query: Optional[str],
+    parquet_output: Optional[ParquetFileWriter],
+    write_to_file: bool,
+) -> Tuple[
+    bool, Optional[Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]]
+]:
+    """Execute a query against a single database.
+
+    Args:
+        sql_client: The SQL client to use.
+        prepared_query: The prepared SQL query to execute.
+        parquet_output: Optional parquet writer for output.
+        write_to_file: Whether to write results to file.
+
+    Returns:
+        Tuple of (success boolean, optional result iterator).
+    """
+    if not prepared_query:
+        logger.error("Prepared query is None, cannot execute")
+        return False, None
+
+    try:
+        batched_iterator = await sql_client.get_batched_results(prepared_query)
+
+        if write_to_file and parquet_output:
+            await parquet_output.write_batches(batched_iterator)  # type: ignore
+            return True, None
+
+        return True, batched_iterator
+    except Exception as e:
+        logger.error(
+            f"Error during query execution or output writing: {e}", exc_info=True
+        )
+        raise
+
+
+async def finalize_multidb_results(
+    write_to_file: bool,
+    concatenate: bool,
+    return_dataframe: bool,
+    parquet_output: Optional[ParquetFileWriter],
+    dataframe_list: List[
+        Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]
+    ],
+    setup_parquet_output_func: Callable[[str, bool], Optional[ParquetFileWriter]],
+    output_path: str,
+    typename: str,
+) -> Optional[Union[ActivityStatistics, "pd.DataFrame"]]:
+    """Finalize results for multi-database execution.
+
+    Args:
+        write_to_file: Whether results were written to file.
+        concatenate: Whether to concatenate in-memory results.
+        return_dataframe: Whether to return the concatenated dataframe.
+        parquet_output: The parquet writer used (if any).
+        dataframe_list: List of dataframe iterators from each DB.
+        setup_parquet_output_func: Callback to create new parquet output.
+        output_path: Full path for output files.
+        typename: Type name for statistics.
+
+    Returns:
+        Statistics or DataFrame, or None.
+    """
+    if write_to_file and parquet_output:
+        return await parquet_output.get_statistics(typename=typename)
+
+    if not write_to_file and concatenate:
+        try:
+            import pandas as pd
+
+            valid_dataframes: List[pd.DataFrame] = []
+            for df_generator in dataframe_list:
+                if df_generator is None:
+                    continue
+                # Handle both async and sync iterators
+                if hasattr(df_generator, "__aiter__"):
+                    async for dataframe in df_generator:  # type: ignore
+                        if dataframe is None or (
+                            hasattr(dataframe, "empty") and dataframe.empty
+                        ):
+                            continue
+                        valid_dataframes.append(dataframe)
+                else:
+                    for dataframe in df_generator:  # type: ignore
+                        if dataframe is None or (
+                            hasattr(dataframe, "empty") and dataframe.empty
+                        ):
+                            continue
+                        valid_dataframes.append(dataframe)
+
+            if not valid_dataframes:
+                logger.warning(
+                    "No valid dataframes collected across databases for concatenation"
+                )
+                return None
+
+            concatenated = pd.concat(valid_dataframes, ignore_index=True)
+
+            if return_dataframe:
+                return concatenated
+
+            # Create new parquet output for concatenated data
+            concatenated_parquet_output = setup_parquet_output_func(output_path, True)
+            if concatenated_parquet_output:
+                await concatenated_parquet_output.write(concatenated)  # type: ignore
+                return await concatenated_parquet_output.get_statistics(
+                    typename=typename
+                )
+        except Exception as e:
+            logger.error(
+                f"Error concatenating multi-DB dataframes: {str(e)}",
+                exc_info=True,
+            )
+            raise
+
+    logger.warning(
+        "multidb execution returned no output to write (write_to_file=False, concatenate=False)"
+    )
+    return None
+
+
+async def execute_multidb_flow(
+    sql_client: BaseSQLClient,
+    sql_query: str,
+    workflow_args: Dict[str, Any],
+    fetch_database_sql: Optional[str],
+    output_path: str,
+    typename: str,
+    write_to_file: bool,
+    concatenate: bool,
+    return_dataframe: bool,
+    parquet_output: Optional[ParquetFileWriter],
+    temp_table_regex_sql: str,
+    setup_parquet_output_func: Callable[[str, bool], Optional[ParquetFileWriter]],
+) -> Optional[Union[ActivityStatistics, "pd.DataFrame"]]:
+    """Execute multi-database flow with proper error handling and result finalization.
+
+    Args:
+        sql_client: The SQL client to use.
+        sql_query: The SQL query to execute on each database.
+        workflow_args: Workflow arguments.
+        fetch_database_sql: SQL to fetch list of databases.
+        output_path: Full path for output files.
+        typename: Type name for statistics.
+        write_to_file: Whether to write results to file.
+        concatenate: Whether to concatenate in-memory results.
+        return_dataframe: Whether to return the concatenated dataframe.
+        parquet_output: The parquet writer used (if any).
+        temp_table_regex_sql: SQL regex for temp table exclusion.
+        setup_parquet_output_func: Callback to create new parquet output.
+
+    Returns:
+        Statistics or DataFrame, or None.
+    """
+    # Resolve databases to iterate
+    database_names = await get_database_names(
+        sql_client, workflow_args, fetch_database_sql
+    )
+    if not database_names:
+        logger.warning("No databases found to process")
+        return None
+
+    successful_databases: List[str] = []
+    dataframe_list: List[
+        Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]
+    ] = []
+
+    # Iterate databases and execute
+    for database_name in database_names or []:
+        try:
+            # Setup connection for this database
+            await setup_database_connection(sql_client, database_name)
+
+            # Prepare query for this database
+            prepared_query = prepare_database_query(
+                sql_query,
+                database_name,
+                workflow_args,
+                temp_table_regex_sql,
+                use_posix_regex=True,
+            )
+
+            # Execute using helper method
+            success, batched_iterator = await execute_single_db(
+                sql_client,
+                prepared_query,
+                parquet_output,
+                write_to_file,
+            )
+
+            if success:
+                logger.info(f"Successfully processed database: {database_name}")
+
+        except Exception as e:
+            logger.error(
+                f"Failed to process database '{database_name}': {str(e)}. Failing the workflow.",
+                exc_info=True,
+            )
+            raise
+
+        if success:
+            successful_databases.append(database_name)
+            if not write_to_file and batched_iterator:
+                dataframe_list.append(batched_iterator)
+
+    # Log results
+    logger.info(
+        f"Successfully processed {len(successful_databases)} databases: {successful_databases}"
+    )
+
+    # Finalize results
+    return await finalize_multidb_results(
+        write_to_file,
+        concatenate,
+        return_dataframe,
+        parquet_output,
+        dataframe_list,
+        setup_parquet_output_func,
+        output_path,
+        typename,
+    )
diff --git a/application_sdk/activities/common/utils.py b/application_sdk/activities/common/utils.py
index 163febf6a..3c4c5f85e 100644
--- a/application_sdk/activities/common/utils.py
+++ b/application_sdk/activities/common/utils.py
@@ -5,11 +5,10 @@
 """

 import asyncio
-import glob
 import os
 from datetime import timedelta
 from functools import wraps
-from typing import Any, Awaitable, Callable, List, Optional, TypeVar, cast
+from typing import Any, Awaitable, Callable, Optional, TypeVar, cast

 from temporalio import activity

@@ -230,46 +229,3 @@ async def send_periodic_heartbeat(delay: float, *details: Any) -> None:
     while True:
         await asyncio.sleep(delay)
         activity.heartbeat(*details)
-
-
-def find_local_files_by_extension(
-    path: str,
-    extension: str,
-    file_names: Optional[List[str]] = None,
-) -> List[str]:
-    """Find local files at the specified local path, optionally filtering by file names.
-
-    Args:
-        path (str): Local path to search in (file or directory)
-        extension (str): File extension to filter by (e.g., '.parquet', '.json')
-        file_names (Optional[List[str]]): List of file names (basenames) to filter by, paths are not supported
-
-    Returns:
-        List[str]: List of matching file paths
-
-    Example:
-        >>> find_local_files_by_extension("/data", ".parquet", ["file1.parquet", "file2.parquet"])
-        ['file1.parquet', 'file2.parquet']
-
-        >>> find_local_files_by_extension("/data/single.json", ".json")
-        ['single.json']
-    """
-    if os.path.isfile(path) and path.endswith(extension):
-        # Single file - return it directly
-        return [path]
-
-    elif os.path.isdir(path):
-        # Directory - find all files in directory
-        all_files = glob.glob(
-            os.path.join(path, "**", f"*{extension}"),
-            recursive=True,
-        )
-
-        # Filter by file names if specified
-        if file_names:
-            file_names_set = set(file_names)  # Convert to set for O(1) lookup
-            return [f for f in all_files if os.path.basename(f) in file_names_set]
-        else:
-            return all_files
-
-    return []
diff --git a/application_sdk/activities/metadata_extraction/sql.py b/application_sdk/activities/metadata_extraction/sql.py
index 9ab39092f..4774b8e2d 100644
--- a/application_sdk/activities/metadata_extraction/sql.py
+++ b/application_sdk/activities/metadata_extraction/sql.py
@@ -2,10 +2,7 @@
 from typing import (
     TYPE_CHECKING,
     Any,
-    AsyncIterator,
     Dict,
-    Iterator,
-    List,
     Optional,
     Tuple,
     Type,
@@ -17,6 +14,7 @@
 from temporalio import activity

 from application_sdk.activities import ActivitiesInterface, ActivitiesState
+from application_sdk.activities.common import sql_utils
 from application_sdk.activities.common.models import ActivityStatistics
 from application_sdk.activities.common.utils import (
     auto_heartbeater,
@@ -24,21 +22,15 @@
     get_workflow_id,
 )
 from application_sdk.clients.sql import BaseSQLClient
-from application_sdk.common.dataframe_utils import is_empty_dataframe
 from application_sdk.common.error_codes import ActivityError
-from application_sdk.common.utils import (
-    get_database_names,
-    parse_credentials_extra,
-    prepare_query,
-    read_sql_files,
-)
+from application_sdk.common.utils import prepare_query, read_sql_files
 from application_sdk.constants import APP_TENANT_ID, APPLICATION_NAME, SQL_QUERIES_PATH
 from application_sdk.handlers.sql import BaseSQLHandler
-from application_sdk.inputs.parquet import ParquetInput
-from application_sdk.inputs.sql_query import SQLQueryInput
+from application_sdk.io import DataframeType
+from application_sdk.io._utils import is_empty_dataframe
+from application_sdk.io.json import JsonFileWriter
+from application_sdk.io.parquet import ParquetFileReader, ParquetFileWriter
 from application_sdk.observability.logger_adaptor import get_logger
-from application_sdk.outputs.json import JsonOutput
-from application_sdk.outputs.parquet import ParquetOutput
 from application_sdk.services.atlan_storage import AtlanStorage
 from application_sdk.services.secretstore import SecretStore
 from application_sdk.transformers import TransformerInterface
@@ -237,105 +229,106 @@ def _validate_output_args(
     @overload
     async def query_executor(
         self,
-        sql_engine: Any,
+        sql_client: BaseSQLClient,
         sql_query: Optional[str],
         workflow_args: Dict[str, Any],
-        output_suffix: str,
+        output_path: str,
         typename: str,
         write_to_file: bool = True,
         concatenate: bool = False,
         return_dataframe: bool = False,
-        sql_client: Optional[BaseSQLClient] = None,
     ) -> Optional[ActivityStatistics]: ...

     @overload
     async def query_executor(
         self,
-        sql_engine: Any,
+        sql_client: BaseSQLClient,
         sql_query: Optional[str],
         workflow_args: Dict[str, Any],
-        output_suffix: str,
+        output_path: str,
         typename: str,
         write_to_file: bool = True,
         concatenate: bool = False,
         return_dataframe: bool = True,
-        sql_client: Optional[BaseSQLClient] = None,
     ) -> Optional[Union[ActivityStatistics, "pd.DataFrame"]]: ...

     async def query_executor(
         self,
-        sql_engine: Any,
+        sql_client: BaseSQLClient,
         sql_query: Optional[str],
         workflow_args: Dict[str, Any],
-        output_suffix: str,
+        output_path: str,
         typename: str,
         write_to_file: bool = True,
         concatenate: bool = False,
         return_dataframe: bool = False,
-        sql_client: Optional[BaseSQLClient] = None,
     ) -> Optional[Union[ActivityStatistics, "pd.DataFrame"]]:
         """
-        Executes a SQL query using the provided engine and saves the results to Parquet.
+        Executes a SQL query using the provided client and saves the results to Parquet.

-        This method validates the input engine and query, prepares the query using
-        workflow arguments, executes it, writes the resulting Daft DataFrame to
+        This method validates the input client and query, prepares the query using
+        workflow arguments, executes it, writes the resulting DataFrame to
         a Parquet file, and returns statistics about the output.

         Args:
-            sql_engine: The SQL engine instance to use for executing the query.
+            sql_client: The SQL client instance to use for executing the query.
             sql_query: The SQL query string to execute. Placeholders can be used which
                    will be replaced using `workflow_args`.
-            workflow_args: Dictionary containing arguments for the workflow, used for
-                           preparing the query and defining output paths. Expected keys:
-                           - "output_prefix": Prefix for the output path.
-                           - "output_path": Base directory for the output.
-            output_suffix: Suffix to append to the output file name.
+            workflow_args: Dictionary containing arguments for the workflow.
+            output_path: Full path where the output files will be written.
             typename: Type name used for generating output statistics.
+            write_to_file: Whether to write results to file. Defaults to True.
+            concatenate: Whether to concatenate results in multidb mode. Defaults to False.
+            return_dataframe: Whether to return a DataFrame instead of statistics. Defaults to False.

         Returns:
             Optional[Union[ActivityStatistics, pd.DataFrame]]: Statistics about the generated Parquet file,
             or a DataFrame if return_dataframe=True, or None if the query is empty or execution fails.

         Raises:
-            ValueError: If `sql_engine` is not provided.
+            ValueError: If `sql_client` is not provided.
         """
         # Common pre-checks and setup shared by both multidb and single-db paths
+        if not sql_client:
+            logger.error("SQL client is not provided")
+            raise ValueError("SQL client is required for query execution")
+
         if not sql_query:
             logger.warning("Query is empty, skipping execution.")
             return None

-        if not sql_engine:
-            logger.error("SQL engine is not set.")
-            raise ValueError("SQL engine must be provided.")
-
         # Setup parquet output using helper method
-        parquet_output = self._setup_parquet_output(
-            workflow_args, output_suffix, write_to_file
-        )
+        parquet_output = self._setup_parquet_output(output_path, write_to_file)

         # If multidb mode is enabled, run per-database flow
         if getattr(self, "multidb", False):
-            return await self._execute_multidb_flow(
-                sql_client,
-                sql_query,
-                workflow_args,
-                output_suffix,
-                typename,
-                write_to_file,
-                concatenate,
-                return_dataframe,
-                parquet_output,
+            return await sql_utils.execute_multidb_flow(
+                sql_client=sql_client,
+                sql_query=sql_query,
+                workflow_args=workflow_args,
+                fetch_database_sql=self.fetch_database_sql,
+                output_path=output_path,
+                typename=typename,
+                write_to_file=write_to_file,
+                concatenate=concatenate,
+                return_dataframe=return_dataframe,
+                parquet_output=parquet_output,
+                temp_table_regex_sql=self._get_temp_table_regex_sql(typename),
+                setup_parquet_output_func=self._setup_parquet_output,
             )

         # Single-db execution path
         # Prepare query for single-db execution
-        prepared_query = self._prepare_database_query(
-            sql_query, None, workflow_args, typename
+        prepared_query = sql_utils.prepare_database_query(
+            sql_query,
+            None,
+            workflow_args,
+            self._get_temp_table_regex_sql(typename),
         )

         # Execute using helper method
-        success, _ = await self._execute_single_db(
-            sql_engine, prepared_query, parquet_output, write_to_file
+        success, _ = await sql_utils.execute_single_db(
+            sql_client, prepared_query, parquet_output, write_to_file
         )

         if not success:
@@ -353,22 +346,23 @@ async def query_executor(

     def _setup_parquet_output(
         self,
-        workflow_args: Dict[str, Any],
-        output_suffix: str,
+        output_path: str,
         write_to_file: bool,
-    ) -> Optional[ParquetOutput]:
+    ) -> Optional[ParquetFileWriter]:
+        """Create a ParquetFileWriter for the given output path.
+
+        Args:
+            output_path: Full path where the output files will be written.
+            write_to_file: Whether to write results to file.
+
+        Returns:
+            Optional[ParquetFileWriter]: A ParquetFileWriter instance, or None if write_to_file is False.
+        """
         if not write_to_file:
             return None
-        output_prefix = workflow_args.get("output_prefix")
-        output_path = workflow_args.get("output_path")
-        if not output_prefix or not output_path:
-            logger.error("Output prefix or path not provided in workflow_args.")
-            raise ValueError(
-                "Output prefix and path must be specified in workflow_args."
-            )
-        return ParquetOutput(
+
+        return ParquetFileWriter(
             output_path=output_path,
-            output_suffix=output_suffix,
             use_consolidation=True,
         )

@@ -381,243 +375,6 @@ def _get_temp_table_regex_sql(self, typename: str) -> str:
         else:
             return ""

-    def _prepare_database_query(
-        self,
-        sql_query: str,
-        database_name: Optional[str],
-        workflow_args: Dict[str, Any],
-        typename: str,
-        use_posix_regex: bool = False,
-    ) -> Optional[str]:
-        """Prepare query for database execution with proper substitutions."""
-        # Replace database name placeholder if provided
-        fetch_sql = sql_query
-        if database_name:
-            fetch_sql = fetch_sql.replace("{database_name}", database_name)
-
-        # Get temp table regex SQL
-        temp_table_regex_sql = self._get_temp_table_regex_sql(typename)
-
-        # Prepare the query
-        prepared_query = prepare_query(
-            query=fetch_sql,
-            workflow_args=workflow_args,
-            temp_table_regex_sql=temp_table_regex_sql,
-            use_posix_regex=use_posix_regex,
-        )
-
-        if prepared_query is None:
-            db_context = f" for database {database_name}" if database_name else ""
-            raise ValueError(f"Failed to prepare query{db_context}")
-
-        return prepared_query
-
-    async def _setup_database_connection(
-        self,
-        sql_client: BaseSQLClient,
-        database_name: str,
-    ) -> None:
-        """Setup connection for a specific database."""
-        extra = parse_credentials_extra(sql_client.credentials)
-        extra["database"] = database_name
-        sql_client.credentials["extra"] = extra
-        await sql_client.load(sql_client.credentials)
-
-    # NOTE: Consolidated: per-database processing is now inlined in the multi-DB loop
-
-    async def _finalize_multidb_results(
-        self,
-        write_to_file: bool,
-        concatenate: bool,
-        return_dataframe: bool,
-        parquet_output: Optional[ParquetOutput],
-        dataframe_list: List[
-            Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]
-        ],
-        workflow_args: Dict[str, Any],
-        output_suffix: str,
-        typename: str,
-    ) -> Optional[Union[ActivityStatistics, "pd.DataFrame"]]:
-        """Finalize results for multi-database execution."""
-        if write_to_file and parquet_output:
-            return await parquet_output.get_statistics(typename=typename)
-
-        if not write_to_file and concatenate:
-            try:
-                import pandas as pd  # type: ignore
-
-                valid_dataframes: List[pd.DataFrame] = []
-                for df_generator in dataframe_list:
-                    if df_generator is None:
-                        continue
-                    for dataframe in df_generator:  # type: ignore[assignment]
-                        if dataframe is None:
-                            continue
-                        if hasattr(dataframe, "empty") and getattr(dataframe, "empty"):
-                            continue
-                        valid_dataframes.append(dataframe)
-
-                if not valid_dataframes:
-                    logger.warning(
-                        "No valid dataframes collected across databases for concatenation"
-                    )
-                    return None
-
-                concatenated = pd.concat(valid_dataframes, ignore_index=True)
-
-                if return_dataframe:
-                    return concatenated  # type: ignore[return-value]
-
-                # Create new parquet output for concatenated data
-                concatenated_parquet_output = self._setup_parquet_output(
-                    workflow_args, output_suffix, True
-                )
-                if concatenated_parquet_output:
-                    await concatenated_parquet_output.write_dataframe(concatenated)  # type: ignore[arg-type]
-                    return await concatenated_parquet_output.get_statistics(
-                        typename=typename
-                    )
-            except Exception as e:  # noqa: BLE001
-                logger.error(
-                    f"Error concatenating multi-DB dataframes: {str(e)}",
-                    exc_info=True,
-                )
-                raise
-
-        logger.warning(
-            "multidb execution returned no output to write (write_to_file=False, concatenate=False)"
-        )
-        return None
-
-    async def _execute_multidb_flow(
-        self,
-        sql_client: Optional[BaseSQLClient],
-        sql_query: str,
-        workflow_args: Dict[str, Any],
-        output_suffix: str,
-        typename: str,
-        write_to_file: bool,
-        concatenate: bool,
-        return_dataframe: bool,
-        parquet_output: Optional[ParquetOutput],
-    ) -> Optional[Union[ActivityStatistics, "pd.DataFrame"]]:
-        """Execute multi-database flow with proper error handling and result finalization."""
-        # Get effective SQL client
-        effective_sql_client = sql_client
-        if effective_sql_client is None:
-            state = cast(
-                BaseSQLMetadataExtractionActivitiesState,
-                await self._get_state(workflow_args),
-            )
-            effective_sql_client = state.sql_client
-
-        if not effective_sql_client:
-            logger.error("SQL client not initialized for multidb execution")
-            raise ValueError("SQL client not initialized")
-
-        # Resolve databases to iterate
-        database_names = await get_database_names(
-            effective_sql_client, workflow_args, self.fetch_database_sql
-        )
-        if not database_names:
-            logger.warning("No databases found to process")
-            return None
-
-        # Validate client
-        if not effective_sql_client.engine:
-            logger.error("SQL client engine not initialized")
-            raise ValueError("SQL client engine not initialized")
-
-        successful_databases: List[str] = []
-        dataframe_list: List[
-            Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]
-        ] = []
-
-        # Iterate databases and execute (consolidated single-db processing)
-        for database_name in database_names or []:
-            try:
-                # Setup connection for this database
-                await self._setup_database_connection(
-                    effective_sql_client, database_name
-                )
-
-                # Prepare query for this database
-                prepared_query = self._prepare_database_query(
-                    sql_query,
-                    database_name,
-                    workflow_args,
-                    typename,
-                    use_posix_regex=True,
-                )
-
-                # Execute using helper method
-                success, batched_iterator = await self._execute_single_db(
-                    effective_sql_client.engine,
-                    prepared_query,
-                    parquet_output,
-                    write_to_file,
-                )
-
-                if success:
-                    logger.info(f"Successfully processed database: {database_name}")
-
-            except Exception as e:  # noqa: BLE001
-                logger.error(
-                    f"Failed to process database '{database_name}': {str(e)}. Failing the workflow.",
-                    exc_info=True,
-                )
-                raise
-
-            if success:
-                successful_databases.append(database_name)
-                if not write_to_file and batched_iterator:
-                    dataframe_list.append(batched_iterator)
-
-        # Log results
-        logger.info(
-            f"Successfully processed {len(successful_databases)} databases: {successful_databases}"
-        )
-
-        # Finalize results
-        return await self._finalize_multidb_results(
-            write_to_file,
-            concatenate,
-            return_dataframe,
-            parquet_output,
-            dataframe_list,
-            workflow_args,
-            output_suffix,
-            typename,
-        )
-
-    async def _execute_single_db(
-        self,
-        sql_engine: Any,
-        prepared_query: Optional[str],
-        parquet_output: Optional[ParquetOutput],
-        write_to_file: bool,
-    ) -> Tuple[
-        bool, Optional[Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]]
-    ]:  # type: ignore
-        if not prepared_query:
-            logger.error("Prepared query is None, cannot execute")
-            return False, None
-
-        try:
-            sql_input = SQLQueryInput(engine=sql_engine, query=prepared_query)
-            batched_iterator = await sql_input.get_batched_dataframe()
-
-            if write_to_file and parquet_output:
-                await parquet_output.write_batched_dataframe(batched_iterator)  # type: ignore
-                return True, None
-
-            return True, batched_iterator
-        except Exception as e:
-            logger.error(
-                f"Error during query execution or output writing: {e}", exc_info=True
-            )
-            raise
-
     @activity.defn
     @auto_heartbeater
     async def fetch_databases(
@@ -626,29 +383,28 @@ async def fetch_databases(
         """Fetch databases from the source database.

         Args:
-            batch_input: DataFrame containing the raw database data.
-            raw_output: JsonOutput instance for writing raw data.
-            **kwargs: Additional keyword arguments.
+            workflow_args: Dictionary containing arguments for the workflow.

         Returns:
-            Dict containing chunk count, typename, and total record count.
+            Optional[ActivityStatistics]: Statistics about the extracted databases.
         """
         state = cast(
             BaseSQLMetadataExtractionActivitiesState,
             await self._get_state(workflow_args),
         )
-        if not state.sql_client or not state.sql_client.engine:
-            logger.error("SQL client or engine not initialized")
-            raise ValueError("SQL client or engine not initialized")
+        if not state.sql_client:
+            logger.error("SQL client not initialized")
+            raise ValueError("SQL client not initialized")

         prepared_query = prepare_query(
             query=self.fetch_database_sql, workflow_args=workflow_args
         )
+        base_output_path = workflow_args.get("output_path", "")
         statistics = await self.query_executor(
-            sql_engine=state.sql_client.engine,
+            sql_client=state.sql_client,
             sql_query=prepared_query,
             workflow_args=workflow_args,
-            output_suffix="raw/database",
+            output_path=os.path.join(base_output_path, "raw", "database"),
             typename="database",
         )
         return statistics
@@ -661,29 +417,28 @@ async def fetch_schemas(
         """Fetch schemas from the source database.

         Args:
-            batch_input: DataFrame containing the raw schema data.
-            raw_output: JsonOutput instance for writing raw data.
-            **kwargs: Additional keyword arguments.
+            workflow_args: Dictionary containing arguments for the workflow.

         Returns:
-            Dict containing chunk count, typename, and total record count.
+            Optional[ActivityStatistics]: Statistics about the extracted schemas.
         """
         state = cast(
             BaseSQLMetadataExtractionActivitiesState,
             await self._get_state(workflow_args),
         )
-        if not state.sql_client or not state.sql_client.engine:
-            logger.error("SQL client or engine not initialized")
-            raise ValueError("SQL client or engine not initialized")
+        if not state.sql_client:
+            logger.error("SQL client not initialized")
+            raise ValueError("SQL client not initialized")

         prepared_query = prepare_query(
             query=self.fetch_schema_sql, workflow_args=workflow_args
         )
+        base_output_path = workflow_args.get("output_path", "")
         statistics = await self.query_executor(
-            sql_engine=state.sql_client.engine,
+            sql_client=state.sql_client,
             sql_query=prepared_query,
             workflow_args=workflow_args,
-            output_suffix="raw/schema",
+            output_path=os.path.join(base_output_path, "raw", "schema"),
             typename="schema",
         )
         return statistics
@@ -696,9 +451,7 @@ async def fetch_tables(
         """Fetch tables from the source database.

         Args:
-            batch_input: DataFrame containing the raw table data.
-            raw_output: JsonOutput instance for writing raw data.
-            **kwargs: Additional keyword arguments.
+            workflow_args: Dictionary containing arguments for the workflow.

         Returns:
             Optional[ActivityStatistics]: Statistics about the extracted tables, or None if extraction failed.
@@ -707,20 +460,21 @@ async def fetch_tables(
             BaseSQLMetadataExtractionActivitiesState,
             await self._get_state(workflow_args),
         )
-        if not state.sql_client or not state.sql_client.engine:
-            logger.error("SQL client or engine not initialized")
-            raise ValueError("SQL client or engine not initialized")
+        if not state.sql_client:
+            logger.error("SQL client not initialized")
+            raise ValueError("SQL client not initialized")

         prepared_query = prepare_query(
             query=self.fetch_table_sql,
             workflow_args=workflow_args,
             temp_table_regex_sql=self.extract_temp_table_regex_table_sql,
         )
+        base_output_path = workflow_args.get("output_path", "")
         statistics = await self.query_executor(
-            sql_engine=state.sql_client.engine,
+            sql_client=state.sql_client,
             sql_query=prepared_query,
             workflow_args=workflow_args,
-            output_suffix="raw/table",
+            output_path=os.path.join(base_output_path, "raw", "table"),
             typename="table",
         )
         return statistics
@@ -733,9 +487,7 @@ async def fetch_columns(
         """Fetch columns from the source database.

         Args:
-            batch_input: DataFrame containing the raw column data.
-            raw_output: JsonOutput instance for writing raw data.
-            **kwargs: Additional keyword arguments.
+            workflow_args: Dictionary containing arguments for the workflow.

         Returns:
             Optional[ActivityStatistics]: Statistics about the extracted columns, or None if extraction failed.
@@ -744,20 +496,21 @@ async def fetch_columns(
             BaseSQLMetadataExtractionActivitiesState,
             await self._get_state(workflow_args),
         )
-        if not state.sql_client or not state.sql_client.engine:
-            logger.error("SQL client or engine not initialized")
-            raise ValueError("SQL client or engine not initialized")
+        if not state.sql_client:
+            logger.error("SQL client not initialized")
+            raise ValueError("SQL client not initialized")

         prepared_query = prepare_query(
             query=self.fetch_column_sql,
             workflow_args=workflow_args,
             temp_table_regex_sql=self.extract_temp_table_regex_column_sql,
         )
+        base_output_path = workflow_args.get("output_path", "")
         statistics = await self.query_executor(
-            sql_engine=state.sql_client.engine,
+            sql_client=state.sql_client,
             sql_query=prepared_query,
             workflow_args=workflow_args,
-            output_suffix="raw/column",
+            output_path=os.path.join(base_output_path, "raw", "column"),
             typename="column",
         )
         return statistics
@@ -770,9 +523,7 @@ async def fetch_procedures(
         """Fetch procedures from the source database.

         Args:
-            batch_input: DataFrame containing the raw column data.
-            raw_output: JsonOutput instance for writing raw data.
-            **kwargs: Additional keyword arguments.
+            workflow_args: Dictionary containing arguments for the workflow.

         Returns:
             Optional[ActivityStatistics]: Statistics about the extracted procedures, or None if extraction failed.
@@ -781,18 +532,19 @@ async def fetch_procedures(
             BaseSQLMetadataExtractionActivitiesState,
             await self._get_state(workflow_args),
         )
-        if not state.sql_client or not state.sql_client.engine:
-            logger.error("SQL client or engine not initialized")
-            raise ValueError("SQL client or engine not initialized")
+        if not state.sql_client:
+            logger.error("SQL client not initialized")
+            raise ValueError("SQL client not initialized")

         prepared_query = prepare_query(
             query=self.fetch_procedure_sql, workflow_args=workflow_args
         )
+        base_output_path = workflow_args.get("output_path", "")
         statistics = await self.query_executor(
-            sql_engine=state.sql_client.engine,
+            sql_client=state.sql_client,
             sql_query=prepared_query,
             workflow_args=workflow_args,
-            output_suffix="raw/extras-procedure",
+            output_path=os.path.join(base_output_path, "raw", "extras-procedure"),
             typename="extras-procedure",
         )
         return statistics
@@ -807,7 +559,7 @@ async def transform_data(

         Args:
             raw_input (Any): Input data to transform.
-            transformed_output (JsonOutput): Output handler for transformed data.
+            transformed_output (JsonFileWriter): Output handler for transformed data.
             **kwargs: Additional keyword arguments.

         Returns:
@@ -824,17 +576,18 @@ async def transform_data(
             self._validate_output_args(workflow_args)
         )

-        raw_input = ParquetInput(
+        raw_input = ParquetFileReader(
             path=os.path.join(output_path, "raw"),
             file_names=workflow_args.get("file_names"),
+            dataframe_type=DataframeType.daft,
         )
-        raw_input = raw_input.get_batched_daft_dataframe()
+        raw_input = raw_input.read_batches()

-        transformed_output = JsonOutput(
-            output_path=output_path,
-            output_suffix="transformed",
+        transformed_output = JsonFileWriter(
+            output_path=os.path.join(output_path, "transformed"),
             typename=typename,
             chunk_start=workflow_args.get("chunk_start"),
+            dataframe_type=DataframeType.daft,
         )
         if state.transformer:
             workflow_args["connection_name"] = workflow_args.get("connection", {}).get(
@@ -849,7 +602,7 @@ async def transform_data(
                     transform_metadata = state.transformer.transform_metadata(
                         dataframe=dataframe, **workflow_args
                     )
-                await transformed_output.write_daft_dataframe(transform_metadata)
+                    await transformed_output.write(transform_metadata)
         return await transformed_output.get_statistics(typename=typename)

     @activity.defn
diff --git a/application_sdk/activities/query_extraction/sql.py b/application_sdk/activities/query_extraction/sql.py
index f712e3439..18e08deb4 100644
--- a/application_sdk/activities/query_extraction/sql.py
+++ b/application_sdk/activities/query_extraction/sql.py
@@ -16,9 +16,8 @@
 from application_sdk.constants import UPSTREAM_OBJECT_STORE_NAME
 from application_sdk.handlers import HandlerInterface
 from application_sdk.handlers.sql import BaseSQLHandler
-from application_sdk.inputs.sql_query import SQLQueryInput
+from application_sdk.io.parquet import ParquetFileWriter
 from application_sdk.observability.logger_adaptor import get_logger
-from application_sdk.outputs.parquet import ParquetOutput
 from application_sdk.services.objectstore import ObjectStore
 from application_sdk.services.secretstore import SecretStore
 from application_sdk.transformers import TransformerInterface
@@ -202,21 +201,23 @@ async def fetch_queries(

         try:
             state = await self._get_state(workflow_args)
-            sql_input = SQLQueryInput(
-                engine=state.sql_client.engine,
-                query=self.get_formatted_query(self.fetch_queries_sql, workflow_args),
-                chunk_size=None,
+            sql_client = state.sql_client
+            if not sql_client:
+                logger.error("SQL client not initialized")
+                raise ValueError("SQL client not initialized")
+
+            formatted_query = self.get_formatted_query(
+                self.fetch_queries_sql, workflow_args
             )
-            sql_input = await sql_input.get_dataframe()
+            sql_results = await sql_client.get_results(formatted_query)

-            raw_output = ParquetOutput(
-                output_path=workflow_args["output_path"],
-                output_suffix="raw/query",
+            raw_output = ParquetFileWriter(
+                output_path=os.path.join(workflow_args["output_path"], "raw/query"),
                 chunk_size=workflow_args["miner_args"].get("chunk_size", 100000),
                 start_marker=workflow_args["start_marker"],
                 end_marker=workflow_args["end_marker"],
             )
-            await raw_output.write_dataframe(sql_input)
+            await raw_output.write(sql_results)
             logger.info(
                 f"Query fetch completed, {raw_output.total_record_count} records processed",
             )
diff --git a/application_sdk/application/__init__.py b/application_sdk/application/__init__.py
index 30bbc0613..70cc1d2bb 100644
--- a/application_sdk/application/__init__.py
+++ b/application_sdk/application/__init__.py
@@ -5,8 +5,8 @@
 from application_sdk.clients.base import BaseClient
 from application_sdk.clients.utils import get_workflow_client
 from application_sdk.constants import ENABLE_MCP
-from application_sdk.events.models import EventRegistration
 from application_sdk.handlers.base import BaseHandler
+from application_sdk.interceptors.models import EventRegistration
 from application_sdk.observability.logger_adaptor import get_logger
 from application_sdk.server import ServerInterface
 from application_sdk.server.fastapi import APIServer, HttpWorkflowTrigger
diff --git a/application_sdk/clients/sql.py b/application_sdk/clients/sql.py
index 1603e3395..15b4927ab 100644
--- a/application_sdk/clients/sql.py
+++ b/application_sdk/clients/sql.py
@@ -6,8 +6,19 @@
 """

 import asyncio
+import concurrent
 from concurrent.futures import ThreadPoolExecutor
-from typing import Any, Dict, List, Optional
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    AsyncIterator,
+    Dict,
+    Iterator,
+    List,
+    Optional,
+    Union,
+    cast,
+)
 from urllib.parse import quote_plus

 from sqlalchemy.ext.asyncio import AsyncConnection, AsyncEngine
@@ -27,6 +38,11 @@
 logger = get_logger(__name__)
 activity.logger = logger

+if TYPE_CHECKING:
+    import daft
+    import pandas as pd
+    from sqlalchemy.orm import Session
+

 class BaseSQLClient(ClientInterface):
     """SQL client for database operations.
@@ -53,6 +69,7 @@ def __init__(
         self,
         use_server_side_cursor: bool = USE_SERVER_SIDE_CURSOR,
         credentials: Dict[str, Any] = {},
+        chunk_size: int = 5000,
     ):
         """
         Initialize the SQL client.
@@ -64,6 +81,7 @@ def __init__(
         """
         self.use_server_side_cursor = use_server_side_cursor
         self.credentials = credentials
+        self.chunk_size = chunk_size

     async def load(self, credentials: Dict[str, Any]) -> None:
         """Load credentials and prepare engine for lazy connections.
@@ -383,6 +401,154 @@ async def run_query(self, query: str, batch_size: int = 100000):

         logger.info("Query execution completed")

+    def _execute_pandas_query(
+        self, conn, query, chunksize: Optional[int]
+    ) -> Union["pd.DataFrame", Iterator["pd.DataFrame"]]:
+        """Helper function to execute SQL query using pandas.
+           The function is responsible for using import_optional_dependency method of the pandas library to import sqlalchemy
+           This function helps pandas in determining weather to use the sqlalchemy connection object and constructs like text()
+           or use the underlying database connection object. This has been done to make sure connectors like the Redshift connector,
+           which do not support the sqlalchemy connection object, can be made compatible with the application-sdk.
+
+        Args:
+            conn: Database connection object.
+
+        Returns:
+            Union["pd.DataFrame", Iterator["pd.DataFrame"]]: Query results as DataFrame
+                or iterator of DataFrames if chunked.
+        """
+        import pandas as pd
+        from pandas.compat._optional import import_optional_dependency
+        from sqlalchemy import text
+
+        if import_optional_dependency("sqlalchemy", errors="ignore"):
+            return pd.read_sql_query(text(query), conn, chunksize=chunksize)
+        else:
+            dbapi_conn = getattr(conn, "connection", None)
+            return pd.read_sql_query(query, dbapi_conn, chunksize=chunksize)
+
+    def _read_sql_query(
+        self, session: "Session", query: str, chunksize: Optional[int]
+    ) -> Union["pd.DataFrame", Iterator["pd.DataFrame"]]:
+        """Execute SQL query using the provided session.
+
+        Args:
+            session: SQLAlchemy session for database operations.
+
+        Returns:
+            Union["pd.DataFrame", Iterator["pd.DataFrame"]]: Query results as DataFrame
+                or iterator of DataFrames if chunked.
+        """
+        conn = session.connection()
+        return self._execute_pandas_query(conn, query, chunksize=chunksize)
+
+    def _execute_query_daft(
+        self, query: str, chunksize: Optional[int]
+    ) -> Union["daft.DataFrame", Iterator["daft.DataFrame"]]:
+        """Execute SQL query using the provided engine and daft.
+
+        Returns:
+            Union["daft.DataFrame", Iterator["daft.DataFrame"]]: Query results as DataFrame
+                or iterator of DataFrames if chunked.
+        """
+        # Daft uses ConnectorX to read data from SQL by default for supported connectors
+        # If a connection string is passed, it will use ConnectorX to read data
+        # For unsupported connectors and if directly engine is passed, it will use SQLAlchemy
+        import daft
+
+        if not self.engine:
+            raise ValueError("Engine is not initialized. Call load() first.")
+
+        if isinstance(self.engine, str):
+            return daft.read_sql(query, self.engine, infer_schema_length=chunksize)
+        return daft.read_sql(query, self.engine.connect, infer_schema_length=chunksize)
+
+    def _execute_query(
+        self, query: str, chunksize: Optional[int]
+    ) -> Union["pd.DataFrame", Iterator["pd.DataFrame"]]:
+        """Execute SQL query using the provided engine and pandas.
+
+        Returns:
+            Union["pd.DataFrame", Iterator["pd.DataFrame"]]: Query results as DataFrame
+                or iterator of DataFrames if chunked.
+        """
+        if not self.engine:
+            raise ValueError("Engine is not initialized. Call load() first.")
+
+        with self.engine.connect() as conn:
+            return self._execute_pandas_query(conn, query, chunksize)
+
+    async def _execute_async_read_operation(
+        self, query: str, chunksize: Optional[int]
+    ) -> Union["pd.DataFrame", Iterator["pd.DataFrame"]]:
+        """Helper to execute async read operation with either async session or thread executor."""
+        if isinstance(self.engine, str):
+            raise ValueError("Engine should be an SQLAlchemy engine object")
+
+        from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession
+
+        async_session = None
+        if self.engine and isinstance(self.engine, AsyncEngine):
+            from sqlalchemy.orm import sessionmaker
+
+            async_session = sessionmaker(
+                self.engine, expire_on_commit=False, class_=AsyncSession
+            )
+
+        if async_session:
+            async with async_session() as session:
+                return await session.run_sync(
+                    self._read_sql_query, query, chunksize=chunksize
+                )
+        else:
+            # Run the blocking operation in a thread pool
+            with concurrent.futures.ThreadPoolExecutor() as executor:
+                return await asyncio.get_event_loop().run_in_executor(
+                    executor, self._execute_query, query, chunksize
+                )
+
+    async def get_batched_results(
+        self,
+        query: str,
+    ) -> Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]:  # type: ignore
+        """Get query results as batched pandas DataFrames asynchronously.
+
+        Returns:
+            AsyncIterator["pd.DataFrame"]: Async iterator yielding batches of query results.
+
+        Raises:
+            ValueError: If engine is a string instead of SQLAlchemy engine.
+            Exception: If there's an error executing the query.
+        """
+        try:
+            # We cast to Iterator because passing chunk_size guarantees an Iterator return
+            result = await self._execute_async_read_operation(query, self.chunk_size)
+            return cast(Iterator["pd.DataFrame"], result)
+        except Exception as e:
+            logger.error(f"Error reading batched data(pandas) from SQL: {str(e)}")
+
+    async def get_results(self, query: str) -> "pd.DataFrame":
+        """Get all query results as a single pandas DataFrame asynchronously.
+
+        Returns:
+            pd.DataFrame: Query results as a DataFrame.
+
+        Raises:
+            ValueError: If engine is a string instead of SQLAlchemy engine.
+            Exception: If there's an error executing the query.
+        """
+        try:
+            result = await self._execute_async_read_operation(query, None)
+            import pandas as pd
+
+            if isinstance(result, pd.DataFrame):
+                return result
+            raise Exception("Unable to get pandas dataframe from SQL query results")
+
+        except Exception as e:
+            logger.error(f"Error reading data(pandas) from SQL: {str(e)}")
+            raise e
+

 class AsyncBaseSQLClient(BaseSQLClient):
     """Asynchronous SQL client for database operations.
diff --git a/application_sdk/clients/temporal.py b/application_sdk/clients/temporal.py
index 3e2a6abcb..2911b388d 100644
--- a/application_sdk/clients/temporal.py
+++ b/application_sdk/clients/temporal.py
@@ -26,18 +26,18 @@
     WORKFLOW_PORT,
     WORKFLOW_TLS_ENABLED,
 )
-from application_sdk.events.models import (
-    ApplicationEventNames,
-    Event,
-    EventTypes,
-    WorkerTokenRefreshEventData,
-)
 from application_sdk.interceptors.cleanup import CleanupInterceptor, cleanup
 from application_sdk.interceptors.correlation_context import (
     CorrelationContextInterceptor,
 )
 from application_sdk.interceptors.events import EventInterceptor, publish_event
 from application_sdk.interceptors.lock import RedisLockInterceptor
+from application_sdk.interceptors.models import (
+    ApplicationEventNames,
+    Event,
+    EventTypes,
+    WorkerTokenRefreshEventData,
+)
 from application_sdk.observability.logger_adaptor import get_logger
 from application_sdk.services.eventstore import EventStore
 from application_sdk.services.secretstore import SecretStore
diff --git a/application_sdk/common/dataframe_utils.py b/application_sdk/common/dataframe_utils.py
deleted file mode 100644
index 21f8aa01b..000000000
--- a/application_sdk/common/dataframe_utils.py
+++ /dev/null
@@ -1,42 +0,0 @@
-from typing import TYPE_CHECKING, Union
-
-from application_sdk.observability.logger_adaptor import get_logger
-
-if TYPE_CHECKING:
-    import daft
-    import pandas as pd
-
-logger = get_logger(__name__)
-
-
-def is_empty_dataframe(dataframe: Union["pd.DataFrame", "daft.DataFrame"]) -> bool:  # noqa: F821
-    """Check if a DataFrame is empty.
-
-    This function determines whether a DataFrame has any rows, supporting both
-    pandas and daft DataFrame types. For pandas DataFrames, it uses the `empty`
-    property, and for daft DataFrames, it checks if the row count is 0.
-
-    Args:
-        dataframe (Union[pd.DataFrame, daft.DataFrame]): The DataFrame to check,
-            can be either a pandas DataFrame or a daft DataFrame.
-
-    Returns:
-        bool: True if the DataFrame has no rows, False otherwise.
-
-    Note:
-        If daft is not available and a daft DataFrame is passed, the function
-        will log a warning and return True.
-    """
-    import pandas as pd
-
-    if isinstance(dataframe, pd.DataFrame):
-        return dataframe.empty
-
-    try:
-        import daft
-
-        if isinstance(dataframe, daft.DataFrame):
-            return dataframe.count_rows() == 0
-    except Exception:
-        logger.warning("Module daft not found")
-    return True
diff --git a/application_sdk/common/types.py b/application_sdk/common/types.py
new file mode 100644
index 000000000..f7968f917
--- /dev/null
+++ b/application_sdk/common/types.py
@@ -0,0 +1,8 @@
+from enum import Enum
+
+
+class DataframeType(Enum):
+    """Enumeration of dataframe types."""
+
+    pandas = "pandas"
+    daft = "daft"
diff --git a/application_sdk/common/utils.py b/application_sdk/common/utils.py
index 6865e85c8..e681c9718 100644
--- a/application_sdk/common/utils.py
+++ b/application_sdk/common/utils.py
@@ -20,7 +20,6 @@
 from application_sdk.activities.common.utils import get_object_store_prefix
 from application_sdk.common.error_codes import CommonError
 from application_sdk.constants import TEMPORARY_PATH
-from application_sdk.inputs.sql_query import SQLQueryInput
 from application_sdk.observability.logger_adaptor import get_logger
 from application_sdk.services.objectstore import ObjectStore

@@ -280,13 +279,7 @@ async def get_database_names(
             temp_table_regex_sql=temp_table_regex_sql,
             use_posix_regex=True,
         )
-        # We'll run the query to get all the database names
-        database_sql_input = SQLQueryInput(
-            engine=sql_client.engine,
-            query=prepared_query,  # type: ignore
-            chunk_size=None,
-        )
-        database_dataframe = await database_sql_input.get_dataframe()
+        database_dataframe = await sql_client.get_results(prepared_query)
         database_names = list(database_dataframe["database_name"])
     return database_names

diff --git a/application_sdk/events/__init__.py b/application_sdk/events/__init__.py
deleted file mode 100644
index 10c42d652..000000000
--- a/application_sdk/events/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-"""Events module for handling application events.
-
-This module provides classes and utilities for handling various types of events
-in the application, including workflow, activity, and worker events.
-"""
diff --git a/application_sdk/handlers/sql.py b/application_sdk/handlers/sql.py
index 7c0e63cc6..8f36a948f 100644
--- a/application_sdk/handlers/sql.py
+++ b/application_sdk/handlers/sql.py
@@ -1,4 +1,5 @@
 import asyncio
+import os
 import re
 from enum import Enum
 from typing import Any, Dict, List, Optional, Set, Tuple
@@ -13,7 +14,6 @@
 )
 from application_sdk.constants import SQL_QUERIES_PATH, SQL_SERVER_MIN_VERSION
 from application_sdk.handlers import HandlerInterface
-from application_sdk.inputs.sql_query import SQLQueryInput
 from application_sdk.observability.logger_adaptor import get_logger
 from application_sdk.server.fastapi.models import MetadataType

@@ -77,10 +77,7 @@ async def prepare_metadata(self) -> List[Dict[Any, Any]]:
         if self.metadata_sql is None:
             raise ValueError("metadata_sql is not defined")

-        sql_input = SQLQueryInput(
-            engine=self.sql_client.engine, query=self.metadata_sql, chunk_size=None
-        )
-        df = await sql_input.get_dataframe()
+        df = await self.sql_client.get_results(self.metadata_sql)
         result: List[Dict[Any, Any]] = []
         try:
             for row in df.to_dict(orient="records"):
@@ -103,12 +100,7 @@ async def test_auth(self) -> bool:
         :raises Exception: If the credentials are invalid.
         """
         try:
-            sql_input = SQLQueryInput(
-                engine=self.sql_client.engine,
-                query=self.test_authentication_sql,
-                chunk_size=None,
-            )
-            df = await sql_input.get_dataframe()
+            df = await self.sql_client.get_results(self.test_authentication_sql)
             df.to_dict(orient="records")
             return True
         except Exception as exc:
@@ -335,16 +327,16 @@ def _build_failure(exc: Exception) -> Dict[str, Any]:
                 # Use the base query executor in multidb mode to get concatenated df
                 activities = BaseSQLMetadataExtractionActivities()
                 activities.multidb = True
+                base_output_path = payload.get("output_path", "")
                 concatenated_df = await activities.query_executor(
-                    sql_engine=self.sql_client.engine if self.sql_client else None,
+                    sql_client=self.sql_client,
                     sql_query=self.tables_check_sql,
                     workflow_args=payload,
-                    output_suffix="raw/table",
+                    output_path=os.path.join(base_output_path, "raw", "table"),
                     typename="table",
                     write_to_file=False,
                     concatenate=True,
                     return_dataframe=True,
-                    sql_client=self.sql_client,
                 )

                 if concatenated_df is None:
@@ -362,12 +354,9 @@ def _build_failure(exc: Exception) -> Dict[str, Any]:
             )
             if not query:
                 raise ValueError("tables_check_sql is not defined")
-            sql_input = SQLQueryInput(
-                engine=self.sql_client.engine, query=query, chunk_size=None
-            )
-            sql_input = await sql_input.get_dataframe()
+            sql_results = await self.sql_client.get_results(query)
             try:
-                total = _sum_counts_from_records(sql_input.to_dict(orient="records"))
+                total = _sum_counts_from_records(sql_results.to_dict(orient="records"))
                 return _build_success(total)
             except Exception as exc:
                 return _build_failure(exc)
@@ -404,13 +393,9 @@ async def check_client_version(self) -> Dict[str, Any]:

             # If dialect version not available and client_version_sql is defined, use SQL query
             if not client_version and self.client_version_sql:
-                sql_input = await SQLQueryInput(
-                    query=self.client_version_sql,
-                    engine=self.sql_client.engine,
-                    chunk_size=None,
-                ).get_dataframe()
+                sql_results = await self.sql_client.get_results(self.client_version_sql)
                 version_string = next(
-                    iter(sql_input.to_dict(orient="records")[0].values())
+                    iter(sql_results.to_dict(orient="records")[0].values())
                 )
                 version_match = re.search(r"(\d+\.\d+(?:\.\d+)?)", version_string)
                 if version_match:
diff --git a/application_sdk/inputs/.cursor/BUGBOT.md b/application_sdk/inputs/.cursor/BUGBOT.md
deleted file mode 100644
index 331fa6e58..000000000
--- a/application_sdk/inputs/.cursor/BUGBOT.md
+++ /dev/null
@@ -1,250 +0,0 @@
-# Input Code Review Guidelines - Data Input Processing
-
-## Context-Specific Patterns
-
-This directory contains input processing implementations for various data formats (JSON, Parquet, SQL). Input processors must handle data efficiently while maintaining data integrity and performance.
-
-### Phase 1: Critical Input Safety Issues
-
-**Object Store Path Management:**
-
-- **Correct path calculation**: Source paths must use the actual object store prefix, not derived local paths
-- **Path validation**: Verify that object store keys are valid and within constraints
-- **User-provided prefixes**: Respect user-configured input prefixes and download paths
-- **Path consistency**: Ensure downloaded files match the expected object store locations
-
-**Data Validation and Security:**
-
-- All input data must be validated before processing
-- File size limits must be enforced to prevent resource exhaustion
-- File type validation required for uploaded/downloaded files
-- Malicious file content detection for executable or script files
-- Input path traversal prevention
-
-```python
-# ✅ DO: Proper object store path handling
-class JsonInput:
-    async def download_from_object_store(
-        self,
-        input_prefix: str,  # User-provided prefix
-        local_destination: str
-    ) -> List[str]:
-        """Download files with correct path handling."""
-
-        # Use the actual input prefix, not derived local path
-        object_store_source = input_prefix  # Keep user's intended source
-
-        downloaded_files = await self.object_store.download_files(
-            source=object_store_source,
-            destination=local_destination
-        )
-
-        return downloaded_files
-
-# ❌ REJECT: Incorrect path handling
-class BadJsonInput:
-    async def download_from_object_store(
-        self,
-        input_prefix: str,
-        local_destination: str
-    ) -> List[str]:
-        # Wrong: derives object store path from local path
-        object_store_source = get_object_store_prefix(local_destination)
-        # This ignores the user's actual input_prefix!
-
-        return await self.object_store.download_files(
-            source=object_store_source,  # Wrong source!
-            destination=local_destination
-        )
-```
-
-### Phase 2: Input Architecture Patterns
-
-**Performance Optimization Requirements:**
-
-- **Parallelization opportunities**: Flag sequential file operations that could be parallelized
-- **Batch processing**: Group related operations to reduce overhead
-- **Memory efficiency**: Process large files in chunks, not all at once
-- **Connection reuse**: Optimize object store connections across operations
-
-**Resource Management:**
-
-- Use proper connection pooling for object store operations
-- Implement timeout handling for download operations
-- Clean up temporary files after processing
-- Handle partial download failures gracefully
-- Monitor memory usage during large file processing
-
-```python
-# ✅ DO: Parallelized file processing
-async def download_multiple_files_parallel(
-    self,
-    file_paths: List[str],
-    destination_dir: str
-) -> List[str]:
-    """Download multiple files in parallel for better performance."""
-
-    async def download_single_file(file_path: str) -> str:
-        """Download a single file with error handling."""
-        try:
-            return await self.object_store.download_file(
-                source=file_path,
-                destination=os.path.join(destination_dir, os.path.basename(file_path))
-            )
-        except Exception as e:
-            logger.error(f"Failed to download {file_path}: {e}")
-            raise
-
-    # Parallel processing with controlled concurrency
-    semaphore = asyncio.Semaphore(10)  # Limit concurrent downloads
-
-    async def download_with_semaphore(file_path: str) -> str:
-        async with semaphore:
-            return await download_single_file(file_path)
-
-    tasks = [download_with_semaphore(path) for path in file_paths]
-    return await asyncio.gather(*tasks)
-
-# ❌ REJECT: Sequential processing
-async def download_multiple_files_sequential(self, file_paths: List[str]) -> List[str]:
-    """Sequential download - should be flagged for parallelization."""
-    downloaded = []
-    for file_path in file_paths:  # FLAG: Could be parallelized
-        result = await self.object_store.download_file(file_path)
-        downloaded.append(result)
-    return downloaded
-```
-
-### Phase 3: Input Testing Requirements
-
-**Data Input Testing:**
-
-- Test with various file formats and sizes
-- Test malformed data handling
-- Test partial download/upload scenarios
-- Mock object store operations in unit tests
-- Include integration tests with real object store
-- Test error recovery and retry logic
-
-**Performance Testing:**
-
-- Include tests for large file processing
-- Test memory usage with different chunk sizes
-- Test concurrent download/upload operations
-- Verify timeout handling works correctly
-- Test connection pool behavior
-
-### Phase 4: Performance and Scalability
-
-**Data Processing Efficiency:**
-
-- Use streaming for large files instead of loading entirely into memory
-- Implement proper chunking for batch operations
-- Use async generators for memory-efficient data processing
-- Monitor memory usage and processing time
-- Optimize file I/O operations
-
-**Object Store Optimization:**
-
-- Use connection pooling for object store clients
-- Implement proper retry logic for transient failures
-- Use parallel operations where appropriate
-- Cache frequently accessed metadata
-- Monitor object store operation metrics
-
-### Phase 5: Input Data Maintainability
-
-**Error Handling and Recovery:**
-
-- Implement comprehensive error handling for all input operations
-- Provide meaningful error messages with context
-- Handle partial failures gracefully (some files fail, others succeed)
-- Implement proper retry logic for transient failures
-- Log all input operations with sufficient context
-
-**Configuration Management:**
-
-- Externalize all input-related configuration
-- Support different input sources and formats
-- Validate input configuration before processing
-- Document all supported input parameters
-- Handle environment-specific input requirements
-
----
-
-## Input-Specific Anti-Patterns
-
-**Always Reject:**
-
-- **Path calculation errors**: Using local paths to derive object store paths
-- **Sequential processing**: Processing multiple files sequentially when parallel processing is possible
-- **Memory inefficiency**: Loading large files entirely into memory
-- **Missing error handling**: Input operations without proper try-catch blocks
-- **Poor path validation**: Not validating object store keys or file paths
-- **Resource leaks**: Not cleaning up temporary files or connections
-
-**Object Store Anti-Patterns:**
-
-```python
-# ❌ REJECT: Incorrect object store usage
-class BadInputProcessor:
-    async def process_files(self, local_files: List[str]):
-        # Wrong: derives object store path from local path
-        for local_file in local_files:
-            object_store_key = get_object_store_prefix(local_file)  # Incorrect!
-            await self.object_store.download_file(object_store_key, local_file)
-
-# ✅ REQUIRE: Correct object store usage
-class GoodInputProcessor:
-    async def process_files(
-        self,
-        object_store_paths: List[str],  # Actual object store paths
-        local_destination_dir: str
-    ):
-        # Use actual object store paths, not derived ones
-        for object_store_path in object_store_paths:
-            local_file = os.path.join(
-                local_destination_dir,
-                os.path.basename(object_store_path)
-            )
-            await self.object_store.download_file(object_store_path, local_file)
-```
-
-**Performance Anti-Patterns:**
-
-```python
-# ❌ REJECT: Sequential file processing
-async def process_files_sequential(file_list: List[str]):
-    results = []
-    for file_path in file_list:  # Should be parallelized
-        result = await process_single_file(file_path)
-        results.append(result)
-    return results
-
-# ✅ REQUIRE: Parallel file processing
-async def process_files_parallel(file_list: List[str], max_concurrency: int = 10):
-    semaphore = asyncio.Semaphore(max_concurrency)
-
-    async def process_with_semaphore(file_path: str):
-        async with semaphore:
-            return await process_single_file(file_path)
-
-    tasks = [process_with_semaphore(path) for path in file_list]
-    return await asyncio.gather(*tasks, return_exceptions=True)
-```
-
-## Educational Context for Input Reviews
-
-When reviewing input code, emphasize:
-
-1. **Data Integrity Impact**: "Incorrect object store path handling can cause data loss or corruption. Files uploaded to wrong locations become inaccessible, breaking data processing pipelines."
-
-2. **Performance Impact**: "Sequential file processing creates unnecessary bottlenecks. For enterprise datasets with hundreds of files, parallelization can reduce processing time from hours to minutes."
-
-3. **Resource Impact**: "Poor memory management in input processing can cause out-of-memory errors with large datasets. Streaming and chunking are essential for enterprise-scale data processing."
-
-4. **User Experience Impact**: "Input path handling errors are often silent until runtime, causing difficult-to-debug failures. Proper validation and clear error messages save hours of troubleshooting."
-
-5. **Scalability Impact**: "Input processing patterns that work for small datasets can fail catastrophically at enterprise scale. Always design for the largest expected dataset size."
-
-6. **Reliability Impact**: "Input operations are often the first point of failure in data pipelines. Robust error handling and retry logic in input processing prevents entire workflows from failing due to transient issues."
diff --git a/application_sdk/inputs/__init__.py b/application_sdk/inputs/__init__.py
deleted file mode 100644
index b2fb30fd7..000000000
--- a/application_sdk/inputs/__init__.py
+++ /dev/null
@@ -1,168 +0,0 @@
-import os
-from abc import ABC, abstractmethod
-from typing import TYPE_CHECKING, AsyncIterator, Iterator, List, Union
-
-from application_sdk.activities.common.utils import (
-    find_local_files_by_extension,
-    get_object_store_prefix,
-)
-from application_sdk.common.error_codes import IOError
-from application_sdk.constants import TEMPORARY_PATH
-from application_sdk.observability.logger_adaptor import get_logger
-from application_sdk.services.objectstore import ObjectStore
-
-logger = get_logger(__name__)
-
-if TYPE_CHECKING:
-    import daft
-    import pandas as pd
-
-
-class Input(ABC):
-    """
-    Abstract base class for input data sources.
-    """
-
-    async def download_files(self) -> List[str]:
-        """Download files from object store if not available locally.
-
-        Flow:
-        1. Check if files exist locally at self.path
-        2. If not, try to download from object store
-        3. Filter by self.file_names if provided
-        4. Return list of file paths for logging purposes
-
-        Returns:
-            List[str]: List of file paths
-
-        Raises:
-            AttributeError: When the input class doesn't support file operations or _extension
-            IOError: When no files found locally or in object store
-        """
-        # Step 1: Check if files exist locally
-        local_files = find_local_files_by_extension(
-            self.path, self._EXTENSION, self.file_names
-        )
-        if local_files:
-            logger.info(
-                f"Found {len(local_files)} {self._EXTENSION} files locally at: {self.path}"
-            )
-            return local_files
-
-        # Step 2: Try to download from object store
-        logger.info(
-            f"No local {self._EXTENSION} files found at {self.path}, checking object store..."
-        )
-
-        try:
-            # Determine what to download based on path type and filters
-            downloaded_paths = []
-
-            if self.path.endswith(self._EXTENSION):
-                # Single file case (file_names validation already ensures this is valid)
-                source_path = get_object_store_prefix(self.path)
-                destination_path = os.path.join(TEMPORARY_PATH, source_path)
-                await ObjectStore.download_file(
-                    source=source_path,
-                    destination=destination_path,
-                )
-                downloaded_paths.append(destination_path)
-
-            elif self.file_names:
-                # Directory with specific files - download each file individually
-                for file_name in self.file_names:
-                    file_path = os.path.join(self.path, file_name)
-                    source_path = get_object_store_prefix(file_path)
-                    destination_path = os.path.join(TEMPORARY_PATH, source_path)
-                    await ObjectStore.download_file(
-                        source=source_path,
-                        destination=destination_path,
-                    )
-                    downloaded_paths.append(destination_path)
-            else:
-                # Download entire directory
-                source_path = get_object_store_prefix(self.path)
-                destination_path = os.path.join(TEMPORARY_PATH, source_path)
-                await ObjectStore.download_prefix(
-                    source=source_path,
-                    destination=destination_path,
-                )
-                # Find the actual files in the downloaded directory
-                found_files = find_local_files_by_extension(
-                    destination_path, self._EXTENSION, getattr(self, "file_names", None)
-                )
-                downloaded_paths.extend(found_files)
-
-            # Check results
-            if downloaded_paths:
-                logger.info(
-                    f"Successfully downloaded {len(downloaded_paths)} {self._EXTENSION} files from object store"
-                )
-                return downloaded_paths
-            else:
-                raise IOError(
-                    f"{IOError.OBJECT_STORE_READ_ERROR}: Downloaded from object store but no {self._EXTENSION} files found"
-                )
-
-        except Exception as e:
-            logger.error(f"Failed to download from object store: {str(e)}")
-            raise IOError(
-                f"{IOError.OBJECT_STORE_DOWNLOAD_ERROR}: No {self._EXTENSION} files found locally at '{self.path}' and failed to download from object store. "
-                f"Error: {str(e)}"
-            )
-
-    @abstractmethod
-    async def get_batched_dataframe(
-        self,
-    ) -> Union[Iterator["pd.DataFrame"], AsyncIterator["pd.DataFrame"]]:
-        """
-        Get an iterator of batched pandas DataFrames.
-
-        Returns:
-            Iterator["pd.DataFrame"]: An iterator of batched pandas DataFrames.
-
-        Raises:
-            NotImplementedError: If the method is not implemented.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    async def get_dataframe(self) -> "pd.DataFrame":
-        """
-        Get a single pandas DataFrame.
-
-        Returns:
-            "pd.DataFrame": A pandas DataFrame.
-
-        Raises:
-            NotImplementedError: If the method is not implemented.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    async def get_batched_daft_dataframe(
-        self,
-    ) -> Union[Iterator["daft.DataFrame"], AsyncIterator["daft.DataFrame"]]:  # noqa: F821
-        """
-        Get an iterator of batched daft DataFrames.
-
-        Returns:
-            Iterator[daft.DataFrame]: An iterator of batched daft DataFrames.
-
-        Raises:
-            NotImplementedError: If the method is not implemented.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    async def get_daft_dataframe(self) -> "daft.DataFrame":  # noqa: F821
-        """
-        Get a single daft DataFrame.
-
-        Returns:
-            daft.DataFrame: A daft DataFrame.
-
-        Raises:
-            NotImplementedError: If the method is not implemented.
-        """
-        raise NotImplementedError
diff --git a/application_sdk/inputs/iceberg.py b/application_sdk/inputs/iceberg.py
deleted file mode 100644
index 5ddb3f461..000000000
--- a/application_sdk/inputs/iceberg.py
+++ /dev/null
@@ -1,75 +0,0 @@
-from typing import TYPE_CHECKING, AsyncIterator, Iterator, Optional, Union
-
-from pyiceberg.table import Table
-
-from application_sdk.inputs import Input
-from application_sdk.observability.logger_adaptor import get_logger
-
-logger = get_logger(__name__)
-
-if TYPE_CHECKING:
-    import daft
-    import pandas as pd
-
-
-class IcebergInput(Input):
-    """
-    Iceberg Input class to read data from Iceberg tables using daft and pandas
-    """
-
-    table: Table
-    chunk_size: Optional[int]
-
-    def __init__(self, table: Table, chunk_size: Optional[int] = 100000):
-        """Initialize the Iceberg input class.
-
-        Args:
-            table (Table): Iceberg table object.
-            chunk_size (Optional[int], optional): Number of rows per batch.
-                Defaults to 100000.
-        """
-        self.table = table
-        self.chunk_size = chunk_size
-
-    async def get_dataframe(self) -> "pd.DataFrame":
-        """
-        Method to read the data from the iceberg table
-        and return as a single combined pandas dataframe
-        """
-        try:
-            daft_dataframe = await self.get_daft_dataframe()
-            return daft_dataframe.to_pandas()
-        except Exception as e:
-            logger.error(f"Error reading data from Iceberg table: {str(e)}")
-            raise
-
-    async def get_batched_dataframe(
-        self,
-    ) -> Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]:
-        # We are not implementing this method as we have to partition the daft dataframe
-        # using dataframe.into_partitions() method. This method does all the partitions in memory
-        # and using that can cause out of memory issues.
-        # ref: https://www.getdaft.io/projects/docs/en/stable/user_guide/poweruser/partitioning.html
-        raise NotImplementedError
-
-    async def get_daft_dataframe(self) -> "daft.DataFrame":  # noqa: F821
-        """
-        Method to read the data from the iceberg table
-        and return as a single combined daft dataframe
-        """
-        try:
-            import daft
-
-            return daft.read_iceberg(self.table)
-        except Exception as e:
-            logger.error(f"Error reading data from Iceberg table using daft: {str(e)}")
-            raise
-
-    async def get_batched_daft_dataframe(
-        self,
-    ) -> Union[AsyncIterator["daft.DataFrame"], Iterator["daft.DataFrame"]]:  # noqa: F821
-        # We are not implementing this method as we have to partition the daft dataframe
-        # using dataframe.into_partitions() method. This method does all the partitions in memory
-        # and using that can cause out of memory issues.
-        # ref: https://www.getdaft.io/projects/docs/en/stable/user_guide/poweruser/partitioning.html
-        raise NotImplementedError
diff --git a/application_sdk/inputs/json.py b/application_sdk/inputs/json.py
deleted file mode 100644
index cf4d5b624..000000000
--- a/application_sdk/inputs/json.py
+++ /dev/null
@@ -1,136 +0,0 @@
-from typing import TYPE_CHECKING, AsyncIterator, Iterator, List, Optional, Union
-
-from application_sdk.inputs import Input
-from application_sdk.observability.logger_adaptor import get_logger
-
-if TYPE_CHECKING:
-    import daft
-    import pandas as pd
-
-logger = get_logger(__name__)
-
-
-class JsonInput(Input):
-    """
-    JSON Input class to read data from JSON files using daft and pandas.
-    Supports reading both single files and directories containing multiple JSON files.
-    """
-
-    _EXTENSION = ".json"
-
-    def __init__(
-        self,
-        path: str,
-        file_names: Optional[List[str]] = None,
-        chunk_size: int = 100000,
-    ):
-        """Initialize the JsonInput class.
-
-        Args:
-            path (str): Path to JSON file or directory containing JSON files.
-                It accepts both types of paths:
-                local path or object store path
-                Wildcards are not supported.
-            file_names (Optional[List[str]]): List of specific file names to read. Defaults to None.
-            chunk_size (int): Number of rows per batch. Defaults to 100000.
-
-        Raises:
-            ValueError: When path is not provided or when single file path is combined with file_names
-        """
-
-        # Validate that single file path and file_names are not both specified
-        if path.endswith(self._EXTENSION) and file_names:
-            raise ValueError(
-                f"Cannot specify both a single file path ('{path}') and file_names filter. "
-                f"Either provide a directory path with file_names, or specify the exact file path without file_names."
-            )
-
-        self.path = path
-        self.chunk_size = chunk_size
-        self.file_names = file_names
-
-    async def get_batched_dataframe(
-        self,
-    ) -> Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]:
-        """
-        Method to read the data from the json files in the path
-        and return as a batched pandas dataframe
-        """
-        try:
-            import pandas as pd
-
-            # Ensure files are available (local or downloaded)
-            json_files = await self.download_files()
-            logger.info(f"Reading {len(json_files)} JSON files in batches")
-
-            for json_file in json_files:
-                json_reader_obj = pd.read_json(
-                    json_file,
-                    chunksize=self.chunk_size,
-                    lines=True,
-                )
-                for chunk in json_reader_obj:
-                    yield chunk
-        except Exception as e:
-            logger.error(f"Error reading batched data from JSON: {str(e)}")
-            raise
-
-    async def get_dataframe(self) -> "pd.DataFrame":
-        """
-        Method to read the data from the json files in the path
-        and return as a single combined pandas dataframe
-        """
-        try:
-            import pandas as pd
-
-            # Ensure files are available (local or downloaded)
-            json_files = await self.download_files()
-            logger.info(f"Reading {len(json_files)} JSON files as pandas dataframe")
-
-            return pd.concat(
-                (pd.read_json(json_file, lines=True) for json_file in json_files),
-                ignore_index=True,
-            )
-
-        except Exception as e:
-            logger.error(f"Error reading data from JSON: {str(e)}")
-            raise
-
-    async def get_batched_daft_dataframe(
-        self,
-    ) -> Union[AsyncIterator["daft.DataFrame"], Iterator["daft.DataFrame"]]:  # noqa: F821
-        """
-        Method to read the data from the json files in the path
-        and return as a batched daft dataframe
-        """
-        try:
-            import daft
-
-            # Ensure files are available (local or downloaded)
-            json_files = await self.download_files()
-            logger.info(f"Reading {len(json_files)} JSON files as daft batches")
-
-            # Yield each discovered file as separate batch with chunking
-            for json_file in json_files:
-                yield daft.read_json(json_file, _chunk_size=self.chunk_size)
-        except Exception as e:
-            logger.error(f"Error reading batched data from JSON using daft: {str(e)}")
-            raise
-
-    async def get_daft_dataframe(self) -> "daft.DataFrame":  # noqa: F821
-        """
-        Method to read the data from the json files in the path
-        and return as a single combined daft dataframe
-        """
-        try:
-            import daft
-
-            # Ensure files are available (local or downloaded)
-            json_files = await self.download_files()
-            logger.info(f"Reading {len(json_files)} JSON files with daft")
-
-            # Use the discovered/downloaded files directly
-            return daft.read_json(json_files)
-        except Exception as e:
-            logger.error(f"Error reading data from JSON using daft: {str(e)}")
-            raise
diff --git a/application_sdk/inputs/parquet.py b/application_sdk/inputs/parquet.py
deleted file mode 100644
index 3f15fd5a4..000000000
--- a/application_sdk/inputs/parquet.py
+++ /dev/null
@@ -1,272 +0,0 @@
-from typing import TYPE_CHECKING, AsyncIterator, Iterator, List, Optional, Union
-
-from application_sdk.inputs import Input
-from application_sdk.observability.logger_adaptor import get_logger
-
-logger = get_logger(__name__)
-
-if TYPE_CHECKING:
-    import daft  # type: ignore
-    import pandas as pd
-
-
-class ParquetInput(Input):
-    """
-    Parquet Input class to read data from Parquet files using daft and pandas.
-    Supports reading both single files and directories containing multiple parquet files.
-    """
-
-    _EXTENSION = ".parquet"
-
-    def __init__(
-        self,
-        path: str,
-        chunk_size: int = 100000,
-        buffer_size: int = 5000,
-        file_names: Optional[List[str]] = None,
-    ):
-        """Initialize the Parquet input class.
-
-        Args:
-            path (str): Path to parquet file or directory containing parquet files.
-                It accepts both types of paths:
-                local path or object store path
-                Wildcards are not supported.
-            chunk_size (int): Number of rows per batch. Defaults to 100000.
-            buffer_size (int): Number of rows per batch. Defaults to 5000.
-            file_names (Optional[List[str]]): List of file names to read. Defaults to None.
-
-        Raises:
-            ValueError: When path is not provided or when single file path is combined with file_names
-        """
-
-        # Validate that single file path and file_names are not both specified
-        if path.endswith(self._EXTENSION) and file_names:
-            raise ValueError(
-                f"Cannot specify both a single file path ('{path}') and file_names filter. "
-                f"Either provide a directory path with file_names, or specify the exact file path without file_names."
-            )
-
-        self.path = path
-        self.chunk_size = chunk_size
-        self.buffer_size = buffer_size
-        self.file_names = file_names
-
-    async def get_dataframe(self) -> "pd.DataFrame":
-        """Read data from parquet file(s) and return as pandas DataFrame.
-
-        Returns:
-            pd.DataFrame: Combined dataframe from specified parquet files
-
-        Raises:
-            ValueError: When no valid path can be determined or no matching files found
-            Exception: When reading parquet files fails
-
-        Example transformation:
-        Input files:
-        +------------------+
-        | file1.parquet    |
-        | file2.parquet    |
-        | file3.parquet    |
-        +------------------+
-
-        With file_names=["file1.parquet", "file3.parquet"]:
-        +-------+-------+-------+
-        | col1  | col2  | col3  |
-        +-------+-------+-------+
-        | val1  | val2  | val3  |  # from file1.parquet
-        | val7  | val8  | val9  |  # from file3.parquet
-        +-------+-------+-------+
-
-        Transformations:
-        - Only specified files are read and combined
-        - Column schemas must be compatible across files
-        - Only reads files in the specified directory
-        """
-        try:
-            import pandas as pd
-
-            # Ensure files are available (local or downloaded)
-            parquet_files = await self.download_files()
-            logger.info(f"Reading {len(parquet_files)} parquet files")
-
-            return pd.concat(
-                (pd.read_parquet(parquet_file) for parquet_file in parquet_files),
-                ignore_index=True,
-            )
-        except Exception as e:
-            logger.error(f"Error reading data from parquet file(s): {str(e)}")
-            raise
-
-    async def get_batched_dataframe(
-        self,
-    ) -> Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]:
-        """Read data from parquet file(s) in batches as pandas DataFrames.
-
-        Returns:
-            AsyncIterator[pd.DataFrame]: Async iterator of pandas dataframes
-
-        Raises:
-            ValueError: When no parquet files found locally or in object store
-            Exception: When reading parquet files fails
-
-        Example transformation:
-        Input files:
-        +------------------+
-        | file1.parquet    |
-        | file2.parquet    |
-        | file3.parquet    |
-        +------------------+
-
-        With file_names=["file1.parquet", "file2.parquet"] and chunk_size=2:
-        Batch 1:
-        +-------+-------+
-        | col1  | col2  |
-        +-------+-------+
-        | val1  | val2  |  # from file1.parquet
-        | val3  | val4  |  # from file1.parquet
-        +-------+-------+
-
-        Batch 2:
-        +-------+-------+
-        | col1  | col2  |
-        +-------+-------+
-        | val5  | val6  |  # from file2.parquet
-        | val7  | val8  |  # from file2.parquet
-        +-------+-------+
-
-        Transformations:
-        - Only specified files are combined then split into chunks
-        - Each batch is a separate DataFrame
-        - Only reads files in the specified directory
-        """
-        try:
-            import pandas as pd
-
-            # Ensure files are available (local or downloaded)
-            parquet_files = await self.download_files()
-            logger.info(f"Reading {len(parquet_files)} parquet files in batches")
-
-            # Process each file individually to maintain memory efficiency
-            for parquet_file in parquet_files:
-                df = pd.read_parquet(parquet_file)
-                for i in range(0, len(df), self.chunk_size):
-                    yield df.iloc[i : i + self.chunk_size]
-        except Exception as e:
-            logger.error(
-                f"Error reading data from parquet file(s) in batches: {str(e)}"
-            )
-            raise
-
-    async def get_daft_dataframe(self) -> "daft.DataFrame":  # noqa: F821
-        """Read data from parquet file(s) and return as daft DataFrame.
-
-        Returns:
-            daft.DataFrame: Combined daft dataframe from specified parquet files
-
-        Raises:
-            ValueError: When no parquet files found locally or in object store
-            Exception: When reading parquet files fails
-
-        Example transformation:
-        Input files:
-        +------------------+
-        | file1.parquet    |
-        | file2.parquet    |
-        | file3.parquet    |
-        +------------------+
-
-        With file_names=["file1.parquet", "file3.parquet"]:
-        +-------+-------+-------+
-        | col1  | col2  | col3  |
-        +-------+-------+-------+
-        | val1  | val2  | val3  |  # from file1.parquet
-        | val7  | val8  | val9  |  # from file3.parquet
-        +-------+-------+-------+
-
-        Transformations:
-        - Only specified parquet files combined into single daft DataFrame
-        - Lazy evaluation for better performance
-        - Column schemas must be compatible across files
-        """
-        try:
-            import daft  # type: ignore
-
-            # Ensure files are available (local or downloaded)
-            parquet_files = await self.download_files()
-            logger.info(f"Reading {len(parquet_files)} parquet files with daft")
-
-            # Use the discovered/downloaded files directly
-            return daft.read_parquet(parquet_files)
-        except Exception as e:
-            logger.error(
-                f"Error reading data from parquet file(s) using daft: {str(e)}"
-            )
-            raise
-
-    async def get_batched_daft_dataframe(self) -> AsyncIterator["daft.DataFrame"]:  # type: ignore
-        """Get batched daft dataframe from parquet file(s).
-
-        Returns:
-            AsyncIterator[daft.DataFrame]: An async iterator of daft DataFrames, each containing
-            a batch of data from individual parquet files
-
-        Raises:
-            ValueError: When no parquet files found locally or in object store
-            Exception: When reading parquet files fails
-
-        Example transformation:
-        Input files:
-        +------------------+
-        | file1.parquet    |
-        | file2.parquet    |
-        | file3.parquet    |
-        +------------------+
-
-        With file_names=["file1.parquet", "file3.parquet"]:
-        Batch 1 (file1.parquet):
-        +-------+-------+
-        | col1  | col2  |
-        +-------+-------+
-        | val1  | val2  |
-        | val3  | val4  |
-        +-------+-------+
-
-        Batch 2 (file3.parquet):
-        +-------+-------+
-        | col1  | col2  |
-        +-------+-------+
-        | val7  | val8  |
-        | val9  | val10 |
-        +-------+-------+
-
-        Transformations:
-        - Each specified file becomes a separate daft DataFrame batch
-        - Lazy evaluation for better performance
-        - Files processed individually for memory efficiency
-        """
-        try:
-            import daft  # type: ignore
-
-            # Ensure files are available (local or downloaded)
-            parquet_files = await self.download_files()
-            logger.info(f"Reading {len(parquet_files)} parquet files as daft batches")
-
-            # Create a lazy dataframe without loading data into memory
-            lazy_df = daft.read_parquet(parquet_files)
-
-            # Get total count efficiently
-            total_rows = lazy_df.count_rows()
-
-            # Yield chunks without loading everything into memory
-            for offset in range(0, total_rows, self.buffer_size):
-                chunk = lazy_df.offset(offset).limit(self.buffer_size)
-                yield chunk
-
-            del lazy_df
-
-        except Exception as error:
-            logger.error(
-                f"Error reading data from parquet file(s) in batches using daft: {error}"
-            )
-            raise
diff --git a/application_sdk/inputs/sql_query.py b/application_sdk/inputs/sql_query.py
deleted file mode 100644
index f1c52a5ca..000000000
--- a/application_sdk/inputs/sql_query.py
+++ /dev/null
@@ -1,271 +0,0 @@
-import asyncio
-import concurrent
-from typing import TYPE_CHECKING, AsyncIterator, Iterator, Optional, Union
-
-from application_sdk.inputs import Input
-from application_sdk.observability.logger_adaptor import get_logger
-
-logger = get_logger(__name__)
-
-if TYPE_CHECKING:
-    import daft
-    import pandas as pd
-    from sqlalchemy.engine import Engine
-    from sqlalchemy.orm import Session
-
-
-class SQLQueryInput(Input):
-    """Input handler for SQL queries.
-
-    This class provides asynchronous functionality to execute SQL queries and return
-    results as DataFrames, with support for both pandas and daft formats.
-
-    Attributes:
-        query (str): The SQL query to execute.
-        engine (Union[Engine, str]): SQLAlchemy engine or connection string.
-        chunk_size (Optional[int]): Number of rows to fetch per batch.
-    """
-
-    query: str
-    engine: Union["Engine", str]
-    chunk_size: Optional[int]
-
-    def __init__(
-        self,
-        query: str,
-        engine: Union["Engine", str],
-        chunk_size: Optional[int] = 5000,
-    ):
-        """Initialize the async SQL query input handler.
-
-        Args:
-            engine (Union[Engine, str]): SQLAlchemy engine or connection string.
-            query (str): The SQL query to execute.
-            chunk_size (Optional[int], optional): Number of rows per batch.
-                Defaults to 5000.
-        """
-        self.query = query
-        self.engine = engine
-        self.chunk_size = chunk_size
-        self.engine = engine
-
-    def _execute_pandas_query(
-        self, conn
-    ) -> Union["pd.DataFrame", Iterator["pd.DataFrame"]]:
-        """Helper function to execute SQL query using pandas.
-           The function is responsible for using import_optional_dependency method of the pandas library to import sqlalchemy
-           This function helps pandas in determining weather to use the sqlalchemy connection object and constructs like text()
-           or use the underlying database connection object. This has been done to make sure connectors like the Redshift connector,
-           which do not support the sqlalchemy connection object, can be made compatible with the application-sdk.
-
-        Args:
-            conn: Database connection object.
-
-        Returns:
-            Union["pd.DataFrame", Iterator["pd.DataFrame"]]: Query results as DataFrame
-                or iterator of DataFrames if chunked.
-        """
-        import pandas as pd
-        from pandas.compat._optional import import_optional_dependency
-        from sqlalchemy import text
-
-        if import_optional_dependency("sqlalchemy", errors="ignore"):
-            return pd.read_sql_query(text(self.query), conn, chunksize=self.chunk_size)
-        else:
-            dbapi_conn = getattr(conn, "connection", None)
-            return pd.read_sql_query(self.query, dbapi_conn, chunksize=self.chunk_size)
-
-    def _read_sql_query(
-        self, session: "Session"
-    ) -> Union["pd.DataFrame", Iterator["pd.DataFrame"]]:
-        """Execute SQL query using the provided session.
-
-        Args:
-            session: SQLAlchemy session for database operations.
-
-        Returns:
-            Union["pd.DataFrame", Iterator["pd.DataFrame"]]: Query results as DataFrame
-                or iterator of DataFrames if chunked.
-        """
-        conn = session.connection()
-        return self._execute_pandas_query(conn)
-
-    def _execute_query_daft(
-        self,
-    ) -> Union["daft.DataFrame", Iterator["daft.DataFrame"]]:
-        """Execute SQL query using the provided engine and daft.
-
-        Returns:
-            Union["daft.DataFrame", Iterator["daft.DataFrame"]]: Query results as DataFrame
-                or iterator of DataFrames if chunked.
-        """
-        # Daft uses ConnectorX to read data from SQL by default for supported connectors
-        # If a connection string is passed, it will use ConnectorX to read data
-        # For unsupported connectors and if directly engine is passed, it will use SQLAlchemy
-        import daft
-
-        if isinstance(self.engine, str):
-            return daft.read_sql(
-                self.query, self.engine, infer_schema_length=self.chunk_size
-            )
-        return daft.read_sql(
-            self.query, self.engine.connect, infer_schema_length=self.chunk_size
-        )
-
-    def _execute_query(self) -> Union["pd.DataFrame", Iterator["pd.DataFrame"]]:
-        """Execute SQL query using the provided engine and pandas.
-
-        Returns:
-            Union["pd.DataFrame", Iterator["pd.DataFrame"]]: Query results as DataFrame
-                or iterator of DataFrames if chunked.
-        """
-        with self.engine.connect() as conn:
-            return self._execute_pandas_query(conn)
-
-    async def get_batched_dataframe(
-        self,
-    ) -> Union[AsyncIterator["pd.DataFrame"], Iterator["pd.DataFrame"]]:  # type: ignore
-        """Get query results as batched pandas DataFrames asynchronously.
-
-        Returns:
-            AsyncIterator["pd.DataFrame"]: Async iterator yielding batches of query results.
-
-        Raises:
-            ValueError: If engine is a string instead of SQLAlchemy engine.
-            Exception: If there's an error executing the query.
-        """
-        try:
-            if isinstance(self.engine, str):
-                raise ValueError("Engine should be an SQLAlchemy engine object")
-
-            from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession
-
-            async_session = None
-            if self.engine and isinstance(self.engine, AsyncEngine):
-                from sqlalchemy.orm import sessionmaker
-
-                async_session = sessionmaker(
-                    self.engine, expire_on_commit=False, class_=AsyncSession
-                )
-
-            if async_session:
-                async with async_session() as session:
-                    return await session.run_sync(self._read_sql_query)
-            else:
-                # Run the blocking operation in a thread pool
-                with concurrent.futures.ThreadPoolExecutor() as executor:
-                    return await asyncio.get_event_loop().run_in_executor(  # type: ignore
-                        executor, self._execute_query
-                    )
-        except Exception as e:
-            logger.error(f"Error reading batched data(pandas) from SQL: {str(e)}")
-
-    async def get_dataframe(self) -> "pd.DataFrame":
-        """Get all query results as a single pandas DataFrame asynchronously.
-
-        Returns:
-            pd.DataFrame: Query results as a DataFrame.
-
-        Raises:
-            ValueError: If engine is a string instead of SQLAlchemy engine.
-            Exception: If there's an error executing the query.
-        """
-        try:
-            if isinstance(self.engine, str):
-                raise ValueError("Engine should be an SQLAlchemy engine object")
-
-            from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession
-
-            async_session = None
-            if self.engine and isinstance(self.engine, AsyncEngine):
-                from sqlalchemy.orm import sessionmaker
-
-                async_session = sessionmaker(
-                    self.engine, expire_on_commit=False, class_=AsyncSession
-                )
-
-            if async_session:
-                async with async_session() as session:
-                    return await session.run_sync(self._read_sql_query)
-            else:
-                # Run the blocking operation in a thread pool
-                with concurrent.futures.ThreadPoolExecutor() as executor:
-                    result = await asyncio.get_event_loop().run_in_executor(
-                        executor, self._execute_query
-                    )
-                    import pandas as pd
-
-                    if isinstance(result, pd.DataFrame):
-                        return result
-                    raise Exception(
-                        "Unable to get pandas dataframe from SQL query results"
-                    )
-
-        except Exception as e:
-            logger.error(f"Error reading data(pandas) from SQL: {str(e)}")
-            raise e
-
-    async def get_daft_dataframe(self) -> "daft.DataFrame":  # noqa: F821
-        """Get query results as a daft DataFrame.
-
-        This method uses ConnectorX to read data from SQL for supported connectors.
-        For unsupported connectors and direct engine usage, it falls back to SQLAlchemy.
-
-        Returns:
-            daft.DataFrame: Query results as a daft DataFrame.
-
-        Raises:
-            ValueError: If engine is a string instead of SQLAlchemy engine.
-            Exception: If there's an error executing the query.
-
-        Note:
-            For ConnectorX supported sources, see:
-            https://sfu-db.github.io/connector-x/intro.html#sources
-        """
-        try:
-            import daft
-
-            # Run the blocking operation in a thread pool
-            with concurrent.futures.ThreadPoolExecutor() as executor:
-                result = await asyncio.get_event_loop().run_in_executor(
-                    executor, self._execute_query_daft
-                )
-                if isinstance(result, daft.DataFrame):
-                    return result
-                raise
-        except Exception as e:
-            logger.error(f"Error reading data(daft) from SQL: {str(e)}")
-            raise
-
-    async def get_batched_daft_dataframe(
-        self,
-    ) -> Union[AsyncIterator["daft.DataFrame"], Iterator["daft.DataFrame"]]:  # noqa: F821
-        """Get query results as batched daft DataFrames.
-
-        This method reads data using pandas in batches since daft does not support
-        batch reading. Each pandas DataFrame is then converted to a daft DataFrame.
-
-        Returns:
-            AsyncIterator[daft.DataFrame]: Async iterator yielding batches of query results
-                as daft DataFrames.
-
-        Raises:
-            ValueError: If engine is a string instead of SQLAlchemy engine.
-            Exception: If there's an error executing the query.
-
-        Note:
-            This method uses pandas for batch reading since daft does not support
-            reading data in batches natively.
-        """
-        try:
-            import daft
-
-            if isinstance(self.engine, str):
-                raise ValueError("Engine should be an SQLAlchemy engine object")
-
-            # Use async for to consume the AsyncIterator properly
-            async for dataframe in self.get_batched_dataframe():
-                daft_dataframe = daft.from_pandas(dataframe)
-                yield daft_dataframe
-        except Exception as e:
-            logger.error(f"Error reading batched data(daft) from SQL: {str(e)}")
diff --git a/application_sdk/interceptors/events.py b/application_sdk/interceptors/events.py
index 95d2fcb36..8dc844240 100644
--- a/application_sdk/interceptors/events.py
+++ b/application_sdk/interceptors/events.py
@@ -12,7 +12,7 @@
     WorkflowInterceptorClassInput,
 )

-from application_sdk.events.models import (
+from application_sdk.interceptors.models import (
     ApplicationEventNames,
     Event,
     EventMetadata,
diff --git a/application_sdk/events/models.py b/application_sdk/interceptors/models.py
similarity index 100%
rename from application_sdk/events/models.py
rename to application_sdk/interceptors/models.py
diff --git a/application_sdk/outputs/__init__.py b/application_sdk/io/__init__.py
similarity index 72%
rename from application_sdk/outputs/__init__.py
rename to application_sdk/io/__init__.py
index ce3b17d23..e595897c9 100644
--- a/application_sdk/outputs/__init__.py
+++ b/application_sdk/io/__init__.py
@@ -13,8 +13,10 @@
     TYPE_CHECKING,
     Any,
     AsyncGenerator,
+    AsyncIterator,
     Dict,
     Generator,
+    Iterator,
     List,
     Optional,
     Union,
@@ -26,8 +28,13 @@

 from application_sdk.activities.common.models import ActivityStatistics
 from application_sdk.activities.common.utils import get_object_store_prefix
-from application_sdk.common.dataframe_utils import is_empty_dataframe
+from application_sdk.common.types import DataframeType
 from application_sdk.constants import ENABLE_ATLAN_UPLOAD, UPSTREAM_OBJECT_STORE_NAME
+from application_sdk.io._utils import (
+    estimate_dataframe_record_size,
+    is_empty_dataframe,
+    path_gen,
+)
 from application_sdk.observability.logger_adaptor import get_logger
 from application_sdk.observability.metrics_adaptor import MetricType
 from application_sdk.services.objectstore import ObjectStore
@@ -41,6 +48,45 @@
     import pandas as pd


+class Reader(ABC):
+    """
+    Abstract base class for reader data sources.
+    """
+
+    @abstractmethod
+    def read_batches(
+        self,
+    ) -> Union[
+        Iterator["pd.DataFrame"],
+        AsyncIterator["pd.DataFrame"],
+        Iterator["daft.DataFrame"],
+        AsyncIterator["daft.DataFrame"],
+    ]:
+        """
+        Get an iterator of batched pandas DataFrames.
+
+        Returns:
+            Iterator["pd.DataFrame"]: An iterator of batched pandas DataFrames.
+
+        Raises:
+            NotImplementedError: If the method is not implemented.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    async def read(self) -> Union["pd.DataFrame", "daft.DataFrame"]:
+        """
+        Get a single pandas or daft DataFrame.
+
+        Returns:
+            Union["pd.DataFrame", "daft.DataFrame"]: A pandas or daft DataFrame.
+
+        Raises:
+            NotImplementedError: If the method is not implemented.
+        """
+        raise NotImplementedError
+
+
 class WriteMode(Enum):
     """Enumeration of write modes for output operations."""

@@ -49,17 +95,22 @@ class WriteMode(Enum):
     OVERWRITE_PARTITIONS = "overwrite-partitions"


-class Output(ABC):
-    """Abstract base class for output handlers.
+class Writer(ABC):
+    """Abstract base class for writer handlers.

-    This class defines the interface for output handlers that can write data
+    This class defines the interface for writer handlers that can write data
     to various destinations in different formats.

     Attributes:
-        output_path (str): Path where the output will be written.
+        output_path (str): Path where the writer will be written.
         upload_file_prefix (str): Prefix for files when uploading to object store.
         total_record_count (int): Total number of records processed.
-        chunk_count (int): Number of chunks the output was split into.
+        chunk_count (int): Number of chunks the writer was split into.
+        buffer_size (int): Size of the buffer to write data to.
+        max_file_size_bytes (int): Maximum size of the file to write data to.
+        current_buffer_size (int): Current size of the buffer to write data to.
+        current_buffer_size_bytes (int): Current size of the buffer to write data to.
+        partitions (List[int]): Partitions of the writer.
     """

     output_path: str
@@ -72,96 +123,40 @@ class Output(ABC):
     current_buffer_size: int
     current_buffer_size_bytes: int
     partitions: List[int]
+    extension: str
+    dataframe_type: DataframeType

-    def estimate_dataframe_record_size(self, dataframe: "pd.DataFrame") -> int:
-        """Estimate File size of a DataFrame by sampling a few records."""
-        if len(dataframe) == 0:
-            return 0
-
-        # Sample up to 10 records to estimate average size
-        sample_size = min(10, len(dataframe))
-        sample = dataframe.head(sample_size)
-        file_type = type(self).__name__.lower().replace("output", "")
-        compression_factor = 1
-        if file_type == "json":
-            sample_file = sample.to_json(orient="records", lines=True)
-        else:
-            sample_file = sample.to_parquet(index=False, compression="snappy")
-            compression_factor = 0.01
-        if sample_file is not None:
-            avg_record_size = len(sample_file) / sample_size * compression_factor
-            return int(avg_record_size)
-
-        return 0
-
-    def path_gen(
-        self,
-        chunk_count: Optional[int] = None,
-        chunk_part: int = 0,
-        start_marker: Optional[str] = None,
-        end_marker: Optional[str] = None,
-    ) -> str:
-        """Generate a file path for a chunk.
-
-        Args:
-            chunk_start (Optional[int]): Starting index of the chunk, or None for single chunk.
-            chunk_count (int): Total number of chunks.
-            start_marker (Optional[str]): Start marker for query extraction.
-            end_marker (Optional[str]): End marker for query extraction.
-
-        Returns:
-            str: Generated file path for the chunk.
+    async def write(self, dataframe: Union["pd.DataFrame", "daft.DataFrame"], **kwargs):
         """
-        # For Query Extraction - use start and end markers without chunk count
-        if start_marker and end_marker:
-            return f"{start_marker}_{end_marker}{self._EXTENSION}"
-
-        # For regular chunking - include chunk count
-        if chunk_count is None:
-            return f"{str(chunk_part)}{self._EXTENSION}"
+        Method to write the pandas dataframe to an iceberg table
+        """
+        if self.dataframe_type == DataframeType.pandas:
+            await self._write_dataframe(dataframe, **kwargs)
+        elif self.dataframe_type == DataframeType.daft:
+            await self._write_daft_dataframe(dataframe, **kwargs)
         else:
-            return f"chunk-{str(chunk_count)}-part{str(chunk_part)}{self._EXTENSION}"
+            raise ValueError(f"Unsupported dataframe_type: {self.dataframe_type}")

-    def process_null_fields(
+    async def write_batches(
         self,
-        obj: Any,
-        preserve_fields: Optional[List[str]] = None,
-        null_to_empty_dict_fields: Optional[List[str]] = None,
-    ) -> Any:
+        dataframe: Union[
+            AsyncGenerator["pd.DataFrame", None],
+            Generator["pd.DataFrame", None, None],
+            AsyncGenerator["daft.DataFrame", None],
+            Generator["daft.DataFrame", None, None],
+        ],
+    ):
         """
-        By default the method removes null values from dictionaries and lists.
-        Except for the fields specified in preserve_fields.
-        And fields in null_to_empty_dict_fields are replaced with empty dict if null.
-
-        Args:
-            obj: The object to clean (dict, list, or other value)
-            preserve_fields: Optional list of field names that should be preserved even if they contain null values
-            null_to_empty_dict_fields: Optional list of field names that should be replaced with empty dict if null
-
-        Returns:
-            The cleaned object with null values removed
+        Method to write the pandas dataframe to an iceberg table
         """
-        if isinstance(obj, dict):
-            result = {}
-            for k, v in obj.items():
-                # Handle null fields that should be converted to empty dicts
-                if k in (null_to_empty_dict_fields or []) and v is None:
-                    result[k] = {}
-                    continue
-
-                # Process the value recursively
-                processed_value = self.process_null_fields(
-                    v, preserve_fields, null_to_empty_dict_fields
-                )
-
-                # Keep the field if it's in preserve_fields or has a non-None processed value
-                if k in (preserve_fields or []) or processed_value is not None:
-                    result[k] = processed_value
-
-            return result
-        return obj
+        if self.dataframe_type == DataframeType.pandas:
+            await self._write_batched_dataframe(dataframe)
+        elif self.dataframe_type == DataframeType.daft:
+            await self._write_batched_daft_dataframe(dataframe)
+        else:
+            raise ValueError(f"Unsupported dataframe_type: {self.dataframe_type}")

-    async def write_batched_dataframe(
+    async def _write_batched_dataframe(
         self,
         batched_dataframe: Union[
             AsyncGenerator["pd.DataFrame", None], Generator["pd.DataFrame", None, None]
@@ -182,7 +177,7 @@ async def write_batched_dataframe(
             if inspect.isasyncgen(batched_dataframe):
                 async for dataframe in batched_dataframe:
                     if not is_empty_dataframe(dataframe):
-                        await self.write_dataframe(dataframe)
+                        await self._write_dataframe(dataframe)
             else:
                 # Cast to Generator since we've confirmed it's not an AsyncGenerator
                 sync_generator = cast(
@@ -190,16 +185,17 @@ async def write_batched_dataframe(
                 )
                 for dataframe in sync_generator:
                     if not is_empty_dataframe(dataframe):
-                        await self.write_dataframe(dataframe)
+                        await self._write_dataframe(dataframe)
         except Exception as e:
             logger.error(f"Error writing batched dataframe: {str(e)}")
             raise

-    async def write_dataframe(self, dataframe: "pd.DataFrame"):
+    async def _write_dataframe(self, dataframe: "pd.DataFrame", **kwargs):
         """Write a pandas DataFrame to Parquet files and upload to object store.

         Args:
             dataframe (pd.DataFrame): The DataFrame to write.
+            **kwargs: Additional parameters (currently unused for pandas DataFrames).
         """
         try:
             if self.chunk_start is None:
@@ -207,7 +203,7 @@ async def write_dataframe(self, dataframe: "pd.DataFrame"):
             if len(dataframe) == 0:
                 return

-            chunk_size_bytes = self.estimate_dataframe_record_size(dataframe)
+            chunk_size_bytes = estimate_dataframe_record_size(dataframe, self.extension)

             for i in range(0, len(dataframe), self.buffer_size):
                 chunk = dataframe[i : i + self.buffer_size]
@@ -216,7 +212,7 @@ async def write_dataframe(self, dataframe: "pd.DataFrame"):
                     self.current_buffer_size_bytes + chunk_size_bytes
                     > self.max_file_size_bytes
                 ):
-                    output_file_name = f"{self.output_path}/{self.path_gen(self.chunk_count, self.chunk_part)}"
+                    output_file_name = f"{self.output_path}/{path_gen(self.chunk_count, self.chunk_part, extension=self.extension)}"
                     if os.path.exists(output_file_name):
                         await self._upload_file(output_file_name)
                         self.chunk_part += 1
@@ -230,7 +226,7 @@ async def write_dataframe(self, dataframe: "pd.DataFrame"):

             if self.current_buffer_size_bytes > 0:
                 # Finally upload the final file to the object store
-                output_file_name = f"{self.output_path}/{self.path_gen(self.chunk_count, self.chunk_part)}"
+                output_file_name = f"{self.output_path}/{path_gen(self.chunk_count, self.chunk_part, extension=self.extension)}"
                 if os.path.exists(output_file_name):
                     await self._upload_file(output_file_name)
                     self.chunk_part += 1
@@ -274,7 +270,7 @@ async def write_dataframe(self, dataframe: "pd.DataFrame"):
             logger.error(f"Error writing pandas dataframe to files: {str(e)}")
             raise

-    async def write_batched_daft_dataframe(
+    async def _write_batched_daft_dataframe(
         self,
         batched_dataframe: Union[
             AsyncGenerator["daft.DataFrame", None],  # noqa: F821
@@ -296,7 +292,7 @@ async def write_batched_daft_dataframe(
             if inspect.isasyncgen(batched_dataframe):
                 async for dataframe in batched_dataframe:
                     if not is_empty_dataframe(dataframe):
-                        await self.write_daft_dataframe(dataframe)
+                        await self._write_daft_dataframe(dataframe)
             else:
                 # Cast to Generator since we've confirmed it's not an AsyncGenerator
                 sync_generator = cast(
@@ -304,16 +300,18 @@ async def write_batched_daft_dataframe(
                 )  # noqa: F821
                 for dataframe in sync_generator:
                     if not is_empty_dataframe(dataframe):
-                        await self.write_daft_dataframe(dataframe)
+                        await self._write_daft_dataframe(dataframe)
         except Exception as e:
             logger.error(f"Error writing batched daft dataframe: {str(e)}")
+            raise

     @abstractmethod
-    async def write_daft_dataframe(self, dataframe: "daft.DataFrame"):  # noqa: F821
+    async def _write_daft_dataframe(self, dataframe: "daft.DataFrame", **kwargs):  # noqa: F821
         """Write a daft DataFrame to the output destination.

         Args:
             dataframe (daft.DataFrame): The DataFrame to write.
+            **kwargs: Additional parameters passed through from write().
         """
         pass

@@ -371,10 +369,8 @@ async def _flush_buffer(self, chunk: "pd.DataFrame", chunk_part: int):
         try:
             if not is_empty_dataframe(chunk):
                 self.total_record_count += len(chunk)
-                output_file_name = (
-                    f"{self.output_path}/{self.path_gen(self.chunk_count, chunk_part)}"
-                )
-                await self.write_chunk(chunk, output_file_name)
+                output_file_name = f"{self.output_path}/{path_gen(self.chunk_count, chunk_part, extension=self.extension)}"
+                await self._write_chunk(chunk, output_file_name)

                 self.current_buffer_size = 0

@@ -451,3 +447,4 @@ async def write_statistics(
             return statistics
         except Exception as e:
             logger.error(f"Error writing statistics: {str(e)}")
+            raise
diff --git a/application_sdk/io/_utils.py b/application_sdk/io/_utils.py
new file mode 100644
index 000000000..badeab345
--- /dev/null
+++ b/application_sdk/io/_utils.py
@@ -0,0 +1,307 @@
+import glob
+import os
+from datetime import datetime
+from typing import TYPE_CHECKING, Any, List, Optional, Union
+
+from application_sdk.activities.common.utils import get_object_store_prefix
+from application_sdk.common.error_codes import IOError
+from application_sdk.constants import TEMPORARY_PATH
+from application_sdk.observability.logger_adaptor import get_logger
+from application_sdk.services.objectstore import ObjectStore
+
+JSON_FILE_EXTENSION = ".json"
+PARQUET_FILE_EXTENSION = ".parquet"
+
+if TYPE_CHECKING:
+    import daft
+    import pandas as pd
+
+
+logger = get_logger(__name__)
+
+
+def find_local_files_by_extension(
+    path: str,
+    extension: str,
+    file_names: Optional[List[str]] = None,
+) -> List[str]:
+    """Find local files at the specified local path, optionally filtering by file names.
+
+    Args:
+        path (str): Local path to search in (file or directory)
+        extension (str): File extension to filter by (e.g., '.parquet', '.json')
+        file_names (Optional[List[str]]): List of file names (basenames) to filter by, paths are not supported
+
+    Returns:
+        List[str]: List of matching file paths
+
+    Example:
+        >>> find_local_files_by_extension("/data", ".parquet", ["file1.parquet", "file2.parquet"])
+        ['file1.parquet', 'file2.parquet']
+
+        >>> find_local_files_by_extension("/data/single.json", ".json")
+        ['single.json']
+    """
+    if os.path.isfile(path) and path.endswith(extension):
+        # Single file - return it directly
+        return [path]
+
+    elif os.path.isdir(path):
+        # Directory - find all files in directory
+        all_files = glob.glob(
+            os.path.join(path, "**", f"*{extension}"),
+            recursive=True,
+        )
+
+        # Filter by file names if specified
+        if file_names:
+            file_names_set = set(file_names)  # Convert to set for O(1) lookup
+            return [f for f in all_files if os.path.basename(f) in file_names_set]
+        else:
+            return all_files
+
+    return []
+
+
+async def download_files(
+    path: str, file_extension: str, file_names: Optional[List[str]] = None
+) -> List[str]:
+    """Download files from object store if not available locally.
+
+    Flow:
+    1. Check if files exist locally at self.path
+    2. If not, try to download from object store
+    3. Filter by self.file_names if provided
+    4. Return list of file paths for logging purposes
+
+    Returns:
+        List[str]: List of file paths
+
+    Raises:
+        AttributeError: When the reader class doesn't support file operations or _extension
+        IOError: When no files found locally or in object store
+    """
+    # Step 1: Check if files exist locally
+    local_files: List[str] = find_local_files_by_extension(
+        path, file_extension, file_names
+    )
+    if local_files:
+        logger.info(
+            f"Found {len(local_files)} {file_extension} files locally at: {path}"
+        )
+        return local_files
+
+    # Step 2: Try to download from object store
+    logger.info(
+        f"No local {file_extension} files found at {path}, checking object store..."
+    )
+
+    try:
+        # Determine what to download based on path type and filters
+        downloaded_paths: List[str] = []
+
+        if path.endswith(file_extension):
+            # Single file case (file_names validation already ensures this is valid)
+            source_path = get_object_store_prefix(path)
+            destination_path = os.path.join(TEMPORARY_PATH, source_path)
+            await ObjectStore.download_file(
+                source=source_path,
+                destination=destination_path,
+            )
+            downloaded_paths.append(destination_path)
+
+        elif file_names:
+            # Directory with specific files - download each file individually
+            for file_name in file_names:
+                file_path = os.path.join(path, file_name)
+                source_path = get_object_store_prefix(file_path)
+                destination_path = os.path.join(TEMPORARY_PATH, source_path)
+                await ObjectStore.download_file(
+                    source=source_path,
+                    destination=destination_path,
+                )
+                downloaded_paths.append(destination_path)
+        else:
+            # Download entire directory
+            source_path = get_object_store_prefix(path)
+            destination_path = os.path.join(TEMPORARY_PATH, source_path)
+            await ObjectStore.download_prefix(
+                source=source_path,
+                destination=destination_path,
+            )
+            # Find the actual files in the downloaded directory
+            found_files = find_local_files_by_extension(
+                destination_path, file_extension, file_names
+            )
+            downloaded_paths.extend(found_files)
+
+        # Check results
+        if downloaded_paths:
+            logger.info(
+                f"Successfully downloaded {len(downloaded_paths)} {file_extension} files from object store"
+            )
+            return downloaded_paths
+        else:
+            raise IOError(
+                f"{IOError.OBJECT_STORE_READ_ERROR}: Downloaded from object store but no {file_extension} files found"
+            )
+
+    except Exception as e:
+        logger.error(f"Failed to download from object store: {str(e)}")
+        raise IOError(
+            f"{IOError.OBJECT_STORE_DOWNLOAD_ERROR}: No {file_extension} files found locally at '{path}' and failed to download from object store. "
+            f"Error: {str(e)}"
+        )
+
+
+def estimate_dataframe_record_size(
+    dataframe: "pd.DataFrame", file_extension: str
+) -> int:
+    """Estimate File size of a DataFrame by sampling a few records.
+
+    Args:
+        dataframe (pd.DataFrame): The DataFrame to estimate the size of.
+        file_extension (str): The extension of the file to estimate the size of.
+
+    Returns:
+        int: The estimated size of the DataFrame in bytes.
+    """
+    if len(dataframe) == 0:
+        return 0
+
+    # Sample up to 10 records to estimate average size
+    sample_size = min(10, len(dataframe))
+    sample = dataframe.head(sample_size)
+    compression_factor = 1
+    if file_extension == JSON_FILE_EXTENSION:
+        sample_file = sample.to_json(orient="records", lines=True)
+    elif file_extension == PARQUET_FILE_EXTENSION:
+        sample_file = sample.to_parquet(index=False, compression="snappy")
+        compression_factor = 0.01
+    else:
+        raise ValueError(f"Unsupported file extension: {file_extension}")
+
+    if sample_file is not None:
+        avg_record_size = len(sample_file) / sample_size * compression_factor
+        return int(avg_record_size)
+
+    return 0
+
+
+def path_gen(
+    chunk_count: Optional[int] = None,
+    chunk_part: int = 0,
+    start_marker: Optional[str] = None,
+    end_marker: Optional[str] = None,
+    extension: str = ".json",
+) -> str:
+    """Generate a file path for a chunk.
+
+    Args:
+        chunk_start (Optional[int]): Starting index of the chunk, or None for single chunk.
+        chunk_count (int): Total number of chunks.
+        start_marker (Optional[str]): Start marker for query extraction.
+        end_marker (Optional[str]): End marker for query extraction.
+
+    Returns:
+        str: Generated file path for the chunk.
+    """
+    # For Query Extraction - use start and end markers without chunk count
+    if start_marker and end_marker:
+        return f"{start_marker}_{end_marker}{extension}"
+
+    # For regular chunking - include chunk count
+    if chunk_count is None:
+        return f"{str(chunk_part)}{extension}"
+    else:
+        return f"chunk-{str(chunk_count)}-part{str(chunk_part)}{extension}"
+
+
+def process_null_fields(
+    obj: Any,
+    preserve_fields: Optional[List[str]] = None,
+    null_to_empty_dict_fields: Optional[List[str]] = None,
+) -> Any:
+    """
+    By default the method removes null values from dictionaries and lists.
+    Except for the fields specified in preserve_fields.
+    And fields in null_to_empty_dict_fields are replaced with empty dict if null.
+
+    Args:
+        obj: The object to clean (dict, list, or other value)
+        preserve_fields: Optional list of field names that should be preserved even if they contain null values
+        null_to_empty_dict_fields: Optional list of field names that should be replaced with empty dict if null
+
+    Returns:
+        The cleaned object with null values removed
+    """
+    if isinstance(obj, dict):
+        result = {}
+        for k, v in obj.items():
+            # Handle null fields that should be converted to empty dicts
+            if k in (null_to_empty_dict_fields or []) and v is None:
+                result[k] = {}
+                continue
+
+            # Process the value recursively
+            processed_value = process_null_fields(
+                v, preserve_fields, null_to_empty_dict_fields
+            )
+
+            # Keep the field if it's in preserve_fields or has a non-None processed value
+            if k in (preserve_fields or []) or processed_value is not None:
+                result[k] = processed_value
+
+        return result
+    return obj
+
+
+def convert_datetime_to_epoch(data: Any) -> Any:
+    """Convert datetime objects to epoch timestamps in milliseconds.
+
+    Args:
+        data: The data to convert
+
+    Returns:
+        The converted data with datetime fields as epoch timestamps
+    """
+    if isinstance(data, datetime):
+        return int(data.timestamp() * 1000)
+    elif isinstance(data, dict):
+        return {k: convert_datetime_to_epoch(v) for k, v in data.items()}
+    elif isinstance(data, list):
+        return [convert_datetime_to_epoch(item) for item in data]
+    return data
+
+
+def is_empty_dataframe(dataframe: Union["pd.DataFrame", "daft.DataFrame"]) -> bool:  # noqa: F821
+    """Check if a DataFrame is empty.
+
+    This function determines whether a DataFrame has any rows, supporting both
+    pandas and daft DataFrame types. For pandas DataFrames, it uses the `empty`
+    property, and for daft DataFrames, it checks if the row count is 0.
+
+    Args:
+        dataframe (Union[pd.DataFrame, daft.DataFrame]): The DataFrame to check,
+            can be either a pandas DataFrame or a daft DataFrame.
+
+    Returns:
+        bool: True if the DataFrame has no rows, False otherwise.
+
+    Note:
+        If daft is not available and a daft DataFrame is passed, the function
+        will log a warning and return True.
+    """
+    import pandas as pd
+
+    if isinstance(dataframe, pd.DataFrame):
+        return dataframe.empty
+
+    try:
+        import daft
+
+        if isinstance(dataframe, daft.DataFrame):
+            return dataframe.count_rows() == 0
+    except Exception:
+        logger.warning("Module daft not found")
+    return True
diff --git a/application_sdk/outputs/iceberg.py b/application_sdk/io/iceberg.py
similarity index 60%
rename from application_sdk/outputs/iceberg.py
rename to application_sdk/io/iceberg.py
index 19a6d606e..5507f18c5 100644
--- a/application_sdk/outputs/iceberg.py
+++ b/application_sdk/io/iceberg.py
@@ -1,12 +1,12 @@
-from typing import TYPE_CHECKING, Union
+from typing import TYPE_CHECKING, AsyncIterator, Optional, Union

 from pyiceberg.catalog import Catalog
 from pyiceberg.table import Table
 from temporalio import activity

+from application_sdk.io import DataframeType, Reader, Writer
 from application_sdk.observability.logger_adaptor import get_logger
 from application_sdk.observability.metrics_adaptor import MetricType, get_metrics
-from application_sdk.outputs import Output

 logger = get_logger(__name__)
 activity.logger = logger
@@ -16,9 +16,84 @@
     import pandas as pd


-class IcebergOutput(Output):
+class IcebergTableReader(Reader):
     """
-    Iceberg Output class to write data to Iceberg tables using daft and pandas
+    Iceberg Table Reader class to read data from Iceberg tables using daft and pandas
+    """
+
+    table: Table
+    chunk_size: Optional[int]
+
+    def __init__(
+        self,
+        table: Table,
+        chunk_size: Optional[int] = 100000,
+        dataframe_type: DataframeType = DataframeType.pandas,
+    ):
+        """Initialize the Iceberg input class.
+
+        Args:
+            table (Table): Iceberg table object.
+            chunk_size (Optional[int], optional): Number of rows per batch.
+                Defaults to 100000.
+        """
+        self.table = table
+        self.chunk_size = chunk_size
+        self.dataframe_type = dataframe_type
+
+    async def read(self) -> Union["pd.DataFrame", "daft.DataFrame"]:
+        """
+        Method to read the data from the iceberg table
+        and return as a single combined dataframe (pandas or daft).
+        """
+        if self.dataframe_type == DataframeType.pandas:
+            return await self._get_dataframe()
+        elif self.dataframe_type == DataframeType.daft:
+            return await self._get_daft_dataframe()
+        else:
+            raise ValueError(f"Unsupported dataframe_type: {self.dataframe_type}")
+
+    async def _get_dataframe(self) -> "pd.DataFrame":
+        """
+        Method to read the data from the iceberg table
+        and return as a single combined pandas dataframe.
+        """
+        try:
+            import daft
+
+            # Iceberg reading is done via daft, then convert to pandas
+            daft_dataframe = daft.read_iceberg(self.table)
+            return daft_dataframe.to_pandas()
+        except Exception as e:
+            logger.error(f"Error reading data from Iceberg table: {str(e)}")
+            raise
+
+    async def _get_daft_dataframe(self) -> "daft.DataFrame":  # noqa: F821
+        """
+        Method to read the data from the iceberg table
+        and return as a single combined daft dataframe.
+        """
+        try:
+            import daft
+
+            return daft.read_iceberg(self.table)
+        except Exception as e:
+            logger.error(f"Error reading data from Iceberg table using daft: {str(e)}")
+            raise
+
+    def read_batches(
+        self,
+    ) -> AsyncIterator["daft.DataFrame"]:  # noqa: F821
+        # We are not implementing this method as we have to partition the daft dataframe
+        # using dataframe.into_partitions() method. This method does all the partitions in memory
+        # and using that can cause out of memory issues.
+        # ref: https://www.getdaft.io/projects/docs/en/stable/user_guide/poweruser/partitioning.html
+        raise NotImplementedError
+
+
+class IcebergTableWriter(Writer):
+    """
+    Iceberg Table Writer class to write data to Iceberg tables using daft and pandas
     """

     def __init__(
@@ -30,8 +105,9 @@ def __init__(
         total_record_count: int = 0,
         chunk_count: int = 0,
         retain_local_copy: bool = False,
+        dataframe_type: DataframeType = DataframeType.pandas,
     ):
-        """Initialize the Iceberg output class.
+        """Initialize the Iceberg writer class.

         Args:
             iceberg_catalog (Catalog): Iceberg catalog object.
@@ -51,8 +127,9 @@ def __init__(
         self.mode = mode
         self.metrics = get_metrics()
         self.retain_local_copy = retain_local_copy
+        self.dataframe_type = dataframe_type

-    async def write_dataframe(self, dataframe: "pd.DataFrame"):
+    async def _write_dataframe(self, dataframe: "pd.DataFrame", **kwargs):
         """
         Method to write the pandas dataframe to an iceberg table
         """
@@ -63,7 +140,7 @@ async def write_dataframe(self, dataframe: "pd.DataFrame"):
                 return
             # convert the pandas dataframe to a daft dataframe
             daft_dataframe = daft.from_pandas(dataframe)
-            await self.write_daft_dataframe(daft_dataframe)
+            await self._write_daft_dataframe(daft_dataframe)

             # Record metrics for successful write
             self.metrics.record_metric(
@@ -83,9 +160,9 @@ async def write_dataframe(self, dataframe: "pd.DataFrame"):
                 description="Number of errors while writing to Iceberg table",
             )
             logger.error(f"Error writing pandas dataframe to iceberg table: {str(e)}")
-            raise e
+            raise

-    async def write_daft_dataframe(self, dataframe: "daft.DataFrame"):  # noqa: F821
+    async def _write_daft_dataframe(self, dataframe: "daft.DataFrame", **kwargs):  # noqa: F821
         """
         Method to write the daft dataframe to an iceberg table
         """
@@ -136,4 +213,4 @@ async def write_daft_dataframe(self, dataframe: "daft.DataFrame"):  # noqa: F821
                 description="Number of errors while writing to Iceberg table",
             )
             logger.error(f"Error writing daft dataframe to iceberg table: {str(e)}")
-            raise e
+            raise
diff --git a/application_sdk/io/json.py b/application_sdk/io/json.py
new file mode 100644
index 000000000..e151ceb9a
--- /dev/null
+++ b/application_sdk/io/json.py
@@ -0,0 +1,432 @@
+import os
+from typing import TYPE_CHECKING, Any, AsyncIterator, Dict, List, Optional, Union
+
+import orjson
+from temporalio import activity
+
+from application_sdk.activities.common.models import ActivityStatistics
+from application_sdk.common.types import DataframeType
+from application_sdk.constants import DAPR_MAX_GRPC_MESSAGE_LENGTH
+from application_sdk.io._utils import (
+    JSON_FILE_EXTENSION,
+    convert_datetime_to_epoch,
+    download_files,
+    path_gen,
+    process_null_fields,
+)
+from application_sdk.observability.logger_adaptor import get_logger
+from application_sdk.observability.metrics_adaptor import MetricType, get_metrics
+
+if TYPE_CHECKING:
+    import daft
+    import pandas as pd
+
+from application_sdk.io import Reader, Writer
+
+logger = get_logger(__name__)
+activity.logger = logger
+
+
+class JsonFileReader(Reader):
+    """
+    JSON File Reader class to read data from JSON files using daft and pandas.
+    Supports reading both single files and directories containing multiple JSON files.
+    """
+
+    def __init__(
+        self,
+        path: str,
+        file_names: Optional[List[str]] = None,
+        chunk_size: int = 100000,
+        dataframe_type: DataframeType = DataframeType.pandas,
+    ):
+        """Initialize the JsonInput class.
+
+        Args:
+            path (str): Path to JSON file or directory containing JSON files.
+                It accepts both types of paths:
+                local path or object store path
+                Wildcards are not supported.
+            file_names (Optional[List[str]]): List of specific file names to read. Defaults to None.
+            chunk_size (int): Number of rows per batch. Defaults to 100000.
+
+        Raises:
+            ValueError: When path is not provided or when single file path is combined with file_names
+        """
+        self.extension = JSON_FILE_EXTENSION
+
+        # Validate that single file path and file_names are not both specified
+        if path.endswith(self.extension) and file_names:
+            raise ValueError(
+                f"Cannot specify both a single file path ('{path}') and file_names filter. "
+                f"Either provide a directory path with file_names, or specify the exact file path without file_names."
+            )
+
+        self.path = path
+        self.chunk_size = chunk_size
+        self.file_names = file_names
+        self.dataframe_type = dataframe_type
+
+    async def read(self) -> Union["pd.DataFrame", "daft.DataFrame"]:
+        """
+        Method to read the data from the json files in the path
+        and return as a single combined pandas dataframe
+        """
+        if self.dataframe_type == DataframeType.pandas:
+            return await self._get_dataframe()
+        elif self.dataframe_type == DataframeType.daft:
+            return await self._get_daft_dataframe()
+        else:
+            raise ValueError(f"Unsupported dataframe_type: {self.dataframe_type}")
+
+    def read_batches(
+        self,
+    ) -> Union[
+        AsyncIterator["pd.DataFrame"],
+        AsyncIterator["daft.DataFrame"],
+    ]:
+        """
+        Method to read the data from the json files in the path
+        and return as a batched pandas dataframe
+        """
+        if self.dataframe_type == DataframeType.pandas:
+            return self._get_batched_dataframe()
+        elif self.dataframe_type == DataframeType.daft:
+            return self._get_batched_daft_dataframe()
+        else:
+            raise ValueError(f"Unsupported dataframe_type: {self.dataframe_type}")
+
+    async def _get_batched_dataframe(
+        self,
+    ) -> AsyncIterator["pd.DataFrame"]:
+        """
+        Method to read the data from the json files in the path
+        and return as a batched pandas dataframe
+        """
+        try:
+            import pandas as pd
+
+            # Ensure files are available (local or downloaded)
+            json_files = await download_files(
+                self.path, self.extension, self.file_names
+            )
+            logger.info(f"Reading {len(json_files)} JSON files in batches")
+
+            for json_file in json_files:
+                json_reader_obj = pd.read_json(
+                    json_file,
+                    chunksize=self.chunk_size,
+                    lines=True,
+                )
+                for chunk in json_reader_obj:
+                    yield chunk
+        except Exception as e:
+            logger.error(f"Error reading batched data from JSON: {str(e)}")
+            raise
+
+    async def _get_dataframe(self) -> "pd.DataFrame":
+        """
+        Method to read the data from the json files in the path
+        and return as a single combined pandas dataframe
+        """
+        try:
+            import pandas as pd
+
+            # Ensure files are available (local or downloaded)
+            json_files = await download_files(
+                self.path, self.extension, self.file_names
+            )
+            logger.info(f"Reading {len(json_files)} JSON files as pandas dataframe")
+
+            return pd.concat(
+                (pd.read_json(json_file, lines=True) for json_file in json_files),
+                ignore_index=True,
+            )
+
+        except Exception as e:
+            logger.error(f"Error reading data from JSON: {str(e)}")
+            raise
+
+    async def _get_batched_daft_dataframe(
+        self,
+    ) -> AsyncIterator["daft.DataFrame"]:  # noqa: F821
+        """
+        Method to read the data from the json files in the path
+        and return as a batched daft dataframe
+        """
+        try:
+            import daft
+
+            # Ensure files are available (local or downloaded)
+            json_files = await download_files(
+                self.path, self.extension, self.file_names
+            )
+            logger.info(f"Reading {len(json_files)} JSON files as daft batches")
+
+            # Yield each discovered file as separate batch with chunking
+            for json_file in json_files:
+                yield daft.read_json(json_file, _chunk_size=self.chunk_size)
+        except Exception as e:
+            logger.error(f"Error reading batched data from JSON using daft: {str(e)}")
+            raise
+
+    async def _get_daft_dataframe(self) -> "daft.DataFrame":  # noqa: F821
+        """
+        Method to read the data from the json files in the path
+        and return as a single combined daft dataframe
+        """
+        try:
+            import daft
+
+            # Ensure files are available (local or downloaded)
+            json_files = await download_files(
+                self.path, self.extension, self.file_names
+            )
+            logger.info(f"Reading {len(json_files)} JSON files with daft")
+
+            # Use the discovered/downloaded files directly
+            return daft.read_json(json_files)
+        except Exception as e:
+            logger.error(f"Error reading data from JSON using daft: {str(e)}")
+            raise
+
+
+class JsonFileWriter(Writer):
+    """Output handler for writing data to JSON files.
+
+    This class provides functionality for writing data to JSON files with support
+    for chunking large datasets, buffering, and automatic file path generation.
+    It can handle both pandas and daft DataFrames as input.
+
+    The output can be written to local files and optionally uploaded to an object
+    store. Files are named using a configurable path generation scheme that
+    includes chunk numbers for split files.
+
+    Attributes:
+        output_path (str): Full path where JSON files will be written.
+        typename (Optional[str]): Type identifier for the data being written.
+        chunk_start (Optional[int]): Starting index for chunk numbering.
+        buffer_size (int): Size of the write buffer in bytes.
+        chunk_size (int): Maximum number of records per chunk.
+        total_record_count (int): Total number of records processed.
+        chunk_count (int): Number of chunks written.
+        buffer (List[Union[pd.DataFrame, daft.DataFrame]]): Buffer for accumulating
+            data before writing.
+    """
+
+    def __init__(
+        self,
+        output_path: str = "",
+        typename: Optional[str] = None,
+        chunk_start: Optional[int] = None,
+        buffer_size: int = 5000,
+        chunk_size: Optional[int] = 50000,  # to limit the memory usage on upload
+        total_record_count: int = 0,
+        chunk_count: int = 0,
+        start_marker: Optional[str] = None,
+        end_marker: Optional[str] = None,
+        retain_local_copy: bool = False,
+        dataframe_type: DataframeType = DataframeType.pandas,
+        **kwargs: Dict[str, Any],
+    ):
+        """Initialize the JSON output handler.
+
+        Args:
+            output_path (str): Full path where JSON files will be written.
+            typename (Optional[str], optional): Type identifier for the data being written.
+                If provided, a subdirectory with this name will be created under output_path.
+                Defaults to None.
+            chunk_start (Optional[int], optional): Starting index for chunk numbering.
+                Defaults to None.
+            buffer_size (int, optional): Size of the buffer in bytes.
+                Defaults to 10MB (1024 * 1024 * 10).
+            chunk_size (Optional[int], optional): Maximum number of records per chunk. If None, uses config value.
+                Defaults to None.
+            total_record_count (int, optional): Initial total record count.
+                Defaults to 0.
+            chunk_count (int, optional): Initial chunk count.
+                Defaults to 0.
+            retain_local_copy (bool, optional): Whether to retain the local copy of the files.
+                Defaults to False.
+            dataframe_type (DataframeType, optional): Type of dataframe to write. Defaults to DataframeType.pandas.
+        """
+        self.output_path = output_path
+        self.typename = typename
+        self.chunk_start = chunk_start
+        self.total_record_count = total_record_count
+        self.chunk_count = chunk_count
+        self.buffer_size = buffer_size
+        self.chunk_size = chunk_size or 50000  # to limit the memory usage on upload
+        self.buffer: List[Union["pd.DataFrame", "daft.DataFrame"]] = []  # noqa: F821
+        self.current_buffer_size = 0
+        self.current_buffer_size_bytes = 0  # Track estimated buffer size in bytes
+        self.max_file_size_bytes = int(
+            DAPR_MAX_GRPC_MESSAGE_LENGTH * 0.9
+        )  # 90% of DAPR limit as safety buffer
+        self.start_marker = start_marker
+        self.end_marker = end_marker
+        self.partitions = []
+        self.chunk_part = 0
+        self.metrics = get_metrics()
+        self.retain_local_copy = retain_local_copy
+        self.extension = JSON_FILE_EXTENSION
+        self.dataframe_type = dataframe_type
+
+        if not self.output_path:
+            raise ValueError("output_path is required")
+
+        if typename:
+            self.output_path = os.path.join(self.output_path, typename)
+        os.makedirs(self.output_path, exist_ok=True)
+
+        if self.chunk_start:
+            self.chunk_count = self.chunk_start + self.chunk_count
+
+    async def _write_daft_dataframe(
+        self,
+        dataframe: "daft.DataFrame",
+        preserve_fields: Optional[List[str]] = None,
+        null_to_empty_dict_fields: Optional[List[str]] = None,
+        **kwargs,
+    ):  # noqa: F821
+        """Write a daft DataFrame to JSON files.
+
+        This method converts the daft DataFrame to pandas and writes it to JSON files.
+
+        Args:
+            dataframe (daft.DataFrame): The DataFrame to write.
+            preserve_fields (Optional[List[str]]): List of fields to preserve during null processing.
+                Defaults to ["identity_cycle", "number_columns_in_part_key",
+                "columns_participating_in_part_key", "engine", "is_insertable_into", "is_typed"].
+            null_to_empty_dict_fields (Optional[List[str]]): List of fields to convert from null to empty dict.
+                Defaults to ["attributes", "customAttributes"].
+
+        Note:
+            Daft does not have built-in JSON writing support, so we are using orjson.
+        """
+        # Initialize default values for mutable arguments
+        if preserve_fields is None:
+            preserve_fields = [
+                "identity_cycle",
+                "number_columns_in_part_key",
+                "columns_participating_in_part_key",
+                "engine",
+                "is_insertable_into",
+                "is_typed",
+            ]
+        if null_to_empty_dict_fields is None:
+            null_to_empty_dict_fields = [
+                "attributes",
+                "customAttributes",
+            ]
+
+        try:
+            if self.chunk_start is None:
+                self.chunk_part = 0
+
+            buffer = []
+            for row in dataframe.iter_rows():
+                self.total_record_count += 1
+                # Convert datetime fields to epoch timestamps before serialization
+                row = convert_datetime_to_epoch(row)
+                # Remove null attributes from the row recursively, preserving specified fields
+                cleaned_row = process_null_fields(
+                    row, preserve_fields, null_to_empty_dict_fields
+                )
+                # Serialize the row and add it to the buffer
+                serialized_row = orjson.dumps(
+                    cleaned_row, option=orjson.OPT_APPEND_NEWLINE
+                )
+                buffer.append(serialized_row)
+                self.current_buffer_size += 1
+                self.current_buffer_size_bytes += len(serialized_row)
+
+                # If the buffer size is reached append to the file and clear the buffer
+                if self.current_buffer_size >= self.buffer_size:
+                    await self._flush_daft_buffer(buffer, self.chunk_part)
+
+                if self.current_buffer_size_bytes > self.max_file_size_bytes or (
+                    self.total_record_count > 0
+                    and self.total_record_count % self.chunk_size == 0
+                ):
+                    output_file_name = f"{self.output_path}/{path_gen(self.chunk_count, self.chunk_part, self.start_marker, self.end_marker, extension=self.extension)}"
+                    if os.path.exists(output_file_name):
+                        await self._upload_file(output_file_name)
+                        self.chunk_part += 1
+
+            # Write any remaining rows in the buffer
+            if self.current_buffer_size > 0:
+                await self._flush_daft_buffer(buffer, self.chunk_part)
+
+            # Record metrics for successful write
+            self.metrics.record_metric(
+                name="json_write_records",
+                value=dataframe.count_rows(),
+                metric_type=MetricType.COUNTER,
+                labels={"type": "daft"},
+                description="Number of records written to JSON files from daft DataFrame",
+            )
+        except Exception as e:
+            # Record metrics for failed write
+            self.metrics.record_metric(
+                name="json_write_errors",
+                value=1,
+                metric_type=MetricType.COUNTER,
+                labels={"type": "daft", "error": str(e)},
+                description="Number of errors while writing to JSON files",
+            )
+            logger.error(f"Error writing daft dataframe to json: {str(e)}")
+            raise
+
+    async def _flush_daft_buffer(self, buffer: List[str], chunk_part: int):
+        """Flush the current buffer to a JSON file.
+
+        This method combines all DataFrames in the buffer, writes them to a JSON file,
+        and uploads the file to the object store.
+        """
+        output_file_name = f"{self.output_path}/{path_gen(self.chunk_count, chunk_part, self.start_marker, self.end_marker, extension=self.extension)}"
+        with open(output_file_name, "ab+") as f:
+            f.writelines(buffer)
+        buffer.clear()  # Clear the buffer
+
+        self.current_buffer_size = 0
+
+        # Record chunk metrics
+        self.metrics.record_metric(
+            name="json_chunks_written",
+            value=1,
+            metric_type=MetricType.COUNTER,
+            labels={"type": "daft"},
+            description="Number of chunks written to JSON files",
+        )
+
+    async def _write_chunk(self, chunk: "pd.DataFrame", file_name: str):
+        """Write a chunk to a JSON file.
+
+        This method writes a chunk to a JSON file and uploads the file to the object store.
+        """
+        mode = "w" if not os.path.exists(file_name) else "a"
+        with open(file_name, mode=mode) as f:
+            chunk.to_json(f, orient="records", lines=True)
+
+    async def get_statistics(
+        self, typename: Optional[str] = None
+    ) -> ActivityStatistics:
+        """Get the statistics of the JSON files.
+
+        This method returns the statistics of the JSON files.
+        """
+        # Finally upload the final file
+        if self.current_buffer_size_bytes > 0:
+            output_file_name = f"{self.output_path}/{path_gen(self.chunk_count, self.chunk_part, self.start_marker, self.end_marker, extension=self.extension)}"
+            if os.path.exists(output_file_name):
+                await self._upload_file(output_file_name)
+                self.chunk_part += 1
+
+        # If chunk_start is set we don't want to increment the chunk_count
+        # Since it should only increment the chunk_part in this case
+        if self.chunk_start is None:
+            self.chunk_count += 1
+        self.partitions.append(self.chunk_part)
+
+        return await super().get_statistics(typename)
diff --git a/application_sdk/outputs/parquet.py b/application_sdk/io/parquet.py
similarity index 61%
rename from application_sdk/outputs/parquet.py
rename to application_sdk/io/parquet.py
index 880c2f0a4..1b7b1b38d 100644
--- a/application_sdk/outputs/parquet.py
+++ b/application_sdk/io/parquet.py
@@ -1,21 +1,34 @@
 import inspect
 import os
 import shutil
-from enum import Enum
-from typing import TYPE_CHECKING, AsyncGenerator, Generator, List, Optional, Union, cast
+from typing import (
+    TYPE_CHECKING,
+    AsyncGenerator,
+    AsyncIterator,
+    Generator,
+    List,
+    Optional,
+    Union,
+    cast,
+)

 from temporalio import activity

 from application_sdk.activities.common.utils import get_object_store_prefix
-from application_sdk.common.dataframe_utils import is_empty_dataframe
 from application_sdk.constants import (
     DAPR_MAX_GRPC_MESSAGE_LENGTH,
     ENABLE_ATLAN_UPLOAD,
     UPSTREAM_OBJECT_STORE_NAME,
 )
+from application_sdk.io import DataframeType, Reader, WriteMode, Writer
+from application_sdk.io._utils import (
+    PARQUET_FILE_EXTENSION,
+    download_files,
+    is_empty_dataframe,
+    path_gen,
+)
 from application_sdk.observability.logger_adaptor import get_logger
 from application_sdk.observability.metrics_adaptor import MetricType, get_metrics
-from application_sdk.outputs import Output
 from application_sdk.services.objectstore import ObjectStore

 logger = get_logger(__name__)
@@ -26,15 +39,306 @@
     import pandas as pd


-class WriteMode(Enum):
-    """Enumeration of write modes for Parquet output operations."""
+class ParquetFileReader(Reader):
+    """
+    Parquet File Reader class to read data from Parquet files using daft and pandas.
+    Supports reading both single files and directories containing multiple parquet files.
+    """
+
+    def __init__(
+        self,
+        path: str,
+        chunk_size: int = 100000,
+        buffer_size: int = 5000,
+        file_names: Optional[List[str]] = None,
+        dataframe_type: DataframeType = DataframeType.pandas,
+    ):
+        """Initialize the Parquet input class.
+
+        Args:
+            path (str): Path to parquet file or directory containing parquet files.
+                It accepts both types of paths:
+                local path or object store path
+                Wildcards are not supported.
+            chunk_size (int): Number of rows per batch. Defaults to 100000.
+            buffer_size (int): Number of rows per batch. Defaults to 5000.
+            file_names (Optional[List[str]]): List of file names to read. Defaults to None.
+
+        Raises:
+            ValueError: When path is not provided or when single file path is combined with file_names
+        """
+
+        # Validate that single file path and file_names are not both specified
+        if path.endswith(PARQUET_FILE_EXTENSION) and file_names:
+            raise ValueError(
+                f"Cannot specify both a single file path ('{path}') and file_names filter. "
+                f"Either provide a directory path with file_names, or specify the exact file path without file_names."
+            )
+
+        self.path = path
+        self.chunk_size = chunk_size
+        self.buffer_size = buffer_size
+        self.file_names = file_names
+        self.dataframe_type = dataframe_type
+
+    async def read(self) -> Union["pd.DataFrame", "daft.DataFrame"]:
+        """
+        Method to read the data from the parquet files in the path
+        and return as a single combined pandas dataframe
+        """
+        if self.dataframe_type == DataframeType.pandas:
+            return await self._get_dataframe()
+        elif self.dataframe_type == DataframeType.daft:
+            return await self._get_daft_dataframe()
+        else:
+            raise ValueError(f"Unsupported dataframe_type: {self.dataframe_type}")
+
+    def read_batches(
+        self,
+    ) -> Union[
+        AsyncIterator["pd.DataFrame"],
+        AsyncIterator["daft.DataFrame"],
+    ]:
+        """
+        Method to read the data from the parquet files in the path
+        and return as a batched pandas dataframe
+        """
+        if self.dataframe_type == DataframeType.pandas:
+            return self._get_batched_dataframe()
+        elif self.dataframe_type == DataframeType.daft:
+            return self._get_batched_daft_dataframe()
+        else:
+            raise ValueError(f"Unsupported dataframe_type: {self.dataframe_type}")
+
+    async def _get_dataframe(self) -> "pd.DataFrame":
+        """Read data from parquet file(s) and return as pandas DataFrame.
+
+        Returns:
+            pd.DataFrame: Combined dataframe from specified parquet files
+
+        Raises:
+            ValueError: When no valid path can be determined or no matching files found
+            Exception: When reading parquet files fails
+
+        Example transformation:
+        Input files:
+        +------------------+
+        | file1.parquet    |
+        | file2.parquet    |
+        | file3.parquet    |
+        +------------------+
+
+        With file_names=["file1.parquet", "file3.parquet"]:
+        +-------+-------+-------+
+        | col1  | col2  | col3  |
+        +-------+-------+-------+
+        | val1  | val2  | val3  |  # from file1.parquet
+        | val7  | val8  | val9  |  # from file3.parquet
+        +-------+-------+-------+
+
+        Transformations:
+        - Only specified files are read and combined
+        - Column schemas must be compatible across files
+        - Only reads files in the specified directory
+        """
+        try:
+            import pandas as pd
+
+            # Ensure files are available (local or downloaded)
+            parquet_files = await download_files(
+                self.path, PARQUET_FILE_EXTENSION, self.file_names
+            )
+            logger.info(f"Reading {len(parquet_files)} parquet files")
+
+            return pd.concat(
+                (pd.read_parquet(parquet_file) for parquet_file in parquet_files),
+                ignore_index=True,
+            )
+        except Exception as e:
+            logger.error(f"Error reading data from parquet file(s): {str(e)}")
+            raise
+
+    async def _get_batched_dataframe(
+        self,
+    ) -> AsyncIterator["pd.DataFrame"]:
+        """Read data from parquet file(s) in batches as pandas DataFrames.
+
+        Returns:
+            AsyncIterator[pd.DataFrame]: Async iterator of pandas dataframes
+
+        Raises:
+            ValueError: When no parquet files found locally or in object store
+            Exception: When reading parquet files fails
+
+        Example transformation:
+        Input files:
+        +------------------+
+        | file1.parquet    |
+        | file2.parquet    |
+        | file3.parquet    |
+        +------------------+
+
+        With file_names=["file1.parquet", "file2.parquet"] and chunk_size=2:
+        Batch 1:
+        +-------+-------+
+        | col1  | col2  |
+        +-------+-------+
+        | val1  | val2  |  # from file1.parquet
+        | val3  | val4  |  # from file1.parquet
+        +-------+-------+
+
+        Batch 2:
+        +-------+-------+
+        | col1  | col2  |
+        +-------+-------+
+        | val5  | val6  |  # from file2.parquet
+        | val7  | val8  |  # from file2.parquet
+        +-------+-------+
+
+        Transformations:
+        - Only specified files are combined then split into chunks
+        - Each batch is a separate DataFrame
+        - Only reads files in the specified directory
+        """
+        try:
+            import pandas as pd
+
+            # Ensure files are available (local or downloaded)
+            parquet_files = await download_files(
+                self.path, PARQUET_FILE_EXTENSION, self.file_names
+            )
+            logger.info(f"Reading {len(parquet_files)} parquet files in batches")

-    APPEND = "append"
-    OVERWRITE = "overwrite"
-    OVERWRITE_PARTITIONS = "overwrite-partitions"
+            # Process each file individually to maintain memory efficiency
+            for parquet_file in parquet_files:
+                df = pd.read_parquet(parquet_file)
+                for i in range(0, len(df), self.chunk_size):
+                    yield df.iloc[i : i + self.chunk_size]
+        except Exception as e:
+            logger.error(
+                f"Error reading data from parquet file(s) in batches: {str(e)}"
+            )
+            raise

+    async def _get_daft_dataframe(self) -> "daft.DataFrame":  # noqa: F821
+        """Read data from parquet file(s) and return as daft DataFrame.

-class ParquetOutput(Output):
+        Returns:
+            daft.DataFrame: Combined daft dataframe from specified parquet files
+
+        Raises:
+            ValueError: When no parquet files found locally or in object store
+            Exception: When reading parquet files fails
+
+        Example transformation:
+        Input files:
+        +------------------+
+        | file1.parquet    |
+        | file2.parquet    |
+        | file3.parquet    |
+        +------------------+
+
+        With file_names=["file1.parquet", "file3.parquet"]:
+        +-------+-------+-------+
+        | col1  | col2  | col3  |
+        +-------+-------+-------+
+        | val1  | val2  | val3  |  # from file1.parquet
+        | val7  | val8  | val9  |  # from file3.parquet
+        +-------+-------+-------+
+
+        Transformations:
+        - Only specified parquet files combined into single daft DataFrame
+        - Lazy evaluation for better performance
+        - Column schemas must be compatible across files
+        """
+        try:
+            import daft  # type: ignore
+
+            # Ensure files are available (local or downloaded)
+            parquet_files = await download_files(
+                self.path, PARQUET_FILE_EXTENSION, self.file_names
+            )
+            logger.info(f"Reading {len(parquet_files)} parquet files with daft")
+
+            # Use the discovered/downloaded files directly
+            return daft.read_parquet(parquet_files)
+        except Exception as e:
+            logger.error(
+                f"Error reading data from parquet file(s) using daft: {str(e)}"
+            )
+            raise
+
+    async def _get_batched_daft_dataframe(self) -> AsyncIterator["daft.DataFrame"]:  # type: ignore
+        """Get batched daft dataframe from parquet file(s).
+
+        Returns:
+            AsyncIterator[daft.DataFrame]: An async iterator of daft DataFrames, each containing
+            a batch of data from individual parquet files
+
+        Raises:
+            ValueError: When no parquet files found locally or in object store
+            Exception: When reading parquet files fails
+
+        Example transformation:
+        Input files:
+        +------------------+
+        | file1.parquet    |
+        | file2.parquet    |
+        | file3.parquet    |
+        +------------------+
+
+        With file_names=["file1.parquet", "file3.parquet"]:
+        Batch 1 (file1.parquet):
+        +-------+-------+
+        | col1  | col2  |
+        +-------+-------+
+        | val1  | val2  |
+        | val3  | val4  |
+        +-------+-------+
+
+        Batch 2 (file3.parquet):
+        +-------+-------+
+        | col1  | col2  |
+        +-------+-------+
+        | val7  | val8  |
+        | val9  | val10 |
+        +-------+-------+
+
+        Transformations:
+        - Each specified file becomes a separate daft DataFrame batch
+        - Lazy evaluation for better performance
+        - Files processed individually for memory efficiency
+        """
+        try:
+            import daft  # type: ignore
+
+            # Ensure files are available (local or downloaded)
+            parquet_files = await download_files(
+                self.path, PARQUET_FILE_EXTENSION, self.file_names
+            )
+            logger.info(f"Reading {len(parquet_files)} parquet files as daft batches")
+
+            # Create a lazy dataframe without loading data into memory
+            lazy_df = daft.read_parquet(parquet_files)
+
+            # Get total count efficiently
+            total_rows = lazy_df.count_rows()
+
+            # Yield chunks without loading everything into memory
+            for offset in range(0, total_rows, self.buffer_size):
+                chunk = lazy_df.offset(offset).limit(self.buffer_size)
+                yield chunk
+
+            del lazy_df
+
+        except Exception as error:
+            logger.error(
+                f"Error reading data from parquet file(s) in batches using daft: {error}"
+            )
+            raise
+
+
+class ParquetFileWriter(Writer):
     """Output handler for writing data to Parquet files.

     This class handles writing DataFrames to Parquet files with support for chunking
@@ -42,7 +346,6 @@ class ParquetOutput(Output):

     Attributes:
         output_path (str): Base path where Parquet files will be written.
-        output_suffix (str): Suffix for output files.
         typename (Optional[str]): Type name of the entity e.g database, schema, table.
         chunk_size (int): Maximum number of records per chunk.
         total_record_count (int): Total number of records processed.
@@ -54,12 +357,9 @@ class ParquetOutput(Output):
         use_consolidation (bool): Whether to use consolidation.
     """

-    _EXTENSION = ".parquet"
-
     def __init__(
         self,
         output_path: str = "",
-        output_suffix: str = "",
         typename: Optional[str] = None,
         chunk_size: Optional[int] = 100000,
         buffer_size: int = 5000,
@@ -71,12 +371,12 @@ def __init__(
         end_marker: Optional[str] = None,
         retain_local_copy: bool = False,
         use_consolidation: bool = False,
+        dataframe_type: DataframeType = DataframeType.pandas,
     ):
         """Initialize the Parquet output handler.

         Args:
             output_path (str): Base path where Parquet files will be written.
-            output_suffix (str): Suffix for output files.
             typename (Optional[str], optional): Type name of the entity e.g database, schema, table.
             chunk_size (int, optional): Maximum records per chunk. Defaults to 100000.
             total_record_count (int, optional): Initial total record count. Defaults to 0.
@@ -91,9 +391,10 @@ def __init__(
                 Defaults to False.
             use_consolidation (bool, optional): Whether to use consolidation.
                 Defaults to False.
+            dataframe_type (DataframeType, optional): Type of dataframe to write. Defaults to DataframeType.pandas.
         """
+        self.extension = PARQUET_FILE_EXTENSION
         self.output_path = output_path
-        self.output_suffix = output_suffix
         self.typename = typename
         self.chunk_size = chunk_size
         self.buffer_size = buffer_size
@@ -112,6 +413,7 @@ def __init__(
         self.partitions = []
         self.metrics = get_metrics()
         self.retain_local_copy = retain_local_copy
+        self.dataframe_type = dataframe_type

         # Consolidation-specific attributes
         # Use consolidation to efficiently write parquet files in buffered manner
@@ -128,13 +430,14 @@ def __init__(
         if self.chunk_start:
             self.chunk_count = self.chunk_start + self.chunk_count

+        if not self.output_path:
+            raise ValueError("output_path is required")
         # Create output directory
-        self.output_path = os.path.join(self.output_path, self.output_suffix)
         if self.typename:
             self.output_path = os.path.join(self.output_path, self.typename)
         os.makedirs(self.output_path, exist_ok=True)

-    async def write_batched_dataframe(
+    async def _write_batched_dataframe(
         self,
         batched_dataframe: Union[
             AsyncGenerator["pd.DataFrame", None], Generator["pd.DataFrame", None, None]
@@ -155,7 +458,7 @@ async def write_batched_dataframe(
         """
         if not self.use_consolidation:
             # Fallback to base class implementation
-            await super().write_batched_dataframe(batched_dataframe)
+            await super()._write_batched_dataframe(batched_dataframe)
             return

         try:
@@ -186,12 +489,13 @@ async def write_batched_dataframe(
             await self._cleanup_temp_folders()  # Cleanup on error
             raise

-    async def write_daft_dataframe(
+    async def _write_daft_dataframe(
         self,
         dataframe: "daft.DataFrame",  # noqa: F821
         partition_cols: Optional[List] = None,
-        write_mode: Union[WriteMode, str] = WriteMode.APPEND,
+        write_mode: Union[WriteMode, str] = WriteMode.APPEND.value,
         morsel_size: int = 100_000,
+        **kwargs,
     ):
         """Write a daft DataFrame to Parquet files and upload to object store.

@@ -324,7 +628,11 @@ def _get_consolidated_file_path(self, folder_index: int, chunk_part: int) -> str
         """Generate final consolidated file path using existing path_gen logic."""
         return os.path.join(
             self.output_path,
-            self.path_gen(chunk_count=folder_index, chunk_part=chunk_part),
+            path_gen(
+                chunk_count=folder_index,
+                chunk_part=chunk_part,
+                extension=self.extension,
+            ),
         )

     async def _accumulate_dataframe(self, dataframe: "pd.DataFrame"):
@@ -374,14 +682,14 @@ async def _write_chunk_to_temp_folder(self, chunk: "pd.DataFrame"):
             [
                 f
                 for f in os.listdir(self.current_temp_folder_path)
-                if f.endswith(".parquet")
+                if f.endswith(self.extension)
             ]
         )
-        chunk_file_name = f"chunk-{existing_files}.parquet"
+        chunk_file_name = f"chunk-{existing_files}{self.extension}"
         chunk_file_path = os.path.join(self.current_temp_folder_path, chunk_file_name)

         # Write chunk using existing write_chunk method
-        await self.write_chunk(chunk, chunk_file_path)
+        await self._write_chunk(chunk, chunk_file_path)

     async def _consolidate_current_folder(self):
         """Consolidate current temp folder using Daft."""
@@ -392,7 +700,7 @@ async def _consolidate_current_folder(self):
             import daft

             # Read all parquet files in temp folder
-            pattern = os.path.join(self.current_temp_folder_path, "*.parquet")
+            pattern = os.path.join(self.current_temp_folder_path, f"*{self.extension}")
             daft_df = daft.read_parquet(pattern)
             partitions = 0

@@ -408,7 +716,7 @@ async def _consolidate_current_folder(self):
                 result_dict = result.to_pydict()
                 partitions = len(result_dict["path"])
                 for i, file_path in enumerate(result_dict["path"]):
-                    if file_path.endswith(".parquet"):
+                    if file_path.endswith(self.extension):
                         consolidated_file_path = self._get_consolidated_file_path(
                             folder_index=self.chunk_count,
                             chunk_part=i,
@@ -475,7 +783,7 @@ async def _cleanup_temp_folders(self):
         except Exception as e:
             logger.warning(f"Error cleaning up temp folders: {str(e)}")

-    async def write_chunk(self, chunk: "pd.DataFrame", file_name: str):
+    async def _write_chunk(self, chunk: "pd.DataFrame", file_name: str):
         """Write a chunk to a Parquet file.

         This method writes a chunk to a Parquet file and uploads the file to the object store.
diff --git a/application_sdk/observability/observability.py b/application_sdk/observability/observability.py
index c94b1529c..850f743b9 100644
--- a/application_sdk/observability/observability.py
+++ b/application_sdk/observability/observability.py
@@ -363,14 +363,14 @@ async def parquet_sink(self, message: Any):
             logging.error(f"Error buffering log: {e}")

     async def _flush_records(self, records: List[Dict[str, Any]]):
-        """Flush records to parquet file and object store using ParquetOutput abstraction.
+        """Flush records to parquet file and object store using ParquetFileWriter.

         Args:
             records: List of records to flush

         This method:
         - Groups records by partition (year/month/day)
-        - Uses ParquetOutput abstraction for efficient writing
+        - Uses ParquetFileWriter for efficient writing
         - Automatically handles chunking, compression, and dual upload
         - Provides robust error handling per partition
         - Cleans up old records if enabled
@@ -395,7 +395,7 @@ async def _flush_records(self, records: List[Dict[str, Any]]):
                     partition_records[partition_path] = []
                 partition_records[partition_path].append(record)

-            # Write records to each partition using ParquetOutput abstraction
+                # Write records to each partition using ParquetFileWriter
             for partition_path, partition_data in partition_records.items():
                 # Create new dataframe from current records
                 new_df = pd.DataFrame(partition_data)
@@ -412,23 +412,34 @@ async def _flush_records(self, records: List[Dict[str, Any]]):
                     elif part.startswith("day="):
                         new_df["day"] = int(part.split("=")[1])

-                # Use new data directly - let ParquetOutput handle consolidation and merging
+                # Use new data directly - let ParquetFileWriter handle consolidation and merging
                 df = new_df

-                # Use ParquetOutput abstraction for efficient writing and uploading
+                # Use ParquetFileWriter for efficient writing and uploading
                 # Set the output path for this partition
                 try:
-                    # Lazy import and instantiation of ParquetOutput
-                    from application_sdk.outputs.parquet import ParquetOutput
+                    # Lazy import and instantiation of ParquetFileWriter
+                    from application_sdk.io.parquet import ParquetFileWriter

-                    parquet_output = ParquetOutput(
+                    parquet_writer = ParquetFileWriter(
                         output_path=partition_path,
                         chunk_start=0,
                         chunk_part=int(time()),
                     )
-                    await parquet_output.write_dataframe(dataframe=df)
-                except Exception as e:
-                    print(f"Error writing records to partition: {str(e)}")
+                    logging.info(
+                        f"Successfully instantiated ParquetFileWriter for partition: {partition_path}"
+                    )
+
+                    await parquet_writer._write_dataframe(dataframe=df)
+
+                    logging.info(
+                        f"Successfully wrote {len(df)} records to partition: {partition_path}"
+                    )
+
+                except Exception as partition_error:
+                    logging.error(
+                        f"Error processing partition {partition_path}: {str(partition_error)}"
+                    )

             # Clean up old records if enabled
             if self._cleanup_enabled:
diff --git a/application_sdk/outputs/.cursor/BUGBOT.md b/application_sdk/outputs/.cursor/BUGBOT.md
deleted file mode 100644
index 21ad80ed1..000000000
--- a/application_sdk/outputs/.cursor/BUGBOT.md
+++ /dev/null
@@ -1,295 +0,0 @@
-# Output Code Review Guidelines - Data Output Processing
-
-## Context-Specific Patterns
-
-This directory contains output processing implementations for various data formats (JSON, Parquet, Iceberg). Output processors must handle data uploads efficiently while maintaining data integrity and correct destination paths.
-
-### Phase 1: Critical Output Safety Issues
-
-**Object Store Path Management:**
-
-- **Correct destination paths**: Upload paths must respect user-configured output prefixes
-- **Path construction accuracy**: Object store keys must be calculated correctly, not hardcoded
-- **User prefix preservation**: Respect user-provided output directories and naming conventions
-- **Path validation**: Ensure upload paths don't conflict with existing data
-
-**Data Integrity and Security:**
-
-- All output data must be validated before upload
-- File permissions and access controls must be properly set
-- Data serialization must be consistent and recoverable
-- Prevent overwriting critical data without confirmation
-- Maintain data lineage information in output metadata
-
-```python
-# ✅ DO: Proper object store upload path handling
-class JsonOutput:
-    async def upload_to_object_store(
-        self,
-        data: List[dict],
-        output_prefix: str,  # User-provided output location
-        filename: str
-    ) -> dict:
-        """Upload data with correct path handling."""
-
-        # Construct full object store path respecting user's output prefix
-        object_store_key = os.path.join(output_prefix, filename)
-
-        # Serialize data
-        json_data = orjson.dumps(data, option=orjson.OPT_APPEND_NEWLINE)
-
-        # Upload to correct location
-        result = await self.object_store.upload_file(
-            data=json_data,
-            destination=object_store_key  # Respect user's intended location
-        )
-
-        return result
-
-# ❌ REJECT: Incorrect path handling
-class BadJsonOutput:
-    async def upload_to_object_store(self, data: List[dict], filename: str):
-        # Wrong: hardcoded or derived path, ignoring user configuration
-        object_store_key = get_object_store_prefix(f"/tmp/{filename}")  # Ignores output_prefix!
-
-        result = await self.object_store.upload_file(
-            data=orjson.dumps(data),
-            destination=object_store_key  # Wrong destination!
-        )
-        return result
-```
-
-### Phase 2: Output Architecture Patterns
-
-**Performance Optimization Requirements:**
-
-- **Parallelization opportunities**: Flag sequential upload operations that could be parallelized
-- **Batch processing**: Group related uploads to reduce overhead
-- **Streaming uploads**: Use streaming for large datasets instead of loading into memory
-- **Connection optimization**: Reuse object store connections across operations
-
-**Resource Management:**
-
-- Use proper connection pooling for object store operations
-- Implement timeout handling for upload operations
-- Clean up temporary files after upload
-- Handle partial upload failures gracefully
-- Monitor memory usage during large data serialization
-
-```python
-# ✅ DO: Parallel upload processing
-async def upload_multiple_datasets_parallel(
-    self,
-    datasets: List[Tuple[List[dict], str]],  # (data, filename) pairs
-    output_prefix: str
-) -> List[dict]:
-    """Upload multiple datasets in parallel for better performance."""
-
-    async def upload_single_dataset(data: List[dict], filename: str) -> dict:
-        """Upload a single dataset with error handling."""
-        try:
-            object_store_key = os.path.join(output_prefix, filename)
-            serialized_data = orjson.dumps(data, option=orjson.OPT_APPEND_NEWLINE)
-
-            return await self.object_store.upload_file(
-                data=serialized_data,
-                destination=object_store_key
-            )
-        except Exception as e:
-            logger.error(f"Failed to upload {filename}: {e}")
-            raise
-
-    # Parallel processing with controlled concurrency
-    semaphore = asyncio.Semaphore(5)  # Limit concurrent uploads
-
-    async def upload_with_semaphore(data: List[dict], filename: str) -> dict:
-        async with semaphore:
-            return await upload_single_dataset(data, filename)
-
-    tasks = [upload_with_semaphore(data, filename) for data, filename in datasets]
-    return await asyncio.gather(*tasks)
-
-# ❌ REJECT: Sequential upload processing
-async def upload_multiple_datasets_sequential(
-    self,
-    datasets: List[Tuple[List[dict], str]],
-    output_prefix: str
-) -> List[dict]:
-    """Sequential uploads - should be flagged for parallelization."""
-    results = []
-    for data, filename in datasets:  # FLAG: Could be parallelized
-        object_store_key = os.path.join(output_prefix, filename)
-        result = await self.object_store.upload_file(data, object_store_key)
-        results.append(result)
-    return results
-```
-
-### Phase 3: Output Testing Requirements
-
-**Data Output Testing:**
-
-- Test with various data formats and sizes
-- Test serialization and deserialization consistency
-- Test partial upload scenarios and recovery
-- Mock object store operations in unit tests
-- Include integration tests with real object store
-- Test data corruption detection and prevention
-
-**Performance Testing:**
-
-- Include tests for large dataset uploads
-- Test memory usage during serialization
-- Test concurrent upload operations
-- Verify timeout handling works correctly
-- Test connection pool behavior under load
-
-### Phase 4: Performance and Scalability
-
-**Data Upload Efficiency:**
-
-- Use streaming uploads for large datasets
-- Implement proper chunking for oversized data
-- Use compression for large text-based outputs
-- Monitor upload progress and provide feedback
-- Optimize serialization performance (use orjson over json)
-
-**Object Store Optimization:**
-
-- Use connection pooling for object store clients
-- Implement proper retry logic for upload failures
-- Use parallel uploads where appropriate
-- Monitor upload metrics and error rates
-- Handle bandwidth limitations gracefully
-
-### Phase 5: Output Maintainability
-
-**Error Handling and Recovery:**
-
-- Implement comprehensive error handling for all upload operations
-- Provide meaningful error messages with upload context
-- Handle partial upload failures gracefully
-- Implement proper retry logic for transient failures
-- Log all upload operations with destination information
-
-**Configuration Management:**
-
-- Externalize all output-related configuration
-- Support different output destinations and formats
-- Validate output configuration before processing
-- Document all supported output parameters
-- Handle environment-specific output requirements
-
----
-
-## Output-Specific Anti-Patterns
-
-**Always Reject:**
-
-- **Path derivation errors**: Deriving object store paths from local temporary paths
-- **Sequential uploads**: Uploading multiple files sequentially when parallel uploads are possible
-- **Memory inefficiency**: Loading entire datasets into memory for serialization
-- **Missing upload verification**: Not verifying successful uploads
-- **Poor error recovery**: Not handling partial upload failures gracefully
-- **Resource leaks**: Not cleaning up temporary files or connections
-
-**Object Store Upload Anti-Patterns:**
-
-```python
-# ❌ REJECT: Incorrect upload path handling
-class BadOutputProcessor:
-    async def upload_results(self, results: List[dict]):
-        # Wrong: derives upload path from temporary local path
-        local_temp_file = "/tmp/results.json"
-        upload_key = get_object_store_prefix(local_temp_file)  # Incorrect!
-
-        await self.object_store.upload_file(results, upload_key)
-
-# ✅ REQUIRE: Correct upload path handling
-class GoodOutputProcessor:
-    async def upload_results(
-        self,
-        results: List[dict],
-        output_prefix: str,  # User-specified destination
-        filename: str = "results.json"
-    ):
-        # Use actual user-configured output location
-        upload_key = os.path.join(output_prefix, filename)
-
-        await self.object_store.upload_file(
-            data=orjson.dumps(results),
-            destination=upload_key  # Correct destination
-        )
-```
-
-**Performance Anti-Patterns:**
-
-```python
-# ❌ REJECT: Sequential upload processing
-async def upload_multiple_files_sequential(file_data_pairs: List[Tuple]):
-    results = []
-    for data, filename in file_data_pairs:  # Should be parallelized
-        result = await upload_single_file(data, filename)
-        results.append(result)
-    return results
-
-# ✅ REQUIRE: Parallel upload processing with proper error handling
-async def upload_multiple_files_parallel(
-    file_data_pairs: List[Tuple],
-    max_concurrency: int = 5
-) -> List[dict]:
-    semaphore = asyncio.Semaphore(max_concurrency)
-
-    async def upload_with_semaphore(data, filename):
-        async with semaphore:
-            try:
-                return await upload_single_file(data, filename)
-            except Exception as e:
-                logger.error(f"Upload failed for {filename}: {e}")
-                return {"filename": filename, "status": "failed", "error": str(e)}
-
-    tasks = [upload_with_semaphore(data, filename) for data, filename in file_data_pairs]
-    return await asyncio.gather(*tasks)
-```
-
-**Memory Management Anti-Patterns:**
-
-```python
-# ❌ REJECT: Loading entire dataset for serialization
-async def bad_large_dataset_upload(large_dataset: List[dict]):
-    # Loads entire dataset into memory
-    json_data = orjson.dumps(large_dataset)  # Could exceed memory limits
-    await upload_data(json_data)
-
-# ✅ REQUIRE: Streaming serialization for large datasets
-async def good_large_dataset_upload(large_dataset: List[dict], chunk_size: int = 1000):
-    """Stream large datasets to avoid memory issues."""
-
-    async def serialize_chunk(chunk: List[dict]) -> bytes:
-        return orjson.dumps(chunk, option=orjson.OPT_APPEND_NEWLINE)
-
-    # Process in chunks to manage memory
-    for i in range(0, len(large_dataset), chunk_size):
-        chunk = large_dataset[i:i + chunk_size]
-        serialized_chunk = await serialize_chunk(chunk)
-
-        await upload_chunk(
-            data=serialized_chunk,
-            chunk_index=i // chunk_size
-        )
-```
-
-## Educational Context for Output Reviews
-
-When reviewing output code, emphasize:
-
-1. **Data Integrity Impact**: "Incorrect upload path handling can cause data to be stored in wrong locations, making it inaccessible to downstream processes. This breaks data pipelines and can cause data loss."
-
-2. **Performance Impact**: "Sequential uploads create unnecessary bottlenecks. For enterprise datasets with multiple output files, parallelization can significantly reduce processing time and improve user experience."
-
-3. **Resource Impact**: "Poor memory management during serialization can cause out-of-memory errors with large datasets. Streaming and chunking are essential for enterprise-scale data output."
-
-4. **User Experience Impact**: "Output path errors are often discovered late in processing, causing wasted computation and frustrating delays. Proper validation and clear error messages improve reliability."
-
-5. **Scalability Impact**: "Output patterns that work for small datasets can fail at enterprise scale. Always design output processes to handle the largest expected dataset sizes efficiently."
-
-6. **Data Pipeline Impact**: "Output processing is the final step in data pipelines. Failures here can invalidate all upstream processing work. Robust error handling and verification are critical for pipeline reliability."
diff --git a/application_sdk/outputs/json.py b/application_sdk/outputs/json.py
deleted file mode 100644
index 38f24150d..000000000
--- a/application_sdk/outputs/json.py
+++ /dev/null
@@ -1,268 +0,0 @@
-import os
-from datetime import datetime
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union
-
-import orjson
-from temporalio import activity
-
-from application_sdk.activities.common.models import ActivityStatistics
-from application_sdk.constants import DAPR_MAX_GRPC_MESSAGE_LENGTH
-from application_sdk.observability.logger_adaptor import get_logger
-from application_sdk.observability.metrics_adaptor import MetricType, get_metrics
-from application_sdk.outputs import Output
-
-logger = get_logger(__name__)
-activity.logger = logger
-
-if TYPE_CHECKING:
-    import daft  # type: ignore
-    import pandas as pd
-
-
-def convert_datetime_to_epoch(data: Any) -> Any:
-    """Convert datetime objects to epoch timestamps in milliseconds.
-
-    Args:
-        data: The data to convert
-
-    Returns:
-        The converted data with datetime fields as epoch timestamps
-    """
-    if isinstance(data, datetime):
-        return int(data.timestamp() * 1000)
-    elif isinstance(data, dict):
-        return {k: convert_datetime_to_epoch(v) for k, v in data.items()}
-    elif isinstance(data, list):
-        return [convert_datetime_to_epoch(item) for item in data]
-    return data
-
-
-class JsonOutput(Output):
-    """Output handler for writing data to JSON files.
-
-    This class provides functionality for writing data to JSON files with support
-    for chunking large datasets, buffering, and automatic file path generation.
-    It can handle both pandas and daft DataFrames as input.
-
-    The output can be written to local files and optionally uploaded to an object
-    store. Files are named using a configurable path generation scheme that
-    includes chunk numbers for split files.
-
-    Attributes:
-        output_path (Optional[str]): Base path where JSON files will be written.
-        output_suffix (str): Suffix added to file paths when uploading to object store.
-        typename (Optional[str]): Type identifier for the data being written.
-        chunk_start (Optional[int]): Starting index for chunk numbering.
-        buffer_size (int): Size of the write buffer in bytes.
-        chunk_size (int): Maximum number of records per chunk.
-        total_record_count (int): Total number of records processed.
-        chunk_count (int): Number of chunks written.
-        buffer (List[Union[pd.DataFrame, daft.DataFrame]]): Buffer for accumulating
-            data before writing.
-    """
-
-    _EXTENSION = ".json"
-
-    def __init__(
-        self,
-        output_suffix: str,
-        output_path: Optional[str] = None,
-        typename: Optional[str] = None,
-        chunk_start: Optional[int] = None,
-        buffer_size: int = 5000,
-        chunk_size: Optional[int] = 50000,  # to limit the memory usage on upload
-        total_record_count: int = 0,
-        chunk_count: int = 0,
-        start_marker: Optional[str] = None,
-        end_marker: Optional[str] = None,
-        retain_local_copy: bool = False,
-        **kwargs: Dict[str, Any],
-    ):
-        """Initialize the JSON output handler.
-
-        Args:
-            output_path (str): Path where JSON files will be written.
-            output_suffix (str): Prefix for files when uploading to object store.
-            chunk_start (Optional[int], optional): Starting index for chunk numbering.
-                Defaults to None.
-            buffer_size (int, optional): Size of the buffer in bytes.
-                Defaults to 10MB (1024 * 1024 * 10).
-            chunk_size (Optional[int], optional): Maximum number of records per chunk. If None, uses config value.
-                Defaults to None.
-            total_record_count (int, optional): Initial total record count.
-                Defaults to 0.
-            chunk_count (int, optional): Initial chunk count.
-                Defaults to 0.
-            retain_local_copy (bool, optional): Whether to retain the local copy of the files.
-                Defaults to False.
-        """
-        self.output_path = output_path
-        self.output_suffix = output_suffix
-        self.typename = typename
-        self.chunk_start = chunk_start
-        self.total_record_count = total_record_count
-        self.chunk_count = chunk_count
-        self.buffer_size = buffer_size
-        self.chunk_size = chunk_size or 50000  # to limit the memory usage on upload
-        self.buffer: List[Union["pd.DataFrame", "daft.DataFrame"]] = []  # noqa: F821
-        self.current_buffer_size = 0
-        self.current_buffer_size_bytes = 0  # Track estimated buffer size in bytes
-        self.max_file_size_bytes = int(
-            DAPR_MAX_GRPC_MESSAGE_LENGTH * 0.9
-        )  # 90% of DAPR limit as safety buffer
-        self.start_marker = start_marker
-        self.end_marker = end_marker
-        self.partitions = []
-        self.chunk_part = 0
-        self.metrics = get_metrics()
-        self.retain_local_copy = retain_local_copy
-
-        if not self.output_path:
-            raise ValueError("output_path is required")
-
-        self.output_path = os.path.join(self.output_path, output_suffix)
-        if typename:
-            self.output_path = os.path.join(self.output_path, typename)
-        os.makedirs(self.output_path, exist_ok=True)
-
-        if self.chunk_start:
-            self.chunk_count = self.chunk_start + self.chunk_count
-
-    async def write_daft_dataframe(
-        self,
-        dataframe: "daft.DataFrame",
-        preserve_fields: Optional[List[str]] = [
-            "identity_cycle",
-            "number_columns_in_part_key",
-            "columns_participating_in_part_key",
-            "engine",
-            "is_insertable_into",
-            "is_typed",
-        ],
-        null_to_empty_dict_fields: Optional[List[str]] = [
-            "attributes",
-            "customAttributes",
-        ],
-    ):  # noqa: F821
-        """Write a daft DataFrame to JSON files.
-
-        This method converts the daft DataFrame to pandas and writes it to JSON files.
-
-        Args:
-            dataframe (daft.DataFrame): The DataFrame to write.
-
-        Note:
-            Daft does not have built-in JSON writing support, so we are using orjson.
-        """
-        try:
-            if self.chunk_start is None:
-                self.chunk_part = 0
-
-            buffer = []
-            for row in dataframe.iter_rows():
-                self.total_record_count += 1
-                # Convert datetime fields to epoch timestamps before serialization
-                row = convert_datetime_to_epoch(row)
-                # Remove null attributes from the row recursively, preserving specified fields
-                cleaned_row = self.process_null_fields(
-                    row, preserve_fields, null_to_empty_dict_fields
-                )
-                # Serialize the row and add it to the buffer
-                serialized_row = orjson.dumps(
-                    cleaned_row, option=orjson.OPT_APPEND_NEWLINE
-                )
-                buffer.append(serialized_row)
-                self.current_buffer_size += 1
-                self.current_buffer_size_bytes += len(serialized_row)
-
-                # If the buffer size is reached append to the file and clear the buffer
-                if self.current_buffer_size >= self.buffer_size:
-                    await self.flush_daft_buffer(buffer, self.chunk_part)
-
-                if self.current_buffer_size_bytes > self.max_file_size_bytes or (
-                    self.total_record_count > 0
-                    and self.total_record_count % self.chunk_size == 0
-                ):
-                    output_file_name = f"{self.output_path}/{self.path_gen(self.chunk_count, self.chunk_part, self.start_marker, self.end_marker)}"
-                    if os.path.exists(output_file_name):
-                        await self._upload_file(output_file_name)
-                        self.chunk_part += 1
-
-            # Write any remaining rows in the buffer
-            if self.current_buffer_size > 0:
-                await self.flush_daft_buffer(buffer, self.chunk_part)
-
-            # Record metrics for successful write
-            self.metrics.record_metric(
-                name="json_write_records",
-                value=dataframe.count_rows(),
-                metric_type=MetricType.COUNTER,
-                labels={"type": "daft"},
-                description="Number of records written to JSON files from daft DataFrame",
-            )
-        except Exception as e:
-            # Record metrics for failed write
-            self.metrics.record_metric(
-                name="json_write_errors",
-                value=1,
-                metric_type=MetricType.COUNTER,
-                labels={"type": "daft", "error": str(e)},
-                description="Number of errors while writing to JSON files",
-            )
-            logger.error(f"Error writing daft dataframe to json: {str(e)}")
-
-    async def flush_daft_buffer(self, buffer: List[str], chunk_part: int):
-        """Flush the current buffer to a JSON file.
-
-        This method combines all DataFrames in the buffer, writes them to a JSON file,
-        and uploads the file to the object store.
-        """
-        output_file_name = (
-            f"{self.output_path}/{self.path_gen(self.chunk_count, chunk_part)}"
-        )
-        with open(output_file_name, "ab+") as f:
-            f.writelines(buffer)
-        buffer.clear()  # Clear the buffer
-
-        self.current_buffer_size = 0
-
-        # Record chunk metrics
-        self.metrics.record_metric(
-            name="json_chunks_written",
-            value=1,
-            metric_type=MetricType.COUNTER,
-            labels={"type": "daft"},
-            description="Number of chunks written to JSON files",
-        )
-
-    async def write_chunk(self, chunk: "pd.DataFrame", file_name: str):
-        """Write a chunk to a JSON file.
-
-        This method writes a chunk to a JSON file and uploads the file to the object store.
-        """
-        mode = "w" if not os.path.exists(file_name) else "a"
-        chunk.to_json(file_name, orient="records", lines=True, mode=mode)
-
-    async def get_statistics(
-        self, typename: Optional[str] = None
-    ) -> ActivityStatistics:
-        """Get the statistics of the JSON files.
-
-        This method returns the statistics of the JSON files.
-        """
-        # Finally upload the final file
-        if self.current_buffer_size_bytes > 0:
-            output_file_name = (
-                f"{self.output_path}/{self.path_gen(self.chunk_count, self.chunk_part)}"
-            )
-            if os.path.exists(output_file_name):
-                await self._upload_file(output_file_name)
-                self.chunk_part += 1
-
-        # If chunk_start is set we don't want to increment the chunk_count
-        # Since it should only increment the chunk_part in this case
-        if self.chunk_start is None:
-            self.chunk_count += 1
-        self.partitions.append(self.chunk_part)
-
-        return await super().get_statistics(typename)
diff --git a/application_sdk/server/fastapi/models.py b/application_sdk/server/fastapi/models.py
index 5f2cceb71..14805c76b 100644
--- a/application_sdk/server/fastapi/models.py
+++ b/application_sdk/server/fastapi/models.py
@@ -5,7 +5,7 @@

 from pydantic import BaseModel, Field, RootModel

-from application_sdk.events.models import Event, EventFilter
+from application_sdk.interceptors.models import Event, EventFilter
 from application_sdk.workflows import WorkflowInterface


diff --git a/application_sdk/common/dapr_utils.py b/application_sdk/services/_utils.py
similarity index 100%
rename from application_sdk/common/dapr_utils.py
rename to application_sdk/services/_utils.py
diff --git a/application_sdk/services/eventstore.py b/application_sdk/services/eventstore.py
index 3e03a2058..59de8a0da 100644
--- a/application_sdk/services/eventstore.py
+++ b/application_sdk/services/eventstore.py
@@ -10,14 +10,14 @@
 from dapr import clients
 from temporalio import activity, workflow

-from application_sdk.common.dapr_utils import is_component_registered
 from application_sdk.constants import (
     APPLICATION_NAME,
     DAPR_BINDING_OPERATION_CREATE,
     EVENT_STORE_NAME,
 )
-from application_sdk.events.models import Event, EventMetadata, WorkflowStates
+from application_sdk.interceptors.models import Event, EventMetadata, WorkflowStates
 from application_sdk.observability.logger_adaptor import get_logger
+from application_sdk.services._utils import is_component_registered

 logger = get_logger(__name__)
 activity.logger = logger
@@ -47,7 +47,7 @@ def enrich_event_metadata(cls, event: Event):
             a Temporal workflow or activity context.

         Examples:
-            >>> from application_sdk.events.models import Event
+            >>> from application_sdk.interceptors.models import Event

             >>> # Create basic event
             >>> event = Event(event_type="data.processed", data={"count": 100})
@@ -109,7 +109,7 @@ async def publish_event(cls, event: Event):
             Exception: If there's an error during event publishing (logged but not re-raised).

         Examples:
-            >>> from application_sdk.events.models import Event
+            >>> from application_sdk.interceptors.models import Event

             >>> # Publish workflow status event
             >>> status_event = Event(
diff --git a/application_sdk/services/secretstore.py b/application_sdk/services/secretstore.py
index 6933adbd2..49d49fa91 100644
--- a/application_sdk/services/secretstore.py
+++ b/application_sdk/services/secretstore.py
@@ -20,7 +20,6 @@

 from dapr.clients import DaprClient

-from application_sdk.common.dapr_utils import is_component_registered
 from application_sdk.common.error_codes import CommonError
 from application_sdk.constants import (
     DEPLOYMENT_NAME,
@@ -30,6 +29,7 @@
     SECRET_STORE_NAME,
 )
 from application_sdk.observability.logger_adaptor import get_logger
+from application_sdk.services._utils import is_component_registered
 from application_sdk.services.statestore import StateStore, StateType

 logger = get_logger(__name__)
diff --git a/application_sdk/test_utils/hypothesis/strategies/outputs/json_output.py b/application_sdk/test_utils/hypothesis/strategies/outputs/json_output.py
index 90076dc2a..c54c495a9 100644
--- a/application_sdk/test_utils/hypothesis/strategies/outputs/json_output.py
+++ b/application_sdk/test_utils/hypothesis/strategies/outputs/json_output.py
@@ -62,7 +62,6 @@ def dataframe_strategy(draw) -> pd.DataFrame:
 json_output_config_strategy = st.fixed_dictionaries(
     {
         "output_path": safe_path_strategy,
-        "output_suffix": st.builds(lambda x: f"/{x}", safe_path_strategy),
         "output_prefix": output_prefix_strategy,
         "chunk_size": chunk_size_strategy,
     }
diff --git a/application_sdk/test_utils/hypothesis/strategies/server/fastapi/__init__.py b/application_sdk/test_utils/hypothesis/strategies/server/fastapi/__init__.py
index 46524c25d..a53811f59 100644
--- a/application_sdk/test_utils/hypothesis/strategies/server/fastapi/__init__.py
+++ b/application_sdk/test_utils/hypothesis/strategies/server/fastapi/__init__.py
@@ -2,7 +2,7 @@

 from hypothesis import strategies as st

-from application_sdk.events.models import Event
+from application_sdk.interceptors.models import Event

 # Strategy for generating auth credentials
 auth_credentials_strategy = st.fixed_dictionaries(
diff --git a/application_sdk/worker.py b/application_sdk/worker.py
index c30dc041f..9000c2c12 100644
--- a/application_sdk/worker.py
+++ b/application_sdk/worker.py
@@ -15,7 +15,7 @@

 from application_sdk.clients.workflow import WorkflowClient
 from application_sdk.constants import DEPLOYMENT_NAME, MAX_CONCURRENT_ACTIVITIES
-from application_sdk.events.models import (
+from application_sdk.interceptors.models import (
     ApplicationEventNames,
     Event,
     EventTypes,
diff --git a/docs/docs/concepts/atlanupload.md b/docs/docs/concepts/atlanupload.md
index 0a68c5869..1e62b2ded 100644
--- a/docs/docs/concepts/atlanupload.md
+++ b/docs/docs/concepts/atlanupload.md
@@ -196,11 +196,13 @@ upload_stats = await AtlanStorageOutput.upload_object_store_data_to_atlan(
 - `total_bytes`: Total bytes uploaded
 - `errors`: List of error messages

-### JsonOutput
+### JsonFileWriter

-#### `write_daft_dataframe()`
+#### `write()`

-Stores a Daft DataFrame in the object store and local files.
+Stores a DataFrame (Pandas or Daft) in local files and automatically uploads to object store.
+
+**Note**: `JsonFileWriter` is part of the I/O module (`application_sdk.io.json`) and handles both local file writing and object store uploads transparently.

 ## Testing

diff --git a/docs/docs/concepts/clients.md b/docs/docs/concepts/clients.md
index 9b9575a6f..925fb39c0 100644
--- a/docs/docs/concepts/clients.md
+++ b/docs/docs/concepts/clients.md
@@ -47,7 +47,7 @@ Both SQL client classes are typically **subclassed** for specific database types
 3.  **Executing Queries (`run_query` method):**
     *   Takes a SQL query string and optional `batch_size`.
     *   Executes the query using the established connection.
-    *   Yields results in batches (lists of dictionaries). This method is useful for direct execution but often less used than `SQLQueryInput` within activities.
+    *   Yields results in batches (lists of dictionaries).

 ### Example `DB_CONFIG`

@@ -66,24 +66,19 @@ class SnowflakeClient(BaseSQLClient):
     )
 ```

-### Interaction with `SQLQueryInput` and Activities
+### Interaction with Activities

-While `BaseSQLClient` establishes the connection and holds the SQLAlchemy engine, the actual execution of queries *within standard activities* (like those for metadata or query extraction) is often delegated to `SQLQueryInput` (from `application_sdk.inputs`).
+`BaseSQLClient` establishes the connection and holds the SQLAlchemy engine, which is used directly by activities to execute queries.

-*   **Role of `SQLClient`:** Creates and manages the underlying database connection (`self.engine`) based on `DB_CONFIG` and credentials. Provides the configured engine to other components.
-*   **Role of `SQLQueryInput`:**
-    *   Takes the `engine` from the initialized `SQLClient` instance and a specific `query` string as input.
-    *   Handles the execution of that single query against the provided engine.
-    *   Provides methods like `get_daft_dataframe()`, `get_dataframe()`, `get_batched_dataframe()` to return the results conveniently as Daft or Pandas DataFrames, abstracting away the details of cursor handling and batch fetching for the activity developer.
+*   **Role of `SQLClient`:** Creates and manages the underlying database connection (`self.engine`) based on `DB_CONFIG` and credentials. Provides the configured engine and the `run_query` method to other components.
 *   **Role of Activities:**
     *   Activities (e.g., `fetch_tables`, `fetch_columns` in `BaseSQLMetadataExtractionActivities`) orchestrate the process.
-    *   They retrieve the initialized `SQLClient` (and its `engine`) from the shared activity state.
-    *   They instantiate `SQLQueryInput` with the client's engine and the appropriate SQL query (often defined as a class attribute on the activity or loaded from a file).
-    *   They call methods on `SQLQueryInput` (like `get_daft_dataframe`) to get the data.
-    *   They process the resulting DataFrame (e.g., save it to Parquet, transform it).
+    *   They retrieve the initialized `SQLClient` from the shared activity state.
+    *   They call methods on the `SQLClient` (like `run_query`) to execute queries and get the data.
+    *   They process the resulting data (e.g., save it to Parquet, transform it).

 **Simplified Flow:**
-`Activity` -> gets `SQLClient` from state -> creates `SQLQueryInput(engine=sql_client.engine, query=...)` -> calls `sql_query_input.get_daft_dataframe()` -> receives DataFrame -> processes DataFrame.
+`Activity` -> gets `SQLClient` from state -> calls `sql_client.run_query(query=...)` -> receives data -> processes data.

 ## Base Client (`base.py`)

@@ -254,6 +249,6 @@ if __name__ == "__main__":

 The `clients` module abstracts interactions with external services.

-`SQLClient` subclasses (configured via `DB_CONFIG`) provide the database engine, which is then typically used by `SQLQueryInput` within activities to fetch data as DataFrames. `TemporalWorkflowClient` (obtained via `get_workflow_client`) manages interactions with the Temporal service for workflow lifecycle management.
+`SQLClient` subclasses (configured via `DB_CONFIG`) provide the database engine and query execution methods, which are used by activities to fetch data. `TemporalWorkflowClient` (obtained via `get_workflow_client`) manages interactions with the Temporal service for workflow lifecycle management.

 `BaseClient` provides a foundation for non-SQL data sources with HTTP request support through the `execute_http_get_request` and `execute_http_post_request` methods. The class also allows for custom retry logic to be configured through the `http_retry_transport` attribute which can be set to a `httpx.AsyncBaseTransport` instance, either through the `httpx` default transport or a custom transport from libraries like `httpx-retries`.
\ No newline at end of file
diff --git a/docs/docs/concepts/handlers.md b/docs/docs/concepts/handlers.md
index 809518640..30b808583 100644
--- a/docs/docs/concepts/handlers.md
+++ b/docs/docs/concepts/handlers.md
@@ -151,8 +151,8 @@ This is a concrete implementation of `HandlerInterface` specifically designed fo
     *   `fetch_schemas_sql`: Query to fetch schema names for a given database.
 *   **Default Implementations:**
     *   `load()`: Calls `load()` on the provided `sql_client`.
-    *   `test_auth()`: Executes `test_authentication_sql` using `SQLQueryInput`.
-    *   `fetch_metadata()`: Based on the `metadata_type` argument, calls `prepare_metadata`, `fetch_databases`, or `fetch_schemas`. `prepare_metadata` executes `metadata_sql` using `SQLQueryInput`.
+    *   `test_auth()`: Executes `test_authentication_sql` using the SQL client's `run_query` method.
+    *   `fetch_metadata()`: Based on the `metadata_type` argument, calls `prepare_metadata`, `fetch_databases`, or `fetch_schemas`. `prepare_metadata` executes `metadata_sql` using the SQL client.
     *   `preflight_check()`: Orchestrates several checks:
         *   `check_schemas_and_databases()`: Executes `metadata_sql` and validates include/exclude filters against the results.
         *   `tables_check()`: Executes `tables_check_sql` (prepared with filters) to count tables.
diff --git a/docs/docs/concepts/inputs.md b/docs/docs/concepts/inputs.md
index 3f90d9e20..4a1faa95a 100644
--- a/docs/docs/concepts/inputs.md
+++ b/docs/docs/concepts/inputs.md
@@ -1,119 +1,219 @@
-# Inputs
+# Inputs (I/O Readers)

-This module provides a standardized way to read data from various sources within the Application SDK framework. It defines a common `Input` interface and offers concrete implementations for common data sources like SQL databases, Parquet files, JSON files, and configuration stores.
+This module provides a standardized way to read data from various sources within the Application SDK framework. It defines a common `Reader` interface and offers concrete implementations for reading from Parquet files, JSON files, and Iceberg tables.

 ## Core Concepts

-1.  **`Input` Interface (`application_sdk.inputs.__init__.py`)**:
+1.  **`Reader` Interface (`application_sdk.io.Reader`)**:
     *   **Purpose:** An abstract base class defining the contract for reading data.
-    *   **Key Methods:** Requires subclasses to implement methods for retrieving data, primarily focused on returning results as Pandas or Daft DataFrames, either entirely or in batches:
-        *   `get_dataframe()`: Returns a single Pandas DataFrame.
-        *   `get_batched_dataframe()`: Returns an iterator/async iterator of Pandas DataFrames.
-        *   `get_daft_dataframe()`: Returns a single Daft DataFrame.
-        *   `get_batched_daft_dataframe()`: Returns an iterator/async iterator of Daft DataFrames.
-    *   **Usage:** Activities typically instantiate a specific `Input` subclass and use one of these methods to retrieve the data they need to process.
+    *   **Key Methods:** Requires subclasses to implement methods for retrieving data as Pandas or Daft DataFrames:
+        *   `read()`: Returns a single DataFrame (Pandas or Daft depending on `dataframe_type`).
+        *   `read_batches()`: Returns an async iterator of DataFrames for memory-efficient processing.
+    *   **Usage:** Activities instantiate a specific `Reader` subclass and use these methods to retrieve data for processing.

-2.  **Concrete Implementations:** The SDK provides several input classes:
+2.  **Concrete Implementations:** The SDK provides several reader classes:

-    *   **`SQLQueryInput` (`sql_query.py`)**: Reads data from a SQL database by executing a query.
-    *   **`ParquetInput` (`parquet.py`)**: Reads data from Parquet files (single file or directory).
-    *   **`JsonInput` (`json.py`)**: Reads data from JSON files (specifically JSON Lines format).
-    *   **`IcebergInput` (`iceberg.py`)**: Reads data from Apache Iceberg tables.
+    *   **`ParquetFileReader` (`application_sdk.io.parquet`)**: Reads data from Parquet files.
+    *   **`JsonFileReader` (`application_sdk.io.json`)**: Reads data from JSON Lines files.
+    *   **`IcebergTableReader` (`application_sdk.io.iceberg`)**: Reads data from Apache Iceberg tables.

-## Usage Patterns and Examples
+## Object Store Integration (Automatic Download)

-Inputs are primarily used within **Activities** to fetch the data required for a specific workflow step.
+**All file-based readers automatically handle object store downloads**, making data access seamless:

-### `SQLQueryInput`
+### How It Works

-Used to execute a SQL query and retrieve results as a DataFrame.
+1. **Check Local Files**: Reader first checks if files exist at the specified local `path`
+2. **Auto-Download**: If files are not found locally, automatically downloads from object store
+3. **Caching**: Downloads are cached locally for subsequent reads
+4. **Transparent Access**: Your code simply calls `read()` - downloads happen automatically

-*   **Initialization:** `SQLQueryInput(engine, query, chunk_size=...)`
-    *   `engine`: An initialized SQLAlchemy engine, typically obtained from the `SQLClient` stored in the activity state (`state.sql_client.engine`).
-    *   `query`: The SQL query string to execute.
-    *   `chunk_size` (optional): How many rows to fetch per batch when using batched methods.
-*   **Common Usage:** Within SQL-based activities (like `BaseSQLMetadataExtractionActivities`) to fetch databases, schemas, tables, columns, etc.
+This means you never need to manually download files from object storage - the readers handle it for you!

-```python
-# Within an Activity method (e.g., fetch_tables in BaseSQLMetadataExtractionActivities)
-from application_sdk.inputs.sql_query import SQLQueryInput
-# ... other imports ...
+### Example Flow
+```
+Activity calls read() → Reader checks local path → Files missing?
+  → Downloads from object store → Caches locally → Returns data
+```

-async def fetch_tables(self, workflow_args: Dict[str, Any]):
-    # ... get state, prepare query ...
-    state: BaseSQLMetadataExtractionActivitiesState = await self._get_state(workflow_args)
-    prepared_query = prepare_query(self.fetch_table_sql, workflow_args, ...) # Prepare query string
+## Naming Convention

-    if not prepared_query or not state.sql_client or not state.sql_client.engine:
-        logger.warning("Missing SQL client engine or query for fetching tables.")
-        return None
+Reader classes follow a clear naming pattern that indicates what they work with:

-    # Instantiate SQLQueryInput with the client's engine and the specific query
-    sql_input = SQLQueryInput(engine=state.sql_client.engine, query=prepared_query)
+- **`*FileReader`**: Work with file formats stored on disk
+  - Read from Parquet, JSON, or other file formats
+  - Automatically download from object store if needed
+  - Examples: `ParquetFileReader`, `JsonFileReader`

-    # Get results as a Daft DataFrame
-    try:
-        daft_df = await sql_input.get_daft_dataframe()
-        # Process the daft_df (e.g., write to ParquetOutput)
-        # ...
-        return {"typename": "table", "total_record_count": len(daft_df), ...}
+- **`*TableReader`**: Work with managed table storage systems
+  - Read directly from table engines like Apache Iceberg
+  - Handle table-specific features (schema evolution, partitioning, time travel)
+  - Examples: `IcebergTableReader`

-    except Exception as e:
-        logger.error(f"Failed to fetch tables: {e}", exc_info=True)
-        raise
-```
+## Usage Patterns and Examples
+
+Readers are primarily used within **Activities** to fetch data for processing.

-### `ParquetInput` & `JsonInput`
+### ParquetFileReader & JsonFileReader

-Used to read data from local Parquet or JSON Lines files, often downloaded from an object store first.
+Used to read data from Parquet or JSON Lines files, with automatic object store download support.

-*   **Initialization:**
-    *   `ParquetInput(path, chunk_size=..., input_prefix=..., file_names=...)`
-    *   `JsonInput(path, file_names=..., download_file_prefix=..., chunk_size=...)`
-    *   `path`: Local directory containing the files.
-    *   `file_names`: Specific list of files within `path` to read.
-    *   `input_prefix` / `download_file_prefix`: If provided, the input class will download the specified `file_names` (or all files if `file_names` is None for Parquet) from this object store prefix into the local `path` before reading.
-    *   `chunk_size` (optional): Rows per batch for batched reading methods.
-*   **Common Usage:** Often used in `transform_data` activities where data fetched in a previous step was saved as Parquet/JSON by an `Output` class. The activity reads this intermediate data for transformation.
+**Initialization:**
+```python
+ParquetFileReader(
+    path="local/path/to/data",           # Local path where files are or should be
+    file_names=["file1.parquet", ...],    # Optional: specific files to read
+    chunk_size=100000,                    # Optional: rows per batch
+    dataframe_type=DataframeType.pandas          # or DataframeType.daft
+)
+
+JsonFileReader(
+    path="local/path/to/data",
+    file_names=["file1.json", ...],
+    chunk_size=100000,
+    dataframe_type=DataframeType.pandas
+)
+```
+
+**Common Usage in Activities:**

 ```python
 # Within a transform_data Activity method
-from application_sdk.inputs.parquet import ParquetInput
-# ... other imports ...
+from application_sdk.io.parquet import ParquetFileReader
+from application_sdk.io import DataframeType

 @activity.defn
 @auto_heartbeater
 async def transform_data(self, workflow_args: Dict[str, Any]):
-    output_prefix, output_path, typename, workflow_id, workflow_run_id = self._validate_output_args(workflow_args)
-    file_names = workflow_args.get("file_names", []) # List of files to process
+    output_path = workflow_args.get("output_path")
+    typename = workflow_args.get("typename", "data")
+    file_names = workflow_args.get("file_names", [])

-    # Path where files were likely written by a previous activity's Output
-    local_input_path = f"{output_path}/{typename}"
+    # Path where files were written by a previous activity
+    local_input_path = f"{output_path}/raw/{typename}"

-    # Instantiate ParquetInput to read the files written earlier
-    # input_prefix=output_prefix ensures files are downloaded from object store if not local
-    parquet_input = ParquetInput(
+    # Instantiate ParquetFileReader
+    # If files aren't local, they'll be automatically downloaded from object store
+    parquet_reader = ParquetFileReader(
         path=local_input_path,
-        input_prefix=output_prefix,
-        file_names=file_names
+        file_names=file_names,
+        dataframe_type=DataframeType.daft  # Use Daft for better performance with large datasets
     )

     try:
-        # Read the data (example: get batched daft dataframes)
-        async for batch_df in parquet_input.get_batched_daft_dataframe():
-            # Process each batch_df (e.g., transform using state.transformer)
-            # ...
-            pass
-        # ... handle results ...
+        # Read data in batches for memory efficiency
+        async for batch_df in parquet_reader.read_batches():
+            # Process each batch (e.g., transform using state.transformer)
+            transformed = await self.transformer.transform(batch_df)
+            # Write transformed data...
+
     except Exception as e:
-        logger.error(f"Error transforming data from Parquet: {e}", exc_info=True)
+        logger.error(f"Error reading data: {e}", exc_info=True)
         raise
 ```

-### Other Inputs
+**Reading All Data at Once:**
+
+```python
+# For smaller datasets, read everything into a single DataFrame
+from application_sdk.io.json import JsonFileReader
+from application_sdk.io import DataframeType
+
+json_reader = JsonFileReader(
+    path="local/data/output",
+    dataframe_type=DataframeType.pandas
+)
+
+# Read all data at once
+df = await json_reader.read()
+print(f"Read {len(df)} records")
+```
+
+### IcebergTableReader

-*   **`IcebergInput`:** Used for reading directly from Iceberg tables. Requires a `pyiceberg.table.Table` object during initialization.
+Used for reading directly from Apache Iceberg tables (requires PyIceberg).
+
+```python
+from application_sdk.io.iceberg import IcebergTableReader
+from application_sdk.io import DataframeType
+from pyiceberg.catalog import load_catalog
+
+# Load Iceberg catalog and table
+catalog = load_catalog("my_catalog")
+table = catalog.load_table("my_database.my_table")
+
+# Create reader
+iceberg_reader = IcebergTableReader(
+    table=table,
+    chunk_size=100000,
+    dataframe_type=DataframeType.daft  # Iceberg works best with Daft
+)
+
+# Read data
+df = await iceberg_reader.read()
+```
+
+## Advanced Features
+
+### Batched Reading for Large Datasets
+
+For memory-efficient processing of large datasets, use `read_batches()`:
+
+```python
+reader = ParquetFileReader(
+    path="/data/large_dataset",
+    chunk_size=50000,  # Process 50K rows at a time
+    dataframe_type=DataframeType.daft
+)
+
+total_records = 0
+async for batch in reader.read_batches():
+    # Process each batch independently
+    processed = process_batch(batch)
+    total_records += batch.count_rows()
+
+print(f"Processed {total_records} records")
+```
+
+### File Filtering
+
+Read only specific files from a directory:
+
+```python
+reader = ParquetFileReader(
+    path="/data/partitioned",
+    file_names=[
+        "chunk-0-0.parquet",
+        "chunk-0-1.parquet"
+    ]  # Only read these specific files
+)
+```
+
+### DataFrame Type Selection
+
+Choose between Pandas and Daft based on your use case:
+
+```python
+# Pandas - Better for small datasets, rich API
+pandas_reader = JsonFileReader(
+    path="/data",
+    dataframe_type=DataframeType.pandas
+)
+
+# Daft - Better for large datasets, distributed processing
+daft_reader = JsonFileReader(
+    path="/data",
+    dataframe_type=DataframeType.daft
+)
+```

 ## Summary

-The `inputs` module provides convenient classes for reading data from diverse sources (SQL, Parquet, JSON, Iceberg) within activities. They abstract the underlying read logic and often provide results as Pandas or Daft DataFrames, integrating seamlessly with the SDK's activity patterns and other components like Clients and Outputs.
\ No newline at end of file
+The readers module provides convenient classes for reading data from diverse sources (Parquet, JSON, Iceberg). Key features include:
+
+- **Automatic object store downloads** - no manual file management needed
+- **Memory-efficient batched reading** - process large datasets without loading everything into memory
+- **Flexible DataFrame support** - choose Pandas or Daft based on your needs
+- **Transparent caching** - downloaded files are cached locally for performance
+
+These readers integrate seamlessly with the SDK's activity patterns and work hand-in-hand with Writers for complete data pipeline workflows.
diff --git a/docs/docs/concepts/outputs.md b/docs/docs/concepts/outputs.md
index c7aa70e77..89f3300a4 100644
--- a/docs/docs/concepts/outputs.md
+++ b/docs/docs/concepts/outputs.md
@@ -1,108 +1,142 @@
-# Outputs
+# Outputs (I/O Writers)

-This module provides a standardized way to write data to various destinations within the Application SDK framework. It mirrors the `inputs` module by defining a common `Output` interface and offering concrete implementations for common formats like JSON Lines and Parquet.
+This module provides a standardized way to write data to various destinations within the Application SDK framework. It defines a common `Writer` interface and offers concrete implementations for common formats like JSON Lines, Parquet, and Iceberg tables.

 ## Core Concepts

-1.  **`Output` Interface (`application_sdk.outputs.__init__.py`)**:
+1.  **`Writer` Interface (`application_sdk.io.Writer`)**:
     *   **Purpose:** An abstract base class defining the contract for writing data.
     *   **Key Methods:** Requires subclasses to implement methods for writing Pandas or Daft DataFrames:
-        *   `write_dataframe(dataframe: pd.DataFrame)`: Write a single Pandas DataFrame.
-        *   `write_daft_dataframe(dataframe: daft.DataFrame)`: Write a single Daft DataFrame.
-    *   **Helper Methods:** Provides base implementations for writing *batched* DataFrames (`write_batched_dataframe`, `write_batched_daft_dataframe`) which iterate over input generators/async generators and call the corresponding single DataFrame write methods.
+        *   `write(dataframe: Union[pd.DataFrame, daft.DataFrame])`: Write a single DataFrame (Pandas or Daft).
+        *   `write_batches(dataframe: Union[AsyncGenerator, Generator])`: Write batched DataFrames.
+        *   `_write_dataframe(dataframe: pd.DataFrame)`: Internal method for writing Pandas DataFrames.
+        *   `_write_daft_dataframe(dataframe: daft.DataFrame)`: Internal method for writing Daft DataFrames.
     *   **Statistics:** Includes methods (`get_statistics`, `write_statistics`) to track and save metadata about the output (record count, chunk count) to a `statistics.json.ignore` file, typically alongside the data output.
-    *   **Usage:** Activities typically instantiate a specific `Output` subclass and use its write methods to persist data fetched or generated during the activity.
+    *   **Usage:** Activities typically instantiate a specific `Writer` subclass and use its write methods to persist data fetched or generated during the activity.

-2.  **Concrete Implementations:** The SDK provides several output classes:
+2.  **Concrete Implementations:** The SDK provides several writer classes:

-    *   **`JsonOutput` (`json.py`)**: Writes DataFrames to JSON Lines files (`.json`).
-    *   **`ParquetOutput` (`parquet.py`)**: Writes DataFrames to Parquet files (`.parquet`).
-    *   **`IcebergOutput` (`iceberg.py`)**: Writes DataFrames to Apache Iceberg tables.
+    *   **`JsonFileWriter` (`application_sdk.io.json`)**: Writes DataFrames to JSON Lines files (`.json`).
+    *   **`ParquetFileWriter` (`application_sdk.io.parquet`)**: Writes DataFrames to Parquet files (`.parquet`).
+    *   **`IcebergTableWriter` (`application_sdk.io.iceberg`)**: Writes DataFrames to Apache Iceberg tables.

-## `JsonOutput` (`json.py`)
+## Object Store Integration (Automatic Upload)
+
+**All file-based writers automatically handle object store uploads**, making data persistence seamless:
+
+### How It Works
+
+1. **Write Locally**: Writer first writes files to the specified local `output_path`
+2. **Auto-Upload**: After writing completes, automatically uploads files to object store
+3. **Optional Cleanup**: Can optionally retain or delete local copies after upload
+4. **Transparent Persistence**: Your code simply calls `write()` - uploads happen automatically
+
+This means you never need to manually upload files to object storage - the writers handle it for you!
+
+### Complete Data Flow
+```
+Activity calls write() → Writer writes to local path →
+  → Uploads to object store → Optionally cleans up local files →
+  → Returns statistics
+```
+
+## Naming Convention
+
+Writer classes follow a clear naming pattern that indicates what they work with:
+
+- **`*FileWriter`**: Work with file formats stored on disk
+  - Write to Parquet, JSON, or other file formats
+  - Automatically upload to object store after writing
+  - Support chunking and compression
+  - Examples: `ParquetFileWriter`, `JsonFileWriter`
+
+- **`*TableWriter`**: Work with managed table storage systems
+  - Write directly to table engines like Apache Iceberg
+  - Handle table-specific features (schema evolution, partitioning, ACID transactions)
+  - Examples: `IcebergTableWriter`
+
+## `JsonFileWriter` (`application_sdk.io.json`)

 Writes Pandas or Daft DataFrames to one or more JSON Lines files locally, optionally uploading them to an object store.

 ### Features

-*   **DataFrame Support:** Can write both Pandas (`write_dataframe`) and Daft (`write_daft_dataframe`) DataFrames. Daft DataFrames are processed row-by-row using `orjson` for memory efficiency.
+*   **DataFrame Support:** Can write both Pandas and Daft DataFrames. Daft DataFrames are processed row-by-row using `orjson` for memory efficiency.
 *   **Chunking:** Automatically splits large DataFrames into multiple output files based on the `chunk_size` parameter.
 *   **Buffering (Pandas):** For Pandas DataFrames, uses an internal buffer to accumulate data before writing chunks, controlled by `buffer_size`.
 *   **File Naming:** Uses a `path_gen` function to name output files, typically incorporating chunk numbers (e.g., `1.json`, `2-100.json`). Can be customized.
-*   **Object Store Integration:** After writing files locally to the specified `output_path`, it uploads the generated files to the location specified by `output_prefix`.
+*   **Object Store Integration:** After writing files locally to the specified `output_path`, it uploads the generated files to object storage.
 *   **Statistics:** Tracks `total_record_count` and `chunk_count` and saves them via `write_statistics`.

 ### Initialization

-`JsonOutput(output_suffix, output_path=..., output_prefix=..., typename=..., chunk_start=..., chunk_size=..., ...)`
+`JsonFileWriter(output_path, typename=..., chunk_start=..., chunk_size=..., ...)`

-*   `output_suffix` (str): A suffix added to the base `output_path`. Often used for specific runs or data types.
-*   `output_path` (str): The base *local* directory where files will be temporarily written (e.g., `/data/workflow_run_123`). The final local path becomes `{output_path}/{output_suffix}/{typename}`.
-*   `output_prefix` (str): The prefix/path in the **object store** where the locally written files will be uploaded.
-*   `typename` (str, optional): A subdirectory name added under `{output_path}/{output_suffix}` (e.g., `tables`, `columns`). Helps organize output.
+*   `output_path` (str): The full path where files will be written (e.g., `/data/workflow_run_123/transformed`). The caller should construct this path explicitly.
+*   `typename` (str, optional): A subdirectory name added under `output_path` (e.g., `tables`, `columns`). Helps organize output.
 *   `chunk_start` (int, optional): Starting index for chunk numbering in filenames.
-*   `chunk_size` (int, optional): Maximum number of records per output file chunk (default: 100,000).
+*   `chunk_size` (int, optional): Maximum number of records per output file chunk (default: 50,000).

 ### Common Usage

-`JsonOutput` (and similarly `ParquetOutput`) is typically used within activities that fetch data and need to persist it for subsequent steps, like a transformation activity.
+`JsonFileWriter` (and similarly `ParquetFileWriter`) is typically used within activities that fetch data and need to persist it for subsequent steps, like a transformation activity.

 ```python
 # Within an Activity method (e.g., query_executor in SQL extraction/query activities)
-from application_sdk.outputs.json import JsonOutput
-# ... other imports, including SQLQueryInput etc ...
+import os
+from application_sdk.io.json import JsonFileWriter
+# ... other imports ...

 async def query_executor(
     self,
-    sql_engine: Any,
+    sql_client: Any,
     sql_query: Optional[str],
     workflow_args: Dict[str, Any],
-    output_suffix: str, # e.g., workflow_run_id
+    output_path: str,   # Full path, e.g., "/data/workflow_run_123/raw/table"
     typename: str,      # e.g., "table", "column"
 ) -> Optional[Dict[str, Any]]:

     # ... (validate inputs, prepare query) ...

-    sql_input = SQLQueryInput(engine=sql_engine, query=prepared_query)
-
-    # Get output path details from workflow_args
-    output_prefix = workflow_args.get("output_prefix") # Object store path
-    output_path = workflow_args.get("output_path")     # Base local path
-
-    if not output_prefix or not output_path:
-        raise ValueError("output_prefix and output_path are required in workflow_args")
-
-    # Instantiate JsonOutput
-    json_output = JsonOutput(
-        output_suffix=output_suffix,
-        output_path=output_path,         # Local base path
-        output_prefix=output_prefix,     # Object store base path
+    # Instantiate JsonFileWriter with the full output path
+    json_writer = JsonFileWriter(
+        output_path=output_path,  # Full path provided by caller
         typename=typename,
         # chunk_size=... (optional)
     )

     try:
-        # Get data using the Input class (e.g., Daft DataFrame)
-        daft_df = await sql_input.get_daft_dataframe()
-
-        # Write the DataFrame using the Output class
-        # This writes locally then uploads to object store path: {output_prefix}/{output_suffix}/{typename}/
-        await json_output.write_daft_dataframe(daft_df)
+        # Get data using the SQL client (e.g., fetch results)
+        results = []
+        async for batch in sql_client.run_query(prepared_query):
+            results.extend(batch)
+
+        # Write the data using the Writer class
+        # This writes locally then uploads to object store
+        # Convert results to DataFrame first if needed
+        import pandas as pd
+        df = pd.DataFrame(results)
+        await json_writer.write(df)

         # Get statistics (record count, chunk count) after writing
-        stats = await json_output.get_statistics(typename=typename)
+        stats = await json_writer.get_statistics(typename=typename)
         return stats.model_dump()

     except Exception as e:
         logger.error(f"Error executing query and writing output for {typename}: {e}", exc_info=True)
         raise
+
+# Example: Constructing the output path in the caller
+base_output_path = workflow_args.get("output_path", "")
+full_output_path = os.path.join(base_output_path, "raw", "table")
+await query_executor(sql_client, sql_query, workflow_args, full_output_path, "table")
 ```

-## Other Output Handlers
+## Other Writer Handlers

-*   **`ParquetOutput`:** Similar to `JsonOutput` but writes DataFrames to Parquet format files. Uses `daft.DataFrame.write_parquet()` or `pandas.DataFrame.to_parquet()`. Also uploads files to object storage after local processing.
-*   **`IcebergOutput`:** Writes DataFrames directly to an Iceberg table using `pyiceberg`.
+*   **`ParquetFileWriter`:** Similar to `JsonFileWriter` but writes DataFrames to Parquet format files. Uses `daft.DataFrame.write_parquet()` or `pandas.DataFrame.to_parquet()`. Also uploads files to object storage after local processing. Supports consolidation mode for efficient writing of large datasets.
+*   **`IcebergTableWriter`:** Writes DataFrames directly to an Iceberg table using `pyiceberg`. Designed for writing to managed table storage rather than files.

 ## Summary

-The `outputs` module complements the `inputs` module by providing classes to write data processed within activities. `JsonOutput` and `ParquetOutput` are commonly used for saving intermediate DataFrames to local files (and then uploading them to object storage), making the data available for subsequent activities like transformations.
\ No newline at end of file
+The I/O module provides both reader and writer classes for data persistence. `JsonFileWriter` and `ParquetFileWriter` are commonly used for saving intermediate DataFrames to local files (and then uploading them to object storage), making the data available for subsequent activities like transformations. The naming convention explicitly indicates the destination format: `*FileWriter` for file-based formats and `*TableWriter` for table-based formats like Iceberg.
\ No newline at end of file
diff --git a/docs/docs/concepts/services.md b/docs/docs/concepts/services.md
index 7317024d2..dcfde4f00 100644
--- a/docs/docs/concepts/services.md
+++ b/docs/docs/concepts/services.md
@@ -28,8 +28,36 @@ The services module provides unified interfaces for interacting with external st
    await ObjectStore.download_prefix(source_prefix, destination_dir=None, store_name=None)
    files = await ObjectStore.list_files(prefix="", store_name=None)

-   # Note: delete method exists but is not implemented
-   # await ObjectStore.delete(key, store_name=None)  # raises NotImplementedError
+   # Delete operations
+   await ObjectStore.delete_file(key, store_name=None)
+   await ObjectStore.delete_prefix(prefix, store_name=None)
+   ```
+
+   **Usage Examples:**
+   ```python
+   from application_sdk.services.objectstore import ObjectStore
+
+   # Upload a file
+   await ObjectStore.upload_file(
+       source="/local/path/data.json",
+       destination="workflows/wf123/data.json"
+   )
+
+   # Download a file
+   await ObjectStore.download_file(
+       source="workflows/wf123/data.json",
+       destination="/local/path/downloaded.json"
+   )
+
+   # List files with prefix
+   files = await ObjectStore.list_files(prefix="workflows/wf123/")
+   print(f"Found {len(files)} files")
+
+   # Delete a specific file
+   await ObjectStore.delete_file("workflows/wf123/data.json")
+
+   # Delete all files with a prefix (e.g., cleanup after workflow)
+   await ObjectStore.delete_prefix("workflows/wf123/")
    ```

 ### 2. **`StateStore` (`application_sdk.services.statestore`)**
@@ -343,7 +371,7 @@ print(f"Applied config: {applied_config}")

 ```python
 from application_sdk.services.eventstore import EventStore
-from application_sdk.events.models import Event
+from application_sdk.interceptors.models import Event

 # =============== Basic Event Publishing ===============

diff --git a/tests/unit/activities/metadata_extraction/test_sql.py b/tests/unit/activities/metadata_extraction/test_sql.py
index 9d37fd403..726330539 100644
--- a/tests/unit/activities/metadata_extraction/test_sql.py
+++ b/tests/unit/activities/metadata_extraction/test_sql.py
@@ -1,10 +1,12 @@
 """Unit tests for SQL metadata extraction activities (context-free)."""

+import os
 from typing import Any, Dict
 from unittest.mock import AsyncMock, Mock, patch

 import pytest

+from application_sdk.activities.common import sql_utils
 from application_sdk.activities.metadata_extraction.sql import (
     ActivityStatistics,
     BaseSQLMetadataExtractionActivities,
@@ -12,7 +14,7 @@
 )
 from application_sdk.clients.sql import BaseSQLClient
 from application_sdk.handlers.sql import BaseSQLHandler
-from application_sdk.outputs.parquet import ParquetOutput
+from application_sdk.io.parquet import ParquetFileWriter
 from application_sdk.transformers import TransformerInterface


@@ -138,17 +140,16 @@ def test_initialization_custom_classes(self):
     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.get_statistics",
+        "application_sdk.io.parquet.ParquetFileWriter.get_statistics",
+        new_callable=AsyncMock,
+    )
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
         new_callable=AsyncMock,
     )
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
-    @patch("application_sdk.outputs.json.JsonOutput.write_dataframe")
     async def test_query_executor_success(
         self,
-        mock_write_dataframe,
         mock_get_batched_dataframe,
-        mock_get_dataframe,
         mock_get_statistics,
         mock_exists,
         mock_makedirs,
@@ -158,16 +159,22 @@ async def test_query_executor_success(
         await mock_activities._set_state(sample_workflow_args)
         mock_dataframe = Mock()
         mock_dataframe.__len__ = Mock(return_value=10)
-        mock_get_dataframe.return_value = mock_dataframe
         mock_get_batched_dataframe.return_value = (df for df in [mock_dataframe])
-        mock_write_dataframe.return_value = None
         mock_get_statistics.return_value = ActivityStatistics(total_record_count=10)
-        sql_engine = Mock()
+
+        # Create a proper mock SQL client with async methods
+        sql_client = Mock()
+        sql_client.get_batched_results = AsyncMock(
+            return_value=(df for df in [mock_dataframe])
+        )
+
         sql_query = "SELECT * FROM test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"
         result = await mock_activities.query_executor(
-            sql_engine, sql_query, sample_workflow_args, output_suffix, typename
+            sql_client, sql_query, sample_workflow_args, output_path, typename
         )
         assert result is not None
         assert isinstance(result, ActivityStatistics)
@@ -176,15 +183,16 @@ async def test_query_executor_success(
     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.get_statistics",
+        "application_sdk.io.parquet.ParquetFileWriter.get_statistics",
+        new_callable=AsyncMock,
+    )
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
         new_callable=AsyncMock,
     )
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
     async def test_query_executor_empty_dataframe(
         self,
         mock_get_batched_dataframe,
-        mock_get_dataframe,
         mock_get_statistics,
         mock_exists,
         mock_makedirs,
@@ -194,15 +202,22 @@ async def test_query_executor_empty_dataframe(
         await mock_activities._set_state(sample_workflow_args)
         mock_dataframe = Mock()
         mock_dataframe.__len__ = Mock(return_value=0)
-        mock_get_dataframe.return_value = mock_dataframe
         mock_get_batched_dataframe.return_value = (df for df in [mock_dataframe])
         mock_get_statistics.return_value = ActivityStatistics(total_record_count=0)
-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = (
+            mock_get_batched_dataframe  # Assign the patched mock here
+        )
         sql_query = "SELECT * FROM empty_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"
         result = await mock_activities.query_executor(
-            sql_engine, sql_query, sample_workflow_args, output_suffix, typename
+            sql_client, sql_query, sample_workflow_args, output_path, typename
         )
         assert isinstance(result, ActivityStatistics)
         assert result.total_record_count == 0
@@ -300,15 +315,15 @@ async def test_fetch_procedures_success(
     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.get_statistics",
+        "application_sdk.io.parquet.ParquetFileWriter.get_statistics",
         new_callable=AsyncMock,
     )
     @patch(
-        "application_sdk.outputs.json.JsonOutput.get_statistics", new_callable=AsyncMock
+        "application_sdk.io.json.JsonFileWriter.get_statistics", new_callable=AsyncMock
     )
-    @patch("application_sdk.inputs.parquet.ParquetInput.get_dataframe")
+    @patch("application_sdk.io.parquet.ParquetFileReader.read")
     @patch(
-        "application_sdk.inputs.Input.download_files",
+        "application_sdk.io.parquet.download_files",
         new_callable=AsyncMock,
     )
     @patch("daft.read_parquet")
@@ -318,7 +333,7 @@ async def test_fetch_procedures_success(
     )
     @patch.object(MockTransformer, "transform_metadata")
     @patch(
-        "application_sdk.outputs.json.JsonOutput.write_daft_dataframe",
+        "application_sdk.io.json.JsonFileWriter.write",
         new_callable=AsyncMock,
     )
     async def test_transform_data_success(
@@ -328,7 +343,7 @@ async def test_transform_data_success(
         mock_is_empty,
         mock_read_parquet,
         mock_download_files,
-        mock_get_dataframe,
+        mock_read,
         mock_get_statistics_json,
         mock_get_statistics_parquet,
         mock_exists,
@@ -341,8 +356,8 @@ async def test_transform_data_success(
         mock_dataframe.__len__ = Mock(return_value=20)
         mock_dataframe.empty = False
         mock_dataframe.shape = (20, 1)
-        # Patch get_dataframe to return a list with one mock dataframe
-        mock_get_dataframe.return_value = [mock_dataframe]
+        # Patch read to return a list with one mock dataframe
+        mock_read.return_value = [mock_dataframe]
         mock_download_files.return_value = ["/test/path/raw/file1.parquet"]

         # Create a proper mock for daft DataFrame with chunked behavior
@@ -363,6 +378,9 @@ async def test_transform_data_success(
         assert result is not None
         assert isinstance(result, ActivityStatistics)
         assert result.total_record_count == 20
+        # Normalize path for cross-platform compatibility
+        expected_path = os.path.join("/test/path", "raw")
+        mock_download_files.assert_called_once_with(expected_path, ".parquet", None)
         mock_transform_metadata.assert_called_once()
         mock_write_daft_dataframe.assert_called_once()

@@ -370,11 +388,14 @@ async def test_transform_data_success(
     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.get_statistics",
+        "application_sdk.io.parquet.ParquetFileWriter.get_statistics",
+        new_callable=AsyncMock,
+    )
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
         new_callable=AsyncMock,
     )
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
-    @patch("application_sdk.outputs.parquet.ParquetOutput.write_batched_dataframe")
+    @patch("application_sdk.io.parquet.ParquetFileWriter.write_batches")
     async def test_query_executor_single_db_success(
         self,
         mock_write_batched_dataframe,
@@ -396,13 +417,19 @@ async def test_query_executor_single_db_success(
         mock_write_batched_dataframe.return_value = None
         mock_get_statistics.return_value = ActivityStatistics(total_record_count=10)

-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = mock_get_batched_dataframe
         sql_query = "SELECT * FROM test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

         result = await mock_activities.query_executor(
-            sql_engine, sql_query, sample_workflow_args, output_suffix, typename
+            sql_client, sql_query, sample_workflow_args, output_path, typename
         )

         assert result is not None
@@ -414,10 +441,13 @@ async def test_query_executor_single_db_success(
     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.get_statistics",
+        "application_sdk.io.parquet.ParquetFileWriter.get_statistics",
+        new_callable=AsyncMock,
+    )
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
         new_callable=AsyncMock,
     )
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
     async def test_query_executor_single_db_async_iterator(
         self,
         mock_get_batched_dataframe,
@@ -439,13 +469,19 @@ async def mock_async_iter():
         mock_get_batched_dataframe.return_value = mock_async_iter()
         mock_get_statistics.return_value = ActivityStatistics(total_record_count=5)

-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock()
         sql_query = "SELECT * FROM test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

         result = await mock_activities.query_executor(
-            sql_engine, sql_query, sample_workflow_args, output_suffix, typename
+            sql_client, sql_query, sample_workflow_args, output_path, typename
         )

         assert result is not None
@@ -454,7 +490,10 @@ async def mock_async_iter():

     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
+        new_callable=AsyncMock,
+    )
     async def test_query_executor_single_db_no_write_to_file(
         self,
         mock_get_batched_dataframe,
@@ -470,16 +509,22 @@ async def test_query_executor_single_db_no_write_to_file(
         mock_dataframe.__len__ = Mock(return_value=10)
         mock_get_batched_dataframe.return_value = [mock_dataframe]

-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock()
         sql_query = "SELECT * FROM test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

         result = await mock_activities.query_executor(
-            sql_engine,
+            sql_client,
             sql_query,
             sample_workflow_args,
-            output_suffix,
+            output_path,
             typename,
             write_to_file=False,
         )
@@ -490,59 +535,56 @@ async def test_query_executor_empty_query(
         self, mock_activities, sample_workflow_args
     ):
         """Test query_executor with empty query."""
-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock()
         sql_query = ""
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

         result = await mock_activities.query_executor(
-            sql_engine, sql_query, sample_workflow_args, output_suffix, typename
+            sql_client, sql_query, sample_workflow_args, output_path, typename
         )

         assert result is None

-    async def test_query_executor_missing_output_args(self, mock_activities):
-        """Test query_executor with missing output arguments."""
-        sql_engine = Mock()
-        sql_query = "SELECT * FROM test_table"
-        output_suffix = "test_suffix"
-        typename = "DATABASE"
-        workflow_args = {"workflow_id": "test"}  # Missing output_prefix and output_path
-
-        with pytest.raises(
-            ValueError, match="Output prefix and path must be specified"
-        ):
-            await mock_activities.query_executor(
-                sql_engine, sql_query, workflow_args, output_suffix, typename
-            )
-
     @patch("os.makedirs")
-    async def test_query_executor_no_sql_engine(
+    async def test_query_executor_no_sql_client(
         self, mock_makedirs, mock_activities, sample_workflow_args
     ):
-        """Test query_executor with no SQL engine."""
-        sql_engine = None
+        """Test query_executor with no SQL client."""
+        sql_client = None
         sql_query = "SELECT * FROM test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

-        with pytest.raises(ValueError, match="SQL engine must be provided"):
+        # The method validates sql_client and raises ValueError if not provided
+        with pytest.raises(ValueError, match="SQL client is required"):
             await mock_activities.query_executor(
-                sql_engine, sql_query, sample_workflow_args, output_suffix, typename
+                sql_client, sql_query, sample_workflow_args, output_path, typename
             )

     # Tests for multidb mode
     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.get_statistics",
+        "application_sdk.io.parquet.ParquetFileWriter.get_statistics",
         new_callable=AsyncMock,
     )
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
-    @patch("application_sdk.outputs.parquet.ParquetOutput.write_batched_dataframe")
-    @patch("application_sdk.activities.metadata_extraction.sql.get_database_names")
-    @patch("application_sdk.activities.metadata_extraction.sql.prepare_query")
-    @patch("application_sdk.activities.metadata_extraction.sql.parse_credentials_extra")
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
+        new_callable=AsyncMock,
+    )
+    @patch("application_sdk.io.parquet.ParquetFileWriter.write_batches")
+    @patch("application_sdk.activities.common.sql_utils.get_database_names")
+    @patch("application_sdk.activities.common.sql_utils.prepare_query")
+    @patch("application_sdk.activities.common.sql_utils.parse_credentials_extra")
     async def test_query_executor_multidb_success(
         self,
         mock_parse_credentials_extra,
@@ -583,13 +625,19 @@ async def test_query_executor_multidb_success(
         mock_write_batched_dataframe.return_value = None
         mock_get_statistics.return_value = ActivityStatistics(total_record_count=10)

-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock()
         sql_query = "SELECT * FROM {database_name}.test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

         result = await mock_activities.query_executor(
-            sql_engine, sql_query, sample_workflow_args, output_suffix, typename
+            sql_client, sql_query, sample_workflow_args, output_path, typename
         )

         assert result is not None
@@ -600,7 +648,7 @@ async def test_query_executor_multidb_success(

     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
-    @patch("application_sdk.activities.metadata_extraction.sql.get_database_names")
+    @patch("application_sdk.activities.common.sql_utils.get_database_names")
     async def test_query_executor_multidb_no_databases(
         self,
         mock_get_database_names,
@@ -627,22 +675,33 @@ async def test_query_executor_multidb_no_databases(
         # Mock no databases found
         mock_get_database_names.return_value = []

-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock()
         sql_query = "SELECT * FROM {database_name}.test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

         result = await mock_activities.query_executor(
-            sql_engine, sql_query, sample_workflow_args, output_suffix, typename
+            sql_client, sql_query, sample_workflow_args, output_path, typename
         )

         assert result is None

     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
-    @patch("application_sdk.activities.metadata_extraction.sql.get_database_names")
+    @patch("application_sdk.activities.common.sql_utils.get_database_names")
+    @patch(
+        "application_sdk.io.parquet.ParquetFileWriter.get_statistics",
+        new_callable=AsyncMock,
+    )
     async def test_query_executor_multidb_no_sql_client(
         self,
+        mock_get_statistics,
         mock_get_database_names,
         mock_exists,
         mock_makedirs,
@@ -659,16 +718,28 @@ async def test_query_executor_multidb_no_sql_client(
         state.sql_client = None

         mock_get_database_names.return_value = ["db1"]
+        mock_get_statistics.return_value = ActivityStatistics(total_record_count=0)

-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock()
         sql_query = "SELECT * FROM {database_name}.test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

-        with pytest.raises(ValueError, match="SQL client not initialized"):
-            await mock_activities.query_executor(
-                sql_engine, sql_query, sample_workflow_args, output_suffix, typename
-            )
+        # The method doesn't validate state.sql_client, it uses the passed sql_client
+        # So this test should actually succeed, not raise an exception
+        result = await mock_activities.query_executor(
+            sql_client, sql_query, sample_workflow_args, output_path, typename
+        )
+
+        # Should return ActivityStatistics even with state.sql_client = None
+        assert result is not None
+        assert isinstance(result, ActivityStatistics)

     # Tests for helper functions
     @patch("os.makedirs")
@@ -676,42 +747,33 @@ def test_setup_parquet_output_success(
         self, mock_makedirs, mock_activities, sample_workflow_args
     ):
         """Test _setup_parquet_output with valid arguments."""
-        output_suffix = "test_suffix"
-
-        result = mock_activities._setup_parquet_output(
-            sample_workflow_args, output_suffix, write_to_file=True
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
         )

+        result = mock_activities._setup_parquet_output(output_path, write_to_file=True)
+
         assert result is not None
-        assert isinstance(result, ParquetOutput)
+        assert isinstance(result, ParquetFileWriter)

     def test_setup_parquet_output_no_write_to_file(
         self, mock_activities, sample_workflow_args
     ):
         """Test _setup_parquet_output with write_to_file=False."""
-        output_suffix = "test_suffix"
-
-        result = mock_activities._setup_parquet_output(
-            sample_workflow_args, output_suffix, write_to_file=False
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
         )

-        assert result is None
-
-    def test_setup_parquet_output_missing_args(self, mock_activities):
-        """Test _setup_parquet_output with missing workflow arguments."""
-        workflow_args = {"workflow_id": "test"}  # Missing output_prefix and output_path
-        output_suffix = "test_suffix"
+        result = mock_activities._setup_parquet_output(output_path, write_to_file=False)

-        with pytest.raises(
-            ValueError, match="Output prefix and path must be specified"
-        ):
-            mock_activities._setup_parquet_output(
-                workflow_args, output_suffix, write_to_file=True
-            )
+        assert result is None

-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.write_batched_dataframe",
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
+        new_callable=AsyncMock,
+    )
+    @patch(
+        "application_sdk.io.parquet.ParquetFileWriter.write_batches",
         new_callable=AsyncMock,
     )
     async def test_execute_single_db_success_with_write(
@@ -720,11 +782,15 @@ async def test_execute_single_db_success_with_write(
         mock_get_batched_dataframe,
         mock_activities,
     ):
-        """Test _execute_single_db with write_to_file=True."""
-        sql_engine = Mock()
+        """Test execute_single_db with write_to_file=True."""
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = mock_get_batched_dataframe
         prepared_query = "SELECT * FROM test_table"
         parquet_output = Mock()
-        parquet_output.write_batched_dataframe = AsyncMock(return_value=None)
+        parquet_output.write_batches = AsyncMock(return_value=None)

         # Mock dataframe
         mock_dataframe = Mock()
@@ -732,23 +798,30 @@ async def test_execute_single_db_success_with_write(
         mock_get_batched_dataframe.return_value = [mock_dataframe]
         mock_write_batched_dataframe.return_value = None

-        success, result = await mock_activities._execute_single_db(
-            sql_engine, prepared_query, parquet_output, write_to_file=True
+        success, result = await sql_utils.execute_single_db(
+            sql_client, prepared_query, parquet_output, write_to_file=True
         )

         assert success is True
         assert result is None
         mock_get_batched_dataframe.assert_called_once()
-        parquet_output.write_batched_dataframe.assert_called_once()
+        parquet_output.write_batches.assert_called_once()

-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
+        new_callable=AsyncMock,
+    )
     async def test_execute_single_db_success_without_write(
         self,
         mock_get_batched_dataframe,
         mock_activities,
     ):
-        """Test _execute_single_db with write_to_file=False."""
-        sql_engine = Mock()
+        """Test execute_single_db with write_to_file=False."""
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = mock_get_batched_dataframe
         prepared_query = "SELECT * FROM test_table"
         parquet_output = Mock()

@@ -758,8 +831,8 @@ async def test_execute_single_db_success_without_write(
         mock_batched_iter = [mock_dataframe]
         mock_get_batched_dataframe.return_value = mock_batched_iter

-        success, result = await mock_activities._execute_single_db(
-            sql_engine, prepared_query, parquet_output, write_to_file=False
+        success, result = await sql_utils.execute_single_db(
+            sql_client, prepared_query, parquet_output, write_to_file=False
         )

         assert success is True
@@ -767,40 +840,55 @@ async def test_execute_single_db_success_without_write(
         mock_get_batched_dataframe.assert_called_once()

     async def test_execute_single_db_no_prepared_query(self, mock_activities):
-        """Test _execute_single_db with no prepared query."""
-        sql_engine = Mock()
+        """Test execute_single_db with no prepared query."""
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock()
         prepared_query = None
         parquet_output = Mock()

-        success, result = await mock_activities._execute_single_db(
-            sql_engine, prepared_query, parquet_output, write_to_file=True
+        success, result = await sql_utils.execute_single_db(
+            sql_client, prepared_query, parquet_output, write_to_file=True
         )

         assert success is False
         assert result is None

-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
+        new_callable=AsyncMock,
+    )
     async def test_execute_single_db_exception(
         self,
         mock_get_batched_dataframe,
         mock_activities,
     ):
-        """Test _execute_single_db with exception during execution."""
-        sql_engine = Mock()
+        """Test execute_single_db with exception during execution."""
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = mock_get_batched_dataframe
         prepared_query = "SELECT * FROM test_table"
         parquet_output = Mock()
+        parquet_output.write_batches = AsyncMock()

         # Mock exception
         mock_get_batched_dataframe.side_effect = Exception("Database error")

         with pytest.raises(Exception, match="Database error"):
-            await mock_activities._execute_single_db(
-                sql_engine, prepared_query, parquet_output, write_to_file=True
+            await sql_utils.execute_single_db(
+                sql_client, prepared_query, parquet_output, write_to_file=True
             )

-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.write_batched_dataframe",
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
+        new_callable=AsyncMock,
+    )
+    @patch(
+        "application_sdk.io.parquet.ParquetFileWriter.write_batches",
         new_callable=AsyncMock,
     )
     async def test_execute_single_db_async_iterator(
@@ -809,11 +897,17 @@ async def test_execute_single_db_async_iterator(
         mock_get_batched_dataframe,
         mock_activities,
     ):
-        """Test _execute_single_db with async iterator."""
-        sql_engine = Mock()
+        """Test execute_single_db with async iterator."""
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = (
+            mock_get_batched_dataframe  # Assign the patched mock here
+        )
         prepared_query = "SELECT * FROM test_table"
         parquet_output = Mock()
-        parquet_output.write_batched_dataframe = AsyncMock(return_value=None)
+        parquet_output.write_batches = AsyncMock(return_value=None)

         # Mock async iterator
         async def mock_async_iter():
@@ -824,30 +918,33 @@ async def mock_async_iter():
         mock_get_batched_dataframe.return_value = mock_async_iter()
         mock_write_batched_dataframe.return_value = None

-        success, result = await mock_activities._execute_single_db(
-            sql_engine, prepared_query, parquet_output, write_to_file=True
+        success, result = await sql_utils.execute_single_db(
+            sql_client, prepared_query, parquet_output, write_to_file=True
         )

         assert success is True
         assert result is None
         mock_get_batched_dataframe.assert_called_once()
-        parquet_output.write_batched_dataframe.assert_called_once()
+        parquet_output.write_batches.assert_called_once()

     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.get_statistics",
+        "application_sdk.io.parquet.ParquetFileWriter.get_statistics",
         new_callable=AsyncMock,
     )
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
-    @patch("application_sdk.outputs.parquet.ParquetOutput.write_batched_dataframe")
     @patch(
-        "application_sdk.outputs.parquet.ParquetOutput.write_dataframe",
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
         new_callable=AsyncMock,
     )
-    @patch("application_sdk.activities.metadata_extraction.sql.get_database_names")
-    @patch("application_sdk.activities.metadata_extraction.sql.prepare_query")
-    @patch("application_sdk.activities.metadata_extraction.sql.parse_credentials_extra")
+    @patch("application_sdk.io.parquet.ParquetFileWriter.write_batches")
+    @patch(
+        "application_sdk.io.parquet.ParquetFileWriter.write",
+        new_callable=AsyncMock,
+    )
+    @patch("application_sdk.activities.common.sql_utils.get_database_names")
+    @patch("application_sdk.activities.common.sql_utils.prepare_query")
+    @patch("application_sdk.activities.common.sql_utils.parse_credentials_extra")
     async def test_query_executor_multidb_concatenate_success(
         self,
         mock_parse_credentials_extra,
@@ -886,19 +983,32 @@ async def test_query_executor_multidb_concatenate_success(
         import pandas as pd

         mock_dataframe = pd.DataFrame({"col1": [1, 2, 3], "col2": ["a", "b", "c"]})
-        mock_get_batched_dataframe.return_value = [mock_dataframe]
+
+        # Create an async iterator that yields the dataframe
+        async def async_dataframe_iterator():
+            yield mock_dataframe
+
+        mock_get_batched_dataframe.return_value = async_dataframe_iterator()
         mock_get_statistics.return_value = ActivityStatistics(total_record_count=10)

-        sql_engine = Mock()
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock(
+            return_value=async_dataframe_iterator()
+        )
         sql_query = "SELECT * FROM {database_name}.test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

         result = await mock_activities.query_executor(
-            sql_engine,
+            sql_client,
             sql_query,
             sample_workflow_args,
-            output_suffix,
+            output_path,
             typename,
             write_to_file=False,
             concatenate=True,
@@ -910,10 +1020,13 @@ async def test_query_executor_multidb_concatenate_success(

     @patch("os.makedirs")
     @patch("os.path.exists", return_value=True)
-    @patch("application_sdk.inputs.sql_query.SQLQueryInput.get_batched_dataframe")
-    @patch("application_sdk.activities.metadata_extraction.sql.get_database_names")
-    @patch("application_sdk.activities.metadata_extraction.sql.prepare_query")
-    @patch("application_sdk.activities.metadata_extraction.sql.parse_credentials_extra")
+    @patch(
+        "application_sdk.clients.sql.BaseSQLClient.get_batched_results",
+        new_callable=AsyncMock,
+    )
+    @patch("application_sdk.activities.common.sql_utils.get_database_names")
+    @patch("application_sdk.activities.common.sql_utils.prepare_query")
+    @patch("application_sdk.activities.common.sql_utils.parse_credentials_extra")
     async def test_query_executor_multidb_concatenate_return_dataframe(
         self,
         mock_parse_credentials_extra,
@@ -949,18 +1062,31 @@ async def test_query_executor_multidb_concatenate_return_dataframe(
         import pandas as pd

         mock_dataframe = pd.DataFrame({"col1": [1, 2, 3], "col2": ["a", "b", "c"]})
-        mock_get_batched_dataframe.return_value = [mock_dataframe]

-        sql_engine = Mock()
+        # Create an async iterator that yields the dataframe
+        async def async_dataframe_iterator():
+            yield mock_dataframe
+
+        mock_get_batched_dataframe.return_value = async_dataframe_iterator()
+
+        # Create a proper mock SQL client with dict-like credentials
+        sql_client = Mock()
+        sql_client.credentials = {"extra": "{}"}
+        sql_client.load = AsyncMock()
+        sql_client.get_batched_results = AsyncMock(
+            return_value=async_dataframe_iterator()
+        )
         sql_query = "SELECT * FROM {database_name}.test_table"
-        output_suffix = "test_suffix"
+        output_path = os.path.join(
+            sample_workflow_args["output_path"], "raw", "database"
+        )
         typename = "DATABASE"

         result = await mock_activities.query_executor(
-            sql_engine,
+            sql_client,
             sql_query,
             sample_workflow_args,
-            output_suffix,
+            output_path,
             typename,
             write_to_file=False,
             concatenate=True,
diff --git a/tests/unit/clients/test_async_sql_client.py b/tests/unit/clients/test_async_sql_client.py
index a5d3fe45d..c71cb43b4 100644
--- a/tests/unit/clients/test_async_sql_client.py
+++ b/tests/unit/clients/test_async_sql_client.py
@@ -92,13 +92,12 @@ async def test_load(
     assert async_sql_client.connection is None


-@patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-async def test_fetch_metadata(mock_run_query: Any, handler: BaseSQLHandler):
+async def test_fetch_metadata(handler: BaseSQLHandler):
     data = [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]

     import pandas as pd

-    mock_run_query.return_value = pd.DataFrame(data)
+    handler.sql_client.get_results = AsyncMock(return_value=pd.DataFrame(data))

     # Sample SQL query
     handler.metadata_sql = "SELECT * FROM information_schema.tables"
@@ -110,18 +109,17 @@ async def test_fetch_metadata(mock_run_query: Any, handler: BaseSQLHandler):

     # Assertions
     assert result == [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]
-    mock_run_query.assert_called_once_with()
+    handler.sql_client.get_results.assert_called_once_with(
+        "SELECT * FROM information_schema.tables"
+    )


-@patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-async def test_fetch_metadata_without_database_alias_key(
-    mock_run_query: Any, handler: BaseSQLHandler
-):
+async def test_fetch_metadata_without_database_alias_key(handler: BaseSQLHandler):
     data = [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]

     import pandas as pd

-    mock_run_query.return_value = pd.DataFrame(data)
+    handler.sql_client.get_results = AsyncMock(return_value=pd.DataFrame(data))

     # Sample SQL query
     handler.metadata_sql = "SELECT * FROM information_schema.tables"
@@ -133,17 +131,16 @@ async def test_fetch_metadata_without_database_alias_key(

     # Assertions
     assert result == [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]
-    mock_run_query.assert_called_once_with()
+    handler.sql_client.get_results.assert_called_once_with(
+        "SELECT * FROM information_schema.tables"
+    )


-@patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-async def test_fetch_metadata_with_result_keys(
-    mock_run_query: Any, handler: BaseSQLHandler
-):
+async def test_fetch_metadata_with_result_keys(handler: BaseSQLHandler):
     data = [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]
     import pandas as pd

-    mock_run_query.return_value = pd.DataFrame(data)
+    handler.sql_client.get_results = AsyncMock(return_value=pd.DataFrame(data))

     # Sample SQL query
     handler.metadata_sql = "SELECT * FROM information_schema.tables"
@@ -156,14 +153,15 @@ async def test_fetch_metadata_with_result_keys(

     # Assertions
     assert result == [{"DATABASE": "test_db", "SCHEMA": "test_schema"}]
-    mock_run_query.assert_called_once_with()
+    handler.sql_client.get_results.assert_called_once_with(
+        "SELECT * FROM information_schema.tables"
+    )


-@patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-async def test_fetch_metadata_with_error(
-    mock_run_query: AsyncMock, handler: BaseSQLHandler
-):
-    mock_run_query.side_effect = Exception("Simulated query failure")
+async def test_fetch_metadata_with_error(handler: BaseSQLHandler):
+    handler.sql_client.get_results = AsyncMock(
+        side_effect=Exception("Simulated query failure")
+    )

     # Sample SQL query
     handler.metadata_sql = "SELECT * FROM information_schema.tables"
@@ -175,7 +173,9 @@ async def test_fetch_metadata_with_error(
         await handler.prepare_metadata()

     # Assertions
-    mock_run_query.assert_called_once_with()
+    handler.sql_client.get_results.assert_called_once_with(
+        "SELECT * FROM information_schema.tables"
+    )


 @pytest.mark.asyncio
diff --git a/tests/unit/clients/test_sql_client.py b/tests/unit/clients/test_sql_client.py
index 77a30fa05..4ec07c075 100644
--- a/tests/unit/clients/test_sql_client.py
+++ b/tests/unit/clients/test_sql_client.py
@@ -102,13 +102,12 @@ def test_load_property_based(
         assert sql_client.connection is None


-@patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-async def test_fetch_metadata(mock_run_query: Any, handler: BaseSQLHandler):
+async def test_fetch_metadata(handler: BaseSQLHandler):
     """Test basic metadata fetching with fixed configuration"""
     data = [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]
     import pandas as pd

-    mock_run_query.return_value = pd.DataFrame(data)
+    handler.sql_client.get_results = AsyncMock(return_value=pd.DataFrame(data))

     # Sample SQL query
     handler.metadata_sql = "SELECT * FROM information_schema.tables"
@@ -118,7 +117,9 @@ async def test_fetch_metadata(mock_run_query: Any, handler: BaseSQLHandler):

     # Assertions
     assert result == [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]
-    mock_run_query.assert_called_once_with()
+    handler.sql_client.get_results.assert_called_once_with(
+        "SELECT * FROM information_schema.tables"
+    )


 @given(args=metadata_args_strategy, data=sql_data_strategy)
@@ -129,58 +130,52 @@ async def test_fetch_metadata_property_based(
     data: List[Dict[str, Any]],
 ):
     """Property-based test for fetching metadata with various arguments and data"""
-    with patch(
-        "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe"
-    ) as mock_run_query:
-        # Update handler with the test arguments
-        if "database_alias_key" in args:
-            handler.database_alias_key = args["database_alias_key"]
-        if "schema_alias_key" in args:
-            handler.schema_alias_key = args["schema_alias_key"]
-        if "database_result_key" in args:
-            handler.database_result_key = args["database_result_key"]
-        if "schema_result_key" in args:
-            handler.schema_result_key = args["schema_result_key"]
-
-        # Create test data with the required keys
-        test_data: List[Dict[str, str]] = []
-        for row in data:
-            test_row = {
-                handler.database_alias_key: row.get("database", "test_db"),
-                handler.schema_alias_key: row.get("schema", "test_schema"),
-            }
-            test_data.append(test_row)
-
-        import pandas as pd
-
-        mock_run_query.return_value = pd.DataFrame(test_data)
-
-        handler.metadata_sql = args["metadata_sql"]
-
-        # Run prepare_metadata
-        result = await handler.prepare_metadata()
-
-        # Assertions
-        assert len(result) == len(test_data)
-        mock_run_query.assert_called_once_with()
-
-        # Verify the keys in the result
-        for row in result:
-            if handler.database_result_key:
-                assert handler.database_result_key in row
-            if handler.schema_result_key:
-                assert handler.schema_result_key in row
-
-
-@patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-async def test_fetch_metadata_without_database_alias_key(
-    mock_run_query: Any, handler: BaseSQLHandler
-):
+    # Update handler with the test arguments
+    if "database_alias_key" in args:
+        handler.database_alias_key = args["database_alias_key"]
+    if "schema_alias_key" in args:
+        handler.schema_alias_key = args["schema_alias_key"]
+    if "database_result_key" in args:
+        handler.database_result_key = args["database_result_key"]
+    if "schema_result_key" in args:
+        handler.schema_result_key = args["schema_result_key"]
+
+    # Create test data with the required keys
+    test_data: List[Dict[str, str]] = []
+    for row in data:
+        test_row = {
+            handler.database_alias_key: row.get("database", "test_db"),
+            handler.schema_alias_key: row.get("schema", "test_schema"),
+        }
+        test_data.append(test_row)
+
+    import pandas as pd
+
+    handler.sql_client.get_results = AsyncMock(return_value=pd.DataFrame(test_data))
+
+    handler.metadata_sql = args["metadata_sql"]
+
+    # Run prepare_metadata
+    result = await handler.prepare_metadata()
+
+    # Assertions
+    assert len(result) == len(test_data)
+    handler.sql_client.get_results.assert_called_once_with(args["metadata_sql"])
+
+    # Verify the keys in the result
+    for row in result:
+        if handler.database_result_key:
+            assert handler.database_result_key in row
+        if handler.schema_result_key:
+            assert handler.schema_result_key in row
+
+
+async def test_fetch_metadata_without_database_alias_key(handler: BaseSQLHandler):
     """Test metadata fetching without database alias key"""
     data = [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]
     import pandas as pd

-    mock_run_query.return_value = pd.DataFrame(data)
+    handler.sql_client.get_results = AsyncMock(return_value=pd.DataFrame(data))

     # Sample SQL query
     handler.metadata_sql = "SELECT * FROM information_schema.tables"
@@ -192,18 +187,17 @@ async def test_fetch_metadata_without_database_alias_key(

     # Assertions
     assert result == [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]
-    mock_run_query.assert_called_once_with()
+    handler.sql_client.get_results.assert_called_once_with(
+        "SELECT * FROM information_schema.tables"
+    )


-@patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-async def test_fetch_metadata_with_result_keys(
-    mock_run_query: Any, handler: BaseSQLHandler
-):
+async def test_fetch_metadata_with_result_keys(handler: BaseSQLHandler):
     """Test metadata fetching with custom result keys"""
     data = [{"TABLE_CATALOG": "test_db", "TABLE_SCHEMA": "test_schema"}]
     import pandas as pd

-    mock_run_query.return_value = pd.DataFrame(data)
+    handler.sql_client.get_results = AsyncMock(return_value=pd.DataFrame(data))

     # Sample SQL query
     handler.metadata_sql = "SELECT * FROM information_schema.tables"
@@ -215,15 +209,16 @@ async def test_fetch_metadata_with_result_keys(

     # Assertions
     assert result == [{"DATABASE": "test_db", "SCHEMA": "test_schema"}]
-    mock_run_query.assert_called_once_with()
+    handler.sql_client.get_results.assert_called_once_with(
+        "SELECT * FROM information_schema.tables"
+    )


-@patch("application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe")
-async def test_fetch_metadata_with_error(
-    mock_run_query: AsyncMock, handler: BaseSQLHandler
-):
+async def test_fetch_metadata_with_error(handler: BaseSQLHandler):
     """Test error handling in metadata fetching"""
-    mock_run_query.side_effect = Exception("Simulated query failure")
+    handler.sql_client.get_results = AsyncMock(
+        side_effect=Exception("Simulated query failure")
+    )

     # Sample SQL query
     handler.metadata_sql = "SELECT * FROM information_schema.tables"
@@ -235,7 +230,9 @@ async def test_fetch_metadata_with_error(
         await handler.prepare_metadata()

     # Assertions
-    mock_run_query.assert_called_once_with()
+    handler.sql_client.get_results.assert_called_once_with(
+        "SELECT * FROM information_schema.tables"
+    )


 @pytest.mark.asyncio
diff --git a/tests/unit/common/test_utils_file_discovery.py b/tests/unit/common/test_utils_file_discovery.py
index e149ea829..2a0f799e3 100644
--- a/tests/unit/common/test_utils_file_discovery.py
+++ b/tests/unit/common/test_utils_file_discovery.py
@@ -3,7 +3,7 @@
 import tempfile
 from pathlib import Path

-from application_sdk.activities.common.utils import find_local_files_by_extension
+from application_sdk.io._utils import find_local_files_by_extension


 class TestFindFilesByExtension:
diff --git a/tests/unit/handlers/sql/test_auth.py b/tests/unit/handlers/sql/test_auth.py
index 3df2f00a6..ddffd9c38 100644
--- a/tests/unit/handlers/sql/test_auth.py
+++ b/tests/unit/handlers/sql/test_auth.py
@@ -1,4 +1,4 @@
-from unittest.mock import AsyncMock, Mock, patch
+from unittest.mock import AsyncMock, Mock

 import pandas as pd
 import pytest
@@ -26,113 +26,92 @@ async def test_successful_authentication(self, handler: BaseSQLHandler) -> None:
         # Mock a successful DataFrame response
         mock_df = pd.DataFrame({"result": [1]})

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = mock_df
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=mock_df)

-            # Test authentication
-            result = await handler.test_auth()
+        # Test authentication
+        result = await handler.test_auth()

-            # Verify success
-            assert result is True
-            mock_get_dataframe.assert_called_once()
+        # Verify success
+        assert result is True
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_failed_authentication(self, handler: BaseSQLHandler) -> None:
         """Test failed authentication with invalid credentials"""
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            # Mock a failed response
-            mock_get_dataframe.side_effect = Exception("Authentication failed")
+        # Mock the sql_client.get_results method to raise an exception
+        handler.sql_client.get_results = AsyncMock(
+            side_effect=Exception("Authentication failed")
+        )

-            # Test authentication and expect exception
-            with pytest.raises(Exception) as exc_info:
-                await handler.test_auth()
+        # Test authentication and expect exception
+        with pytest.raises(Exception) as exc_info:
+            await handler.test_auth()

-            # Verify error
-            assert str(exc_info.value) == "Authentication failed"
-            mock_get_dataframe.assert_called_once()
+        # Verify error
+        assert str(exc_info.value) == "Authentication failed"
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_empty_dataframe_authentication(
         self, handler: BaseSQLHandler
     ) -> None:
         """Test authentication with empty DataFrame response"""
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            # Mock an empty DataFrame
-            mock_df = pd.DataFrame({})
-            mock_get_dataframe.return_value = mock_df
+        # Mock the sql_client.get_results method directly
+        mock_df = pd.DataFrame({})
+        handler.sql_client.get_results = AsyncMock(return_value=mock_df)

-            # Test authentication should still succeed as DataFrame is valid
-            result = await handler.test_auth()
+        # Test authentication should still succeed as DataFrame is valid
+        result = await handler.test_auth()

-            # Verify success
-            assert result is True
-            mock_get_dataframe.assert_called_once()
+        # Verify success
+        assert result is True
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_none_dataframe_authentication(self, handler: BaseSQLHandler) -> None:
         """Test authentication with None DataFrame response"""
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            # Mock None response
-            mock_get_dataframe.return_value = None
+        # Mock the sql_client.get_results method to return None
+        handler.sql_client.get_results = AsyncMock(return_value=None)

-            # Test authentication and expect exception
-            with pytest.raises(AttributeError) as exc_info:
-                await handler.test_auth()
+        # Test authentication and expect exception
+        with pytest.raises(AttributeError) as exc_info:
+            await handler.test_auth()

-            # Verify error and call
-            assert "object has no attribute 'to_dict'" in str(exc_info.value)
-            mock_get_dataframe.assert_called_once()
+        # Verify error and call
+        assert "object has no attribute 'to_dict'" in str(exc_info.value)
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_malformed_dataframe_authentication(
         self, handler: BaseSQLHandler
     ) -> None:
         """Test authentication with malformed DataFrame that raises on to_dict"""
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            # Create a mock DataFrame that raises on to_dict
-            mock_df = Mock(spec=pd.DataFrame)
-            mock_df.to_dict.side_effect = Exception("DataFrame conversion error")
-            mock_get_dataframe.return_value = mock_df
-
-            # Test authentication and expect exception
-            with pytest.raises(Exception) as exc_info:
-                await handler.test_auth()
-
-            # Verify error and call
-            assert str(exc_info.value) == "DataFrame conversion error"
-            mock_get_dataframe.assert_called_once()
+        # Create a mock DataFrame that raises on to_dict
+        mock_df = Mock(spec=pd.DataFrame)
+        mock_df.to_dict.side_effect = Exception("DataFrame conversion error")
+        handler.sql_client.get_results = AsyncMock(return_value=mock_df)
+
+        # Test authentication and expect exception
+        with pytest.raises(Exception) as exc_info:
+            await handler.test_auth()
+
+        # Verify error and call
+        assert str(exc_info.value) == "DataFrame conversion error"
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_custom_sql_query(self, handler: BaseSQLHandler) -> None:
         """Test authentication with custom SQL query"""
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            # Set custom test query
-            handler.test_authentication_sql = "SELECT version();"
-            mock_df = pd.DataFrame({"version": ["test_version"]})
-            mock_get_dataframe.return_value = mock_df
-
-            # Test authentication
-            result = await handler.test_auth()
-
-            # Verify success and correct query was used
-            assert result is True
-            mock_get_dataframe.assert_called_once()
-            assert handler.test_authentication_sql == "SELECT version();"
+        # Set custom test query
+        handler.test_authentication_sql = "SELECT version();"
+        mock_df = pd.DataFrame({"version": ["test_version"]})
+        handler.sql_client.get_results = AsyncMock(return_value=mock_df)
+
+        # Test authentication
+        result = await handler.test_auth()
+
+        # Verify success and correct query was used
+        assert result is True
+        handler.sql_client.get_results.assert_called_once()
+        assert handler.test_authentication_sql == "SELECT version();"
diff --git a/tests/unit/handlers/sql/test_check_schemas_and_databases.py b/tests/unit/handlers/sql/test_check_schemas_and_databases.py
index 92d8bd931..210f415c0 100644
--- a/tests/unit/handlers/sql/test_check_schemas_and_databases.py
+++ b/tests/unit/handlers/sql/test_check_schemas_and_databases.py
@@ -1,4 +1,4 @@
-from unittest.mock import AsyncMock, Mock, patch
+from unittest.mock import AsyncMock, Mock

 import pandas as pd
 import pytest
@@ -34,20 +34,16 @@ async def test_successful_check(self, handler: BaseSQLHandler) -> None:
             {"TABLE_CATALOG": ["db1", "db1"], "TABLE_SCHEMA": ["schema1", "schema2"]}
         )

-        # Mock the SQLQueryInput.get_daft_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)

-            payload = {"metadata": {"include-filter": '{"^db1$": ["^schema1$"]}'}}
-            result = await handler.check_schemas_and_databases(payload)
+        payload = {"metadata": {"include-filter": '{"^db1$": ["^schema1$"]}'}}
+        result = await handler.check_schemas_and_databases(payload)

-            assert result["success"] is True
-            assert result["successMessage"] == "Schemas and Databases check successful"
-            assert result["failureMessage"] == ""
-            mock_get_dataframe.assert_called_once()
+        assert result["success"] is True
+        assert result["successMessage"] == "Schemas and Databases check successful"
+        assert result["failureMessage"] == ""
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_invalid_database(self, handler: BaseSQLHandler) -> None:
@@ -57,22 +53,16 @@ async def test_invalid_database(self, handler: BaseSQLHandler) -> None:
             {"TABLE_CATALOG": ["db1"], "TABLE_SCHEMA": ["schema1"]}
         )

-        # Mock the SQLQueryInput.get_daft_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)

-            payload = {
-                "metadata": {"include-filter": '{"^invalid_db$": ["^schema1$"]}'}
-            }
-            result = await handler.check_schemas_and_databases(payload)
+        payload = {"metadata": {"include-filter": '{"^invalid_db$": ["^schema1$"]}'}}
+        result = await handler.check_schemas_and_databases(payload)

-            assert result["success"] is False
-            assert result["successMessage"] == ""
-            assert "invalid_db database" in result["failureMessage"]
-            mock_get_dataframe.assert_called_once()
+        assert result["success"] is False
+        assert result["successMessage"] == ""
+        assert "invalid_db database" in result["failureMessage"]
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_invalid_schema(self, handler: BaseSQLHandler) -> None:
@@ -82,22 +72,16 @@ async def test_invalid_schema(self, handler: BaseSQLHandler) -> None:
             {"TABLE_CATALOG": ["db1"], "TABLE_SCHEMA": ["schema1"]}
         )

-        # Mock the SQLQueryInput.get_daft_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)

-            payload = {
-                "metadata": {"include-filter": '{"^db1$": ["^invalid_schema$"]}'}
-            }
-            result = await handler.check_schemas_and_databases(payload)
+        payload = {"metadata": {"include-filter": '{"^db1$": ["^invalid_schema$"]}'}}
+        result = await handler.check_schemas_and_databases(payload)

-            assert result["success"] is False
-            assert result["successMessage"] == ""
-            assert "db1.invalid_schema schema" in result["failureMessage"]
-            mock_get_dataframe.assert_called_once()
+        assert result["success"] is False
+        assert result["successMessage"] == ""
+        assert "db1.invalid_schema schema" in result["failureMessage"]
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_wildcard_schema(self, handler: BaseSQLHandler) -> None:
@@ -107,20 +91,16 @@ async def test_wildcard_schema(self, handler: BaseSQLHandler) -> None:
             {"TABLE_CATALOG": ["db1", "db1"], "TABLE_SCHEMA": ["schema1", "schema2"]}
         )

-        # Mock the SQLQueryInput.get_daft_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)

-            payload = {"metadata": {"include-filter": '{"^db1$": "*"}'}}
-            result = await handler.check_schemas_and_databases(payload)
+        payload = {"metadata": {"include-filter": '{"^db1$": "*"}'}}
+        result = await handler.check_schemas_and_databases(payload)

-            assert result["success"] is True
-            assert result["successMessage"] == "Schemas and Databases check successful"
-            assert result["failureMessage"] == ""
-            mock_get_dataframe.assert_called_once()
+        assert result["success"] is True
+        assert result["successMessage"] == "Schemas and Databases check successful"
+        assert result["failureMessage"] == ""
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_empty_metadata(self, handler: BaseSQLHandler) -> None:
@@ -128,20 +108,16 @@ async def test_empty_metadata(self, handler: BaseSQLHandler) -> None:
         # Test data - empty DataFrame
         test_data = pd.DataFrame({"TABLE_CATALOG": [], "TABLE_SCHEMA": []})

-        # Mock the SQLQueryInput.get_daft_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)

-            payload = {"metadata": {}}
-            result = await handler.check_schemas_and_databases(payload)
+        payload = {"metadata": {}}
+        result = await handler.check_schemas_and_databases(payload)

-            assert result["success"] is True
-            assert result["successMessage"] == "Schemas and Databases check successful"
-            assert result["failureMessage"] == ""
-            mock_get_dataframe.assert_called_once()
+        assert result["success"] is True
+        assert result["successMessage"] == "Schemas and Databases check successful"
+        assert result["failureMessage"] == ""
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_invalid_json_filter(self, handler: BaseSQLHandler) -> None:
@@ -149,39 +125,34 @@ async def test_invalid_json_filter(self, handler: BaseSQLHandler) -> None:
         # Test data
         test_data = pd.DataFrame({"TABLE_CATALOG": [], "TABLE_SCHEMA": []})

-        # Mock the SQLQueryInput.get_daft_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)

-            payload = {"metadata": {"include-filter": "invalid json"}}
-            result = await handler.check_schemas_and_databases(payload)
+        payload = {"metadata": {"include-filter": "invalid json"}}
+        result = await handler.check_schemas_and_databases(payload)

-            assert result["success"] is False
-            assert result["successMessage"] == ""
-            assert "Schemas and Databases check failed" in result["failureMessage"]
-            assert "error" in result
-            mock_get_dataframe.assert_called_once()
+        assert result["success"] is False
+        assert result["successMessage"] == ""
+        assert "Schemas and Databases check failed" in result["failureMessage"]
+        assert "error" in result
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_prepare_metadata_error(self, handler: BaseSQLHandler) -> None:
         """Test check when prepare_metadata raises an error"""
-        # Mock the SQLQueryInput.get_dataframe to raise an exception
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-            side_effect=Exception("Database error"),
-        ) as mock_get_dataframe:
-            payload = {"metadata": {"include-filter": "{}"}}
-            result = await handler.check_schemas_and_databases(payload)
-
-            assert result["success"] is False
-            assert result["successMessage"] == ""
-            assert "Schemas and Databases check failed" in result["failureMessage"]
-            assert result["error"] == "Database error"
-            mock_get_dataframe.assert_called_once()
+        # Mock the sql_client.get_results method to raise an exception
+        handler.sql_client.get_results = AsyncMock(
+            side_effect=Exception("Database error")
+        )
+
+        payload = {"metadata": {"include-filter": "{}"}}
+        result = await handler.check_schemas_and_databases(payload)
+
+        assert result["success"] is False
+        assert result["successMessage"] == ""
+        assert "Schemas and Databases check failed" in result["failureMessage"]
+        assert result["error"] == "Database error"
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_multiple_databases_and_schemas(
@@ -196,24 +167,20 @@ async def test_multiple_databases_and_schemas(
             }
         )

-        # Mock the SQLQueryInput.get_daft_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
-
-            payload = {
-                "metadata": {
-                    "include-filter": '{"^db1$": ["^schema1$", "^schema2$"], "^db2$": ["^schema1$"]}'
-                }
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)
+
+        payload = {
+            "metadata": {
+                "include-filter": '{"^db1$": ["^schema1$", "^schema2$"], "^db2$": ["^schema1$"]}'
             }
-            result = await handler.check_schemas_and_databases(payload)
+        }
+        result = await handler.check_schemas_and_databases(payload)

-            assert result["success"] is True
-            assert result["successMessage"] == "Schemas and Databases check successful"
-            assert result["failureMessage"] == ""
-            mock_get_dataframe.assert_called_once()
+        assert result["success"] is True
+        assert result["successMessage"] == "Schemas and Databases check successful"
+        assert result["failureMessage"] == ""
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_missing_metadata_key(self, handler: BaseSQLHandler) -> None:
@@ -221,20 +188,16 @@ async def test_missing_metadata_key(self, handler: BaseSQLHandler) -> None:
         # Test data - empty DataFrame
         test_data = pd.DataFrame({"TABLE_CATALOG": [], "TABLE_SCHEMA": []})

-        # Mock the SQLQueryInput.get_daft_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)

-            payload = {}  # Missing metadata key
-            result = await handler.check_schemas_and_databases(payload)
+        payload = {}  # Missing metadata key
+        result = await handler.check_schemas_and_databases(payload)

-            assert result["success"] is True  # Should default to empty filter
-            assert result["successMessage"] == "Schemas and Databases check successful"
-            assert result["failureMessage"] == ""
-            mock_get_dataframe.assert_called_once()
+        assert result["success"] is True  # Should default to empty filter
+        assert result["successMessage"] == "Schemas and Databases check successful"
+        assert result["failureMessage"] == ""
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_include_filter_string_and_dict_formats(
@@ -246,39 +209,29 @@ async def test_include_filter_string_and_dict_formats(
             {"TABLE_CATALOG": ["db1", "db1"], "TABLE_SCHEMA": ["schema1", "schema2"]}
         )

-        # Mock the SQLQueryInput.get_dataframe to return our test data
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = test_data
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=test_data)

-            # Test case 1: include-filter as JSON string
-            payload_string = {
-                "metadata": {"include-filter": '{"^db1$": ["^schema1$"]}'}
-            }
-            result_string = await handler.check_schemas_and_databases(payload_string)
-
-            assert result_string["success"] is True
-            assert (
-                result_string["successMessage"]
-                == "Schemas and Databases check successful"
-            )
-            assert result_string["failureMessage"] == ""
-
-            # Test case 2: include-filter as dict (already parsed)
-            payload_dict = {"metadata": {"include-filter": {"^db1$": ["^schema1$"]}}}
-            result_dict = await handler.check_schemas_and_databases(payload_dict)
-
-            assert result_dict["success"] is True
-            assert (
-                result_dict["successMessage"]
-                == "Schemas and Databases check successful"
-            )
-            assert result_dict["failureMessage"] == ""
-
-            # Both cases should produce the same result
-            assert result_string == result_dict
-
-            # Verify that get_dataframe was called twice (once for each test case)
-            assert mock_get_dataframe.call_count == 2
+        # Test case 1: include-filter as JSON string
+        payload_string = {"metadata": {"include-filter": '{"^db1$": ["^schema1$"]}'}}
+        result_string = await handler.check_schemas_and_databases(payload_string)
+
+        assert result_string["success"] is True
+        assert (
+            result_string["successMessage"] == "Schemas and Databases check successful"
+        )
+        assert result_string["failureMessage"] == ""
+
+        # Test case 2: include-filter as dict (already parsed)
+        payload_dict = {"metadata": {"include-filter": {"^db1$": ["^schema1$"]}}}
+        result_dict = await handler.check_schemas_and_databases(payload_dict)
+
+        assert result_dict["success"] is True
+        assert result_dict["successMessage"] == "Schemas and Databases check successful"
+        assert result_dict["failureMessage"] == ""
+
+        # Both cases should produce the same result
+        assert result_string == result_dict
+
+        # Verify that get_results was called twice (once for each test case)
+        assert handler.sql_client.get_results.call_count == 2
diff --git a/tests/unit/handlers/sql/test_preflight_check.py b/tests/unit/handlers/sql/test_preflight_check.py
index 4f3ef0274..44755f6e9 100644
--- a/tests/unit/handlers/sql/test_preflight_check.py
+++ b/tests/unit/handlers/sql/test_preflight_check.py
@@ -242,14 +242,14 @@ async def test_check_client_version_sql_query(
     # Set up SQL query for version
     handler.get_client_version_sql = "SELECT version();"

-    # Mock SQLQueryInput.get_dataframe to return a DataFrame with version
+    # Mock SQL client's run_query to return a DataFrame with version
     mock_df = Mock()
     mock_df.to_dict.return_value = {
         "records": [{"version": "PostgreSQL 15.4 on x86_64-pc-linux-gnu"}]
     }

     with patch(
-        "application_sdk.inputs.sql_query.SQLQueryInput", new_callable=AsyncMock
+        "application_sdk.clients.sql.BaseSQLClient.run_query", new_callable=AsyncMock
     ) as mock_sql_input:
         # Configure the mock to return our mock dataframe
         mock_instance = mock_sql_input.return_value
diff --git a/tests/unit/handlers/sql/test_prepare_metadata.py b/tests/unit/handlers/sql/test_prepare_metadata.py
index a2db5c240..a6f9b9c25 100644
--- a/tests/unit/handlers/sql/test_prepare_metadata.py
+++ b/tests/unit/handlers/sql/test_prepare_metadata.py
@@ -1,4 +1,4 @@
-from unittest.mock import AsyncMock, Mock, patch
+from unittest.mock import AsyncMock, Mock

 import pandas as pd
 import pytest
@@ -37,18 +37,16 @@ async def test_successful_metadata_preparation(
             }
         )

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            result = await handler.prepare_metadata()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)

-            assert len(result) == 3
-            assert result[0] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
-            assert result[1] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema2"}
-            assert result[2] == {"TABLE_CATALOG": "db2", "TABLE_SCHEMA": "schema1"}
-            mock_get_dataframe.assert_called_once()
+        result = await handler.prepare_metadata()
+
+        assert len(result) == 3
+        assert result[0] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
+        assert result[1] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema2"}
+        assert result[2] == {"TABLE_CATALOG": "db2", "TABLE_SCHEMA": "schema1"}
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_empty_dataframe(self, handler: BaseSQLHandler) -> None:
@@ -60,16 +58,14 @@ async def test_empty_dataframe(self, handler: BaseSQLHandler) -> None:
             }
         )

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            result = await handler.prepare_metadata()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)
+
+        result = await handler.prepare_metadata()

-            assert len(result) == 0
-            assert isinstance(result, list)
-            mock_get_dataframe.assert_called_once()
+        assert len(result) == 0
+        assert isinstance(result, list)
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_custom_alias_keys(self, handler: BaseSQLHandler) -> None:
@@ -79,16 +75,14 @@ async def test_custom_alias_keys(self, handler: BaseSQLHandler) -> None:

         df = pd.DataFrame({"DB_NAME": ["db1"], "SCHEMA_NAME": ["schema1"]})

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            result = await handler.prepare_metadata()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)

-            assert len(result) == 1
-            assert result[0] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
-            mock_get_dataframe.assert_called_once()
+        result = await handler.prepare_metadata()
+
+        assert len(result) == 1
+        assert result[0] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_custom_result_keys(self, handler: BaseSQLHandler) -> None:
@@ -98,16 +92,14 @@ async def test_custom_result_keys(self, handler: BaseSQLHandler) -> None:

         df = pd.DataFrame({"TABLE_CATALOG": ["db1"], "TABLE_SCHEMA": ["schema1"]})

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            result = await handler.prepare_metadata()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)
+
+        result = await handler.prepare_metadata()

-            assert len(result) == 1
-            assert result[0] == {"DATABASE": "db1", "SCHEMA": "schema1"}
-            mock_get_dataframe.assert_called_once()
+        assert len(result) == 1
+        assert result[0] == {"DATABASE": "db1", "SCHEMA": "schema1"}
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_missing_columns(self, handler: BaseSQLHandler) -> None:
@@ -118,15 +110,13 @@ async def test_missing_columns(self, handler: BaseSQLHandler) -> None:
             }
         )

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            with pytest.raises(KeyError) as exc_info:
-                await handler.prepare_metadata()
-            assert "TABLE_SCHEMA" in str(exc_info.value)
-            mock_get_dataframe.assert_called_once()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)
+
+        with pytest.raises(KeyError) as exc_info:
+            await handler.prepare_metadata()
+        assert "TABLE_SCHEMA" in str(exc_info.value)
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_null_values(self, handler: BaseSQLHandler) -> None:
@@ -138,18 +128,16 @@ async def test_null_values(self, handler: BaseSQLHandler) -> None:
             }
         )

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            result = await handler.prepare_metadata()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)
+
+        result = await handler.prepare_metadata()

-            assert len(result) == 3
-            assert result[0] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
-            assert result[1] == {"TABLE_CATALOG": None, "TABLE_SCHEMA": "schema2"}
-            assert result[2] == {"TABLE_CATALOG": "db2", "TABLE_SCHEMA": None}
-            mock_get_dataframe.assert_called_once()
+        assert len(result) == 3
+        assert result[0] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
+        assert result[1] == {"TABLE_CATALOG": None, "TABLE_SCHEMA": "schema2"}
+        assert result[2] == {"TABLE_CATALOG": "db2", "TABLE_SCHEMA": None}
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_special_characters(self, handler: BaseSQLHandler) -> None:
@@ -161,18 +149,16 @@ async def test_special_characters(self, handler: BaseSQLHandler) -> None:
             }
         )

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            result = await handler.prepare_metadata()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)
+
+        result = await handler.prepare_metadata()

-            assert len(result) == 3
-            assert result[0] == {"TABLE_CATALOG": "db-1", "TABLE_SCHEMA": "schema-1"}
-            assert result[1] == {"TABLE_CATALOG": "db.2", "TABLE_SCHEMA": "schema.2"}
-            assert result[2] == {"TABLE_CATALOG": "db@3", "TABLE_SCHEMA": "schema@3"}
-            mock_get_dataframe.assert_called_once()
+        assert len(result) == 3
+        assert result[0] == {"TABLE_CATALOG": "db-1", "TABLE_SCHEMA": "schema-1"}
+        assert result[1] == {"TABLE_CATALOG": "db.2", "TABLE_SCHEMA": "schema.2"}
+        assert result[2] == {"TABLE_CATALOG": "db@3", "TABLE_SCHEMA": "schema@3"}
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_duplicate_entries(self, handler: BaseSQLHandler) -> None:
@@ -184,33 +170,29 @@ async def test_duplicate_entries(self, handler: BaseSQLHandler) -> None:
             }
         )

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            result = await handler.prepare_metadata()
-
-            assert (
-                len(result) == 3
-            )  # Should preserve duplicates as they might be meaningful
-            assert all(
-                entry == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
-                for entry in result
-            )
-            mock_get_dataframe.assert_called_once()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)
+
+        result = await handler.prepare_metadata()
+
+        assert (
+            len(result) == 3
+        )  # Should preserve duplicates as they might be meaningful
+        assert all(
+            entry == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
+            for entry in result
+        )
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_invalid_dataframe(self, handler: BaseSQLHandler) -> None:
         """Test metadata preparation with invalid DataFrame input"""
-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = None
-            with pytest.raises(Exception):
-                await handler.prepare_metadata()
-            mock_get_dataframe.assert_called_once()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=None)
+
+        with pytest.raises(Exception):
+            await handler.prepare_metadata()
+        handler.sql_client.get_results.assert_called_once()

     @pytest.mark.asyncio
     async def test_extra_columns(self, handler: BaseSQLHandler) -> None:
@@ -223,14 +205,12 @@ async def test_extra_columns(self, handler: BaseSQLHandler) -> None:
             }
         )

-        with patch(
-            "application_sdk.inputs.sql_query.SQLQueryInput.get_dataframe",
-            new_callable=AsyncMock,
-        ) as mock_get_dataframe:
-            mock_get_dataframe.return_value = df
-            result = await handler.prepare_metadata()
-
-            assert len(result) == 1
-            assert result[0] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
-            assert "EXTRA_COLUMN" not in result[0]
-            mock_get_dataframe.assert_called_once()
+        # Mock the sql_client.get_results method directly
+        handler.sql_client.get_results = AsyncMock(return_value=df)
+
+        result = await handler.prepare_metadata()
+
+        assert len(result) == 1
+        assert result[0] == {"TABLE_CATALOG": "db1", "TABLE_SCHEMA": "schema1"}
+        assert "EXTRA_COLUMN" not in result[0]
+        handler.sql_client.get_results.assert_called_once()
diff --git a/tests/unit/inputs/test_json.py b/tests/unit/io/readers/test_json_reader.py
similarity index 58%
rename from tests/unit/inputs/test_json.py
rename to tests/unit/io/readers/test_json_reader.py
index e00a5a302..c050b29ea 100644
--- a/tests/unit/inputs/test_json.py
+++ b/tests/unit/io/readers/test_json_reader.py
@@ -6,7 +6,9 @@
 import pytest
 from hypothesis import HealthCheck, given, settings

-from application_sdk.inputs.json import JsonInput
+from application_sdk.common.types import DataframeType
+from application_sdk.io._utils import download_files
+from application_sdk.io.json import JsonFileReader
 from application_sdk.test_utils.hypothesis.strategies.inputs.json_input import (
     json_input_config_strategy,
 )
@@ -20,7 +22,7 @@

 @given(config=json_input_config_strategy)
 def test_init(config: Dict[str, Any]) -> None:
-    json_input = JsonInput(
+    json_input = JsonFileReader(
         path=config["path"],
         file_names=config["file_names"],
     )
@@ -30,9 +32,9 @@ def test_init(config: Dict[str, Any]) -> None:


 def test_init_single_file_with_file_names_raises_error() -> None:
-    """Test that JsonInput raises ValueError when single file path is combined with file_names."""
+    """Test that JsonFileReader raises ValueError when single file path is combined with file_names."""
     with pytest.raises(ValueError, match="Cannot specify both a single file path"):
-        JsonInput(path="/data/test.json", file_names=["other.json"])
+        JsonFileReader(path="/data/test.json", file_names=["other.json"])


 @pytest.mark.asyncio
@@ -46,9 +48,9 @@ async def test_not_download_file_that_exists() -> None:
     ), patch(
         "application_sdk.services.objectstore.ObjectStore.download_file"
     ) as mock_download:
-        json_input = JsonInput(path=path)  # No file_names
+        json_input = JsonFileReader(path=path)  # No file_names

-        result = await json_input.download_files()
+        result = await download_files(json_input.path, ".json", json_input.file_names)
         mock_download.assert_not_called()
         assert result == [path]  # Should return the local file

@@ -61,29 +63,48 @@ async def test_download_file_invoked_for_missing_files() -> None:

     def mock_isfile(path):
         # Return False for initial local check, True for downloaded files
-        if path in ["./local/tmp/local/a.json", "./local/tmp/local/b.json"]:
+        # Normalize paths for cross-platform comparison
+        expected_paths = [
+            os.path.join("./local/tmp/local", "a.json"),
+            os.path.join("./local/tmp/local", "b.json"),
+        ]
+        if path in expected_paths:
             return True
         return False

     with patch("os.path.isfile", side_effect=mock_isfile), patch(
         "os.path.isdir", return_value=True
     ), patch("glob.glob", side_effect=[[]]), patch(  # Only for initial local check
-        "application_sdk.inputs.get_object_store_prefix",
+        "application_sdk.activities.common.utils.get_object_store_prefix",
         side_effect=lambda p: p.lstrip("/").replace("\\", "/"),
     ), patch(
         "application_sdk.services.objectstore.ObjectStore.download_file"
     ) as mock_download:
-        json_input = JsonInput(path=path, file_names=file_names)
+        json_input = JsonFileReader(
+            path=path, file_names=file_names, dataframe_type=DataframeType.daft
+        )

-        result = await json_input.download_files()
+        result = await download_files(json_input.path, ".json", json_input.file_names)

         # Each file should be attempted to be downloaded - using correct signature (with destination)
+        # Normalize paths for cross-platform compatibility
         expected_calls = [
-            call(source="local/a.json", destination="./local/tmp/local/a.json"),
-            call(source="local/b.json", destination="./local/tmp/local/b.json"),
+            call(
+                source=os.path.join("local", "a.json"),
+                destination=os.path.join("./local/tmp/local", "a.json"),
+            ),
+            call(
+                source=os.path.join("local", "b.json"),
+                destination=os.path.join("./local/tmp/local", "b.json"),
+            ),
         ]
         mock_download.assert_has_calls(expected_calls, any_order=True)
-        assert result == ["./local/tmp/local/a.json", "./local/tmp/local/b.json"]
+        # Normalize result paths for comparison
+        expected_result = [
+            os.path.join("./local/tmp/local", "a.json"),
+            os.path.join("./local/tmp/local", "b.json"),
+        ]
+        assert result == expected_result


 @pytest.mark.asyncio
@@ -97,9 +118,11 @@ async def test_download_file_not_invoked_when_file_present() -> None:
     ), patch("glob.glob", return_value=["/local/exists.json"]), patch(
         "application_sdk.services.objectstore.ObjectStore.download_file"
     ) as mock_download:
-        json_input = JsonInput(path=path, file_names=file_names)
+        json_input = JsonFileReader(
+            path=path, file_names=file_names, dataframe_type=DataframeType.daft
+        )

-        result = await json_input.download_files()
+        result = await download_files(json_input.path, ".json", json_input.file_names)

         mock_download.assert_not_called()
         assert result == ["/local/exists.json"]
@@ -117,16 +140,18 @@ async def test_download_file_error_propagation() -> None:
     with patch("os.path.isfile", return_value=False), patch(
         "os.path.isdir", return_value=True
     ), patch("glob.glob", return_value=[]), patch(
-        "application_sdk.inputs.get_object_store_prefix",
+        "application_sdk.activities.common.utils.get_object_store_prefix",
         side_effect=lambda p: p.lstrip("/").replace("\\", "/"),
     ), patch(
         "application_sdk.services.objectstore.ObjectStore.download_file",
         side_effect=Exception("Download failed"),
     ):
-        json_input = JsonInput(path=path, file_names=file_names)
+        json_input = JsonFileReader(
+            path=path, file_names=file_names, dataframe_type=DataframeType.daft
+        )

         with pytest.raises(SDKIOError, match="ATLAN-IO-503-00"):
-            await json_input.download_files()
+            await download_files(json_input.path, ".json", json_input.file_names)


 # ---------------------------------------------------------------------------
@@ -161,8 +186,8 @@ def concat(objs, ignore_index=None):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_batched_dataframe_with_mocked_pandas(monkeypatch) -> None:
-    """Verify that get_batched_dataframe streams chunks and respects chunk_size."""
+async def test_read_batches_with_mocked_pandas(monkeypatch) -> None:
+    """Verify that read_batches streams chunks and respects chunk_size."""

     file_names = ["abc.json"]
     path = "/data"
@@ -170,23 +195,23 @@ async def test_get_batched_dataframe_with_mocked_pandas(monkeypatch) -> None:
     expected_chunksize = 5
     call_log = _install_dummy_pandas(monkeypatch)

-    async def dummy_download(self):  # noqa: D401, ANN001
-        return (
-            [os.path.join(self.path, fn) for fn in self.file_names]
-            if hasattr(self, "file_names") and self.file_names
-            else []
-        )
-
-    # Mock the base Input class method since JsonInput calls super().download_files()
-    from application_sdk.inputs import Input
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [os.path.join(path, fn) for fn in file_names] if file_names else []

-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since JsonFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.json.download_files", dummy_download, raising=False
+    )

-    json_input = JsonInput(
-        path=path, file_names=file_names, chunk_size=expected_chunksize
+    json_input = JsonFileReader(
+        path=path,
+        file_names=file_names,
+        chunk_size=expected_chunksize,
+        dataframe_type=DataframeType.pandas,
     )

-    chunks = [chunk async for chunk in json_input.get_batched_dataframe()]
+    batches = json_input.read_batches()
+    chunks = [chunk async for chunk in batches]

     # Two chunks per file as defined in dummy pandas implementation
     assert chunks == ["chunk1-abc.json", "chunk2-abc.json"]
@@ -202,22 +227,25 @@ async def dummy_download(self):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_batched_dataframe_empty_file_list(monkeypatch) -> None:
+async def test_read_batches_empty_file_list(monkeypatch) -> None:
     """An empty file list should result in no yielded batches."""

     call_log = _install_dummy_pandas(monkeypatch)

-    async def dummy_download(self):  # noqa: D401, ANN001
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
         return []

-    # Mock the base Input class method since JsonInput calls super().download_files()
-    from application_sdk.inputs import Input
-
-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since JsonFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.json.download_files", dummy_download, raising=False
+    )

-    json_input = JsonInput(path="/data", file_names=[])
+    json_input = JsonFileReader(
+        path="/data", file_names=[], dataframe_type=DataframeType.pandas
+    )

-    batches = [chunk async for chunk in json_input.get_batched_dataframe()]
+    batches_result = json_input.read_batches()
+    batches = [chunk async for chunk in batches_result]

     assert batches == []
     # No pandas.read_json calls should have been made
@@ -248,32 +276,31 @@ def read_json(path, _chunk_size=None):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_daft_dataframe(monkeypatch) -> None:
-    """Verify that get_daft_dataframe merges path correctly and delegates to daft.read_json."""
+async def test_read(monkeypatch) -> None:
+    """Verify that read merges path correctly and delegates to daft.read_json."""

     call_log = _install_dummy_daft(monkeypatch)

-    async def dummy_download(self):  # noqa: D401, ANN001
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
         return (
-            [
-                os.path.join(self.path, fn).replace(os.path.sep, "/")
-                for fn in self.file_names
-            ]
-            if hasattr(self, "file_names") and self.file_names
+            [os.path.join(path, fn).replace(os.path.sep, "/") for fn in file_names]
+            if file_names
             else []
         )

-    # Mock the base Input class method since JsonInput calls super().download_files()
-    from application_sdk.inputs import Input
-
-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since JsonFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.json.download_files", dummy_download, raising=False
+    )

     path = "/tmp"
     file_names = ["dir/file1.json", "dir/file2.json"]

-    json_input = JsonInput(path=path, file_names=file_names)
+    json_input = JsonFileReader(
+        path=path, file_names=file_names, dataframe_type=DataframeType.daft
+    )

-    result = await json_input.get_daft_dataframe()
+    result = await json_input.read()

     expected_files = ["/tmp/dir/file1.json", "/tmp/dir/file2.json"]

@@ -282,22 +309,24 @@ async def dummy_download(self):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_daft_dataframe_no_files(monkeypatch) -> None:
-    """Calling get_daft_dataframe without files should return empty result."""
+async def test_read_no_files(monkeypatch) -> None:
+    """Calling read without files should return empty result."""

     call_log = _install_dummy_daft(monkeypatch)

-    async def dummy_download(self):  # noqa: D401, ANN001
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
         return []  # Return empty list when no files found

-    # Mock the base Input class method since JsonInput calls super().download_files()
-    from application_sdk.inputs import Input
-
-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since JsonFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.json.download_files", dummy_download, raising=False
+    )

-    json_input = JsonInput(path="/tmp", file_names=[])
+    json_input = JsonFileReader(
+        path="/tmp", file_names=[], dataframe_type=DataframeType.daft
+    )

-    result = await json_input.get_daft_dataframe()
+    result = await json_input.read()

     # Should return empty daft result
     assert result == "daft_df:[]"
@@ -305,29 +334,31 @@ async def dummy_download(self):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_batched_daft_dataframe(monkeypatch) -> None:
-    """Ensure get_batched_daft_dataframe yields a frame per file and passes chunk size."""
+async def test_read_batches(monkeypatch) -> None:
+    """Ensure read_batches yields a frame per file and passes chunk size."""

     call_log = _install_dummy_daft(monkeypatch)

-    async def dummy_download(self):  # noqa: D401, ANN001
-        return (
-            [os.path.join(self.path, fn) for fn in self.file_names]
-            if hasattr(self, "file_names") and self.file_names
-            else []
-        )
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [os.path.join(path, fn) for fn in file_names] if file_names else []

-    # Mock the base Input class method since JsonInput calls super().download_files()
-    from application_sdk.inputs import Input
-
-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since JsonFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.json.download_files", dummy_download, raising=False
+    )

     path = "/data"
     file_names = ["one.json", "two.json"]

-    json_input = JsonInput(path=path, file_names=file_names, chunk_size=123)
+    json_input = JsonFileReader(
+        path=path,
+        file_names=file_names,
+        chunk_size=123,
+        dataframe_type=DataframeType.daft,
+    )

-    frames = [frame async for frame in json_input.get_batched_daft_dataframe()]
+    batches = json_input.read_batches()
+    frames = [frame async for frame in batches]

     expected_frames = [f"daft_df:{os.path.join(path, fn)}" for fn in file_names]

diff --git a/tests/unit/inputs/test_parquet.py b/tests/unit/io/readers/test_parquet_reader.py
similarity index 55%
rename from tests/unit/inputs/test_parquet.py
rename to tests/unit/io/readers/test_parquet_reader.py
index b25482fff..af8ee987f 100644
--- a/tests/unit/inputs/test_parquet.py
+++ b/tests/unit/io/readers/test_parquet_reader.py
@@ -6,7 +6,9 @@
 import pytest
 from hypothesis import HealthCheck, given, settings

-from application_sdk.inputs.parquet import ParquetInput
+from application_sdk.common.types import DataframeType
+from application_sdk.io._utils import download_files
+from application_sdk.io.parquet import ParquetFileReader
 from application_sdk.test_utils.hypothesis.strategies.inputs.parquet_input import (
     parquet_input_config_strategy,
 )
@@ -20,10 +22,11 @@

 @given(config=parquet_input_config_strategy)
 def test_init(config: Dict[str, Any]) -> None:
-    parquet_input = ParquetInput(
+    parquet_input = ParquetFileReader(
         path=config["path"],
         chunk_size=config["chunk_size"],
         file_names=config["file_names"],
+        dataframe_type=DataframeType.pandas,
     )

     assert parquet_input.path == config["path"]
@@ -32,9 +35,13 @@ def test_init(config: Dict[str, Any]) -> None:


 def test_init_single_file_with_file_names_raises_error() -> None:
-    """Test that ParquetInput raises ValueError when single file path is combined with file_names."""
+    """Test that ParquetFileReader raises ValueError when single file path is combined with file_names."""
     with pytest.raises(ValueError, match="Cannot specify both a single file path"):
-        ParquetInput(path="/data/test.parquet", file_names=["other.parquet"])
+        ParquetFileReader(
+            path="/data/test.parquet",
+            file_names=["other.parquet"],
+            dataframe_type=DataframeType.pandas,
+        )


 @pytest.mark.asyncio
@@ -46,12 +53,15 @@ async def test_not_download_file_that_exists() -> None:
     with patch("os.path.isfile", return_value=True), patch(
         "application_sdk.services.objectstore.ObjectStore.download_file"
     ) as mock_download:
-        parquet_input = ParquetInput(
+        parquet_input = ParquetFileReader(
             path=path,
             chunk_size=100000,  # No file_names
+            dataframe_type=DataframeType.pandas,
         )

-        result = await parquet_input.download_files()
+        result = await download_files(
+            parquet_input.path, ".parquet", parquet_input.file_names
+        )
         mock_download.assert_not_called()
         assert result == [path]

@@ -66,12 +76,16 @@ async def test_download_file_invoked_for_missing_files() -> None:
     ), patch("glob.glob", return_value=[]), patch(
         "application_sdk.services.objectstore.ObjectStore.download_file"
     ) as mock_download, patch(
-        "application_sdk.inputs.get_object_store_prefix",
+        "application_sdk.activities.common.utils.get_object_store_prefix",
         return_value="local/test.parquet",
     ):
-        parquet_input = ParquetInput(path=path, chunk_size=100000)
+        parquet_input = ParquetFileReader(
+            path=path, chunk_size=100000, dataframe_type=DataframeType.pandas
+        )

-        result = await parquet_input.download_files()
+        result = await download_files(
+            parquet_input.path, ".parquet", parquet_input.file_names
+        )

         # Should attempt to download the file
         mock_download.assert_called_once_with(
@@ -89,12 +103,14 @@ async def test_download_file_invoked_for_missing_files() -> None:

 @pytest.mark.asyncio
 async def test_download_files_uses_base_class() -> None:
-    """Test that ParquetInput uses the base class download_files method."""
+    """Test that ParquetFileReader uses the base class download_files method."""
     path = "/data/test.parquet"
-    parquet_input = ParquetInput(path=path)
+    parquet_input = ParquetFileReader(path=path, dataframe_type=DataframeType.pandas)

     with patch("os.path.isfile", return_value=True):
-        result = await parquet_input.download_files()
+        result = await download_files(
+            parquet_input.path, ".parquet", parquet_input.file_names
+        )

         assert result == [path]

@@ -166,24 +182,24 @@ def iloc(self):


 @pytest.mark.asyncio
-async def test_get_dataframe_with_mocked_pandas(monkeypatch) -> None:
-    """Verify that get_dataframe calls pandas.read_parquet correctly."""
+async def test_read_with_mocked_pandas(monkeypatch) -> None:
+    """Verify that read calls pandas.read_parquet correctly."""

     path = "/data/test.parquet"
     call_log = _install_dummy_pandas(monkeypatch)

     # Mock download_files to return the path
-    async def dummy_download(self):  # noqa: D401, ANN001
-        return [self.path]  # Return the path as a list of files
-
-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [path]  # Return the path as a list of files

-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

-    parquet_input = ParquetInput(path=path, chunk_size=100000)
+    parquet_input = ParquetFileReader(path=path, chunk_size=100000)

-    result = await parquet_input.get_dataframe()
+    result = await parquet_input.read()

     # Should return the mock DataFrame
     assert hasattr(result, "data")
@@ -194,25 +210,28 @@ async def dummy_download(self):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_batched_dataframe_with_mocked_pandas(monkeypatch) -> None:
-    """Verify that get_batched_dataframe streams chunks and respects chunk_size."""
+async def test_read_batches_with_mocked_pandas(monkeypatch) -> None:
+    """Verify that read_batches streams chunks and respects chunk_size."""

     path = "/data/test.parquet"
     expected_chunksize = 30
     call_log = _install_dummy_pandas(monkeypatch)

     # Mock download_files to return the path
-    async def dummy_download(self):  # noqa: D401, ANN001
-        return [self.path]  # Return the path as a list of files
-
-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [path]  # Return the path as a list of files

-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

-    parquet_input = ParquetInput(path=path, chunk_size=expected_chunksize)
+    parquet_input = ParquetFileReader(
+        path=path, chunk_size=expected_chunksize, dataframe_type=DataframeType.pandas
+    )

-    chunks = [chunk async for chunk in parquet_input.get_batched_dataframe()]
+    batches = parquet_input.read_batches()
+    chunks = [chunk async for chunk in batches]

     # With 100 rows and chunk_size=30, we should get 4 chunks
     expected_chunks = [
@@ -228,24 +247,27 @@ async def dummy_download(self):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_batched_dataframe_with_chunk_size(monkeypatch) -> None:
-    """Verify that get_batched_dataframe chunks data properly with specified chunk_size."""
+async def test_read_batches_with_chunk_size(monkeypatch) -> None:
+    """Verify that read_batches chunks data properly with specified chunk_size."""

     path = "/data/test.parquet"
     call_log = _install_dummy_pandas(monkeypatch)

     # Mock download_files to return the path
-    async def dummy_download(self):  # noqa: D401, ANN001
-        return [self.path]  # Return the path as a list of files
-
-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [path]  # Return the path as a list of files

-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

-    parquet_input = ParquetInput(path=path, chunk_size=100)
+    parquet_input = ParquetFileReader(
+        path=path, chunk_size=100, dataframe_type=DataframeType.pandas
+    )

-    chunks = [chunk async for chunk in parquet_input.get_batched_dataframe()]
+    batches = parquet_input.read_batches()
+    chunks = [chunk async for chunk in batches]

     # With 100 rows and chunk_size=100, we should get 1 chunk
     assert len(chunks) == 1
@@ -304,10 +326,10 @@ def __str__(self):
     def read_parquet(path, _chunk_size=None):  # noqa: D401, ANN001
         call_log.append({"path": path})
         if isinstance(path, list) and len(path) > 1:
-            # For get_batched_daft_dataframe tests that need MockDaftDataFrame
+            # For read_batches tests that need MockDaftDataFrame
             return MockDaftDataFrame(path)
         elif isinstance(path, list):
-            # For get_daft_dataframe tests that expect simple string return
+            # For read tests that expect simple string return
             return f"daft_df:{path}"
         return MockDaftDataFrame(path)

@@ -319,62 +341,61 @@ def read_parquet(path, _chunk_size=None):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_daft_dataframe(monkeypatch) -> None:
-    """Verify that get_daft_dataframe delegates to daft.read_parquet correctly."""
+async def test_read(monkeypatch) -> None:
+    """Verify that read delegates to pandas.read_parquet correctly."""

-    call_log = _install_dummy_daft(monkeypatch)
+    call_log = _install_dummy_pandas(monkeypatch)

     # Mock download_files to return a list of files
-    async def dummy_download(self):  # noqa: D401, ANN001
-        return [f"{self.path}/file1.parquet", f"{self.path}/file2.parquet"]
-
-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [f"{path}/file1.parquet", f"{path}/file2.parquet"]

-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

     path = "/tmp/data"
-    parquet_input = ParquetInput(path=path)
+    parquet_input = ParquetFileReader(path=path, dataframe_type=DataframeType.pandas)

-    result = await parquet_input.get_daft_dataframe()
+    result = await parquet_input.read()

     expected_files = ["/tmp/data/file1.parquet", "/tmp/data/file2.parquet"]
-    # Since we have multiple files, the mock returns a MockDaftDataFrame object
+    # Since we have multiple files, pandas.concat returns a CombinedMockDataFrame object
     assert hasattr(
-        result, "path"
-    ), "Result should be a MockDaftDataFrame with path attribute"
-    assert result.path == expected_files
-    assert call_log == [{"path": expected_files}]
+        result, "data"
+    ), "Result should be a CombinedMockDataFrame with data attribute"
+    assert len(result.data) == 200  # 100 rows from each file
+    assert call_log == [{"path": expected_files[0]}, {"path": expected_files[1]}]


 @pytest.mark.asyncio
-async def test_get_daft_dataframe_with_file_names(monkeypatch) -> None:
-    """Verify that get_daft_dataframe works correctly with file_names parameter."""
+async def test_read_with_file_names(monkeypatch) -> None:
+    """Verify that read works correctly with file_names parameter."""

     call_log = _install_dummy_daft(monkeypatch)

     # Mock download_files to return the specific files
-    async def dummy_download(self):  # noqa: D401, ANN001
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
         return (
-            [
-                os.path.join(self.path, fn).replace(os.path.sep, "/")
-                for fn in self.file_names
-            ]
-            if hasattr(self, "file_names") and self.file_names
+            [os.path.join(path, fn).replace(os.path.sep, "/") for fn in file_names]
+            if file_names
             else []
         )

-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
-
-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

     path = "/tmp"
     file_names = ["dir/file1.parquet", "dir/file2.parquet"]

-    parquet_input = ParquetInput(path=path, file_names=file_names)
+    parquet_input = ParquetFileReader(
+        path=path, file_names=file_names, dataframe_type=DataframeType.daft
+    )

-    result = await parquet_input.get_daft_dataframe()
+    result = await parquet_input.read()

     expected_files = ["/tmp/dir/file1.parquet", "/tmp/dir/file2.parquet"]
     # Since we have multiple files, the mock returns a MockDaftDataFrame object
@@ -386,64 +407,67 @@ async def dummy_download(self):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_daft_dataframe_with_input_prefix(monkeypatch) -> None:
-    """Verify that get_daft_dataframe downloads files when input_prefix is provided."""
+async def test_read_with_input_prefix(monkeypatch) -> None:
+    """Verify that read downloads files when input_prefix is provided."""

-    call_log = _install_dummy_daft(monkeypatch)
+    call_log = _install_dummy_pandas(monkeypatch)

     # Mock download_files to return a list of files
-    async def dummy_download(self):  # noqa: D401, ANN001
-        return [f"{self.path}/file1.parquet", f"{self.path}/file2.parquet"]
-
-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [f"{path}/file1.parquet", f"{path}/file2.parquet"]

-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

     path = "/tmp/data"
-    parquet_input = ParquetInput(path=path)
+    parquet_input = ParquetFileReader(path=path, dataframe_type=DataframeType.pandas)

-    result = await parquet_input.get_daft_dataframe()
+    result = await parquet_input.read()

     expected_files = ["/tmp/data/file1.parquet", "/tmp/data/file2.parquet"]
-    # Since we have multiple files, the mock returns a MockDaftDataFrame object
+    # Since we have multiple files, pandas.concat returns a CombinedMockDataFrame object
     assert hasattr(
-        result, "path"
-    ), "Result should be a MockDaftDataFrame with path attribute"
-    assert result.path == expected_files
-    assert call_log == [{"path": expected_files}]
+        result, "data"
+    ), "Result should be a CombinedMockDataFrame with data attribute"
+    assert len(result.data) == 200  # 100 rows from each file
+    assert call_log == [{"path": expected_files[0]}, {"path": expected_files[1]}]


 @pytest.mark.asyncio
-async def test_get_batched_daft_dataframe_with_file_names(monkeypatch) -> None:
-    """Ensure get_batched_daft_dataframe yields chunks from combined files when file_names provided."""
+async def test_read_batches_with_file_names(monkeypatch) -> None:
+    """Ensure read_batches yields chunks from combined files when file_names provided."""

     call_log = _install_dummy_daft(monkeypatch)

     # Mock download_files to return the specific files
-    async def dummy_download(self):  # noqa: D401, ANN001
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
         return (
-            [
-                os.path.join(self.path, fn).replace(os.path.sep, "/")
-                for fn in self.file_names
-            ]
-            if hasattr(self, "file_names") and self.file_names
+            [os.path.join(path, fn).replace(os.path.sep, "/") for fn in file_names]
+            if file_names
             else []
         )

-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
-
-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

     path = "/data"
     file_names = [
         "one.parquet",
         "two.parquet",
     ]
-    parquet_input = ParquetInput(path=path, file_names=file_names, buffer_size=50)
+    parquet_input = ParquetFileReader(
+        path=path,
+        file_names=file_names,
+        buffer_size=50,
+        dataframe_type=DataframeType.daft,
+    )

-    frames = [frame async for frame in parquet_input.get_batched_daft_dataframe()]
+    batches = parquet_input.read_batches()
+    frames = [frame async for frame in batches]

     # With 100 total rows and buffer_size=50, expect 2 chunks
     assert len(frames) == 2
@@ -455,24 +479,27 @@ async def dummy_download(self):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_batched_daft_dataframe_without_file_names(monkeypatch) -> None:
-    """Ensure get_batched_daft_dataframe works with chunked processing when no file_names provided."""
+async def test_read_batches_without_file_names(monkeypatch) -> None:
+    """Ensure read_batches works with chunked processing when no file_names provided."""

     call_log = _install_dummy_daft(monkeypatch)

     # Mock download_files to return a list of files
-    async def dummy_download(self):  # noqa: D401, ANN001
-        return [f"{self.path}/file1.parquet", f"{self.path}/file2.parquet"]
-
-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [f"{path}/file1.parquet", f"{path}/file2.parquet"]

-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

     path = "/data"
-    parquet_input = ParquetInput(path=path, buffer_size=50)
+    parquet_input = ParquetFileReader(
+        path=path, buffer_size=50, dataframe_type=DataframeType.daft
+    )

-    frames = [frame async for frame in parquet_input.get_batched_daft_dataframe()]
+    batches = parquet_input.read_batches()
+    frames = [frame async for frame in batches]

     # With 100 total rows and buffer_size=50, expect 2 chunks
     assert len(frames) == 2
@@ -484,25 +511,28 @@ async def dummy_download(self):  # noqa: D401, ANN001


 @pytest.mark.asyncio
-async def test_get_batched_daft_dataframe_no_input_prefix(monkeypatch) -> None:
-    """Ensure get_batched_daft_dataframe works with chunked processing without input_prefix."""
+async def test_read_batches_no_input_prefix(monkeypatch) -> None:
+    """Ensure read_batches works with chunked processing without input_prefix."""

     call_log = _install_dummy_daft(monkeypatch)

     # Mock download_files to return a list of files
-    async def dummy_download(self):  # noqa: D401, ANN001
-        return [f"{self.path}/file1.parquet", f"{self.path}/file2.parquet"]
-
-    # Mock the base Input class method since ParquetInput calls super().download_files()
-    from application_sdk.inputs import Input
+    async def dummy_download(path, file_extension, file_names=None):  # noqa: D401, ANN001
+        return [f"{path}/file1.parquet", f"{path}/file2.parquet"]

-    monkeypatch.setattr(Input, "download_files", dummy_download, raising=False)
+    # Mock the base Input class method since ParquetFileReader calls super().download_files()
+    monkeypatch.setattr(
+        "application_sdk.io.parquet.download_files", dummy_download, raising=False
+    )

     path = "/data"

-    parquet_input = ParquetInput(path=path, buffer_size=50)
+    parquet_input = ParquetFileReader(
+        path=path, buffer_size=50, dataframe_type=DataframeType.daft
+    )

-    frames = [frame async for frame in parquet_input.get_batched_daft_dataframe()]
+    batches = parquet_input.read_batches()
+    frames = [frame async for frame in batches]

     # With 100 total rows and buffer_size=50, expect 2 chunks
     assert len(frames) == 2
diff --git a/tests/unit/inputs/test_base_input.py b/tests/unit/io/test_base_io.py
similarity index 70%
rename from tests/unit/inputs/test_base_input.py
rename to tests/unit/io/test_base_io.py
index 746ec2bdd..7b763b0f1 100644
--- a/tests/unit/inputs/test_base_input.py
+++ b/tests/unit/io/test_base_io.py
@@ -1,4 +1,4 @@
-"""Unit tests for the base Input class download_files method."""
+"""Unit tests for the base Reader and Writer classes."""

 import os
 from typing import List
@@ -7,74 +7,61 @@
 import pytest

 from application_sdk.common.error_codes import IOError as SDKIOError
-from application_sdk.inputs import Input
+from application_sdk.io import Reader
+from application_sdk.io._utils import download_files


-class MockInput(Input):
-    """Mock implementation of Input for testing."""
+class MockReader(Reader):
+    """Mock implementation of Reader for testing."""

     def __init__(self, path: str, file_names: List[str] = None):
         self.path = path
         self.file_names = file_names
         self._EXTENSION = ".parquet"  # Default extension for testing

-    async def get_batched_dataframe(self):
+    async def read(self):
         """Mock implementation."""
         pass

-    async def get_dataframe(self):
-        """Mock implementation."""
-        pass
-
-    async def get_batched_daft_dataframe(self):
-        """Mock implementation."""
-        pass
-
-    async def get_daft_dataframe(self):
+    async def read_batches(self):
         """Mock implementation."""
         pass


-class MockInputNoPath(Input):
+class MockReaderNoPath(Reader):
     """Mock implementation without path attribute for testing."""

     def __init__(self):
         pass

-    async def get_batched_dataframe(self):
+    async def read(self):
         """Mock implementation."""
         pass

-    async def get_dataframe(self):
+    async def read_batches(self):
         """Mock implementation."""
         pass

-    async def get_batched_daft_dataframe(self):
-        """Mock implementation."""
-        pass

-    async def get_daft_dataframe(self):
-        """Mock implementation."""
-        pass
-
-
-class TestInputDownloadFiles:
-    """Test cases for Input.download_files method."""
+class TestReaderDownloadFiles:
+    """Test cases for Reader.download_files method."""

     @pytest.mark.asyncio
     async def test_download_files_no_path_attribute(self):
         """Test that AttributeError is raised when input has no path attribute."""
-        input_instance = MockInputNoPath()
+        input_instance = MockReaderNoPath()

         with pytest.raises(
-            AttributeError, match="'MockInputNoPath' object has no attribute 'path'"
+            AttributeError, match="'MockReaderNoPath' object has no attribute 'path'"
         ):
-            await input_instance.download_files()
+            await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

     @pytest.mark.asyncio
     async def test_download_files_empty_path(self):
         """Test behavior when path is empty."""
-        input_instance = MockInput("")
+        input_instance = MockReader("")

         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=False
@@ -83,16 +70,20 @@ async def test_download_files_empty_path(self):
             side_effect=Exception("Object store download failed"),
         ):
             with pytest.raises(SDKIOError, match="ATLAN-IO-503-00"):
-                await input_instance.download_files()
+                await download_files(
+                    input_instance.path, ".parquet", input_instance.file_names
+                )

     @pytest.mark.asyncio
     async def test_download_files_local_single_file_exists(self):
         """Test successful local file discovery for single file."""
         path = "/data/test.parquet"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)

         with patch("os.path.isfile", return_value=True):
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

         assert result == [path]

@@ -100,13 +91,15 @@ async def test_download_files_local_single_file_exists(self):
     async def test_download_files_local_directory_exists(self):
         """Test successful local file discovery for directory."""
         path = "/data"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)
         expected_files = ["/data/file1.parquet", "/data/file2.parquet"]

         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=True
         ), patch("glob.glob", return_value=expected_files):
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

         assert result == expected_files

@@ -115,7 +108,7 @@ async def test_download_files_local_directory_with_file_names_filter(self):
         """Test local file discovery with file_names filtering."""
         path = "/data"
         file_names = ["file1.parquet", "file3.parquet"]
-        input_instance = MockInput(path, file_names)
+        input_instance = MockReader(path, file_names)
         all_files = [
             "/data/file1.parquet",
             "/data/file2.parquet",
@@ -126,7 +119,9 @@ async def test_download_files_local_directory_with_file_names_filter(self):
         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=True
         ), patch("glob.glob", return_value=all_files):
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

         assert set(result) == set(expected_files)

@@ -135,10 +130,12 @@ async def test_download_files_single_file_with_file_names_match(self):
         """Test single file with file_names filter that matches."""
         path = "/data/test.parquet"
         file_names = ["test.parquet"]
-        input_instance = MockInput(path, file_names)
+        input_instance = MockReader(path, file_names)

         with patch("os.path.isfile", return_value=True):
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

         assert result == [path]

@@ -153,19 +150,21 @@ async def test_download_files_single_file_with_file_names_no_filtering(self):
         file_names = [
             "other.parquet"
         ]  # This doesn't match the file, but won't be used for filtering
-        input_instance = MockInput(path, file_names)
+        input_instance = MockReader(path, file_names)

         # MockInput allows this configuration, and single file will be found locally
         with patch("os.path.isfile", return_value=True):
             # Local single file exists and will be returned (no filtering applied)
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )
             assert result == ["/data/test.parquet"]

     @pytest.mark.asyncio
     async def test_download_files_download_single_file_success(self):
         """Test successful download of single file from object store."""
         path = "/data/test.parquet"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)

         with patch("os.path.isfile", side_effect=[False, True]), patch(
             "os.path.isdir", return_value=False
@@ -173,10 +172,12 @@ async def test_download_files_download_single_file_success(self):
             "application_sdk.services.objectstore.ObjectStore.download_file",
             new_callable=AsyncMock,
         ) as mock_download, patch(
-            "application_sdk.inputs.get_object_store_prefix",
+            "application_sdk.activities.common.utils.get_object_store_prefix",
             return_value="data/test.parquet",
         ):
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

             mock_download.assert_called_once_with(
                 source="data/test.parquet", destination="./local/tmp/data/test.parquet"
@@ -189,7 +190,7 @@ async def test_download_files_download_single_file_success(self):
     async def test_download_files_download_directory_success(self):
         """Test successful download of directory from object store."""
         path = "/data"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)
         expected_files = ["/data/file1.parquet", "/data/file2.parquet"]

         with patch("os.path.isfile", return_value=False), patch(
@@ -198,12 +199,12 @@ async def test_download_files_download_directory_success(self):
             "application_sdk.services.objectstore.ObjectStore.download_prefix",
             new_callable=AsyncMock,
         ) as mock_download, patch(
-            "application_sdk.inputs.get_object_store_prefix",
+            "application_sdk.activities.common.utils.get_object_store_prefix",
             return_value="data",
         ):
             # Mock the file finding function to return empty for local check, then files after download
             with patch(
-                "application_sdk.inputs.find_local_files_by_extension"
+                "application_sdk.io._utils.find_local_files_by_extension"
             ) as mock_find_files:
                 # Use a function that returns different values based on the path
                 def mock_find_files_func(path, extension, file_names=None):
@@ -214,7 +215,9 @@ def mock_find_files_func(path, extension, file_names=None):

                 mock_find_files.side_effect = mock_find_files_func

-                result = await input_instance.download_files()
+                result = await download_files(
+                    input_instance.path, ".parquet", input_instance.file_names
+                )

                 mock_download.assert_called_once_with(
                     source="data", destination="./local/tmp/data"
@@ -226,19 +229,22 @@ async def test_download_files_download_specific_files_success(self):
         """Test successful download of specific files from object store."""
         path = "/data"
         file_names = ["file1.parquet", "file2.parquet"]
-        input_instance = MockInput(path, file_names)
+        input_instance = MockReader(path, file_names)
         # Expected files will be in temporary directory after download
+        # Normalize paths for cross-platform compatibility
         expected_files = [
-            "./local/tmp/data/file1.parquet",
-            "./local/tmp/data/file2.parquet",
+            os.path.join("./local/tmp/data", "file1.parquet"),
+            os.path.join("./local/tmp/data", "file2.parquet"),
         ]

         def mock_isfile(path):
             # Return False for initial local check, True for downloaded files
-            if path in [
-                "./local/tmp/data/file1.parquet",
-                "./local/tmp/data/file2.parquet",
-            ]:
+            # Normalize paths for cross-platform comparison
+            expected_paths = [
+                os.path.join("./local/tmp/data", "file1.parquet"),
+                os.path.join("./local/tmp/data", "file2.parquet"),
+            ]
+            if path in expected_paths:
                 return True
             return False

@@ -251,20 +257,23 @@ def mock_isfile(path):
             "application_sdk.services.objectstore.ObjectStore.download_file",
             new_callable=AsyncMock,
         ) as mock_download, patch(
-            "application_sdk.inputs.get_object_store_prefix",
+            "application_sdk.activities.common.utils.get_object_store_prefix",
             side_effect=lambda p: p.lstrip("/").replace("\\", "/"),
         ):
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

             # Should download each specific file
+            # Normalize paths for cross-platform compatibility
             assert mock_download.call_count == 2
             mock_download.assert_any_call(
-                source="data/file1.parquet",
-                destination="./local/tmp/data/file1.parquet",
+                source=os.path.join("data", "file1.parquet"),
+                destination=os.path.join("./local/tmp/data", "file1.parquet"),
             )
             mock_download.assert_any_call(
-                source="data/file2.parquet",
-                destination="./local/tmp/data/file2.parquet",
+                source=os.path.join("data", "file2.parquet"),
+                destination=os.path.join("./local/tmp/data", "file2.parquet"),
             )
             assert result == expected_files

@@ -272,7 +281,7 @@ def mock_isfile(path):
     async def test_download_files_download_failure(self):
         """Test download failure from object store."""
         path = "/data/test.parquet"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)

         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=False
@@ -281,17 +290,19 @@ async def test_download_files_download_failure(self):
             new_callable=AsyncMock,
             side_effect=Exception("Download failed"),
         ), patch(
-            "application_sdk.inputs.get_object_store_prefix",
+            "application_sdk.activities.common.utils.get_object_store_prefix",
             return_value="data/test.parquet",
         ):
             with pytest.raises(SDKIOError, match="ATLAN-IO-503-00"):
-                await input_instance.download_files()
+                await download_files(
+                    input_instance.path, ".parquet", input_instance.file_names
+                )

     @pytest.mark.asyncio
     async def test_download_files_download_success_but_no_files_found(self):
         """Test download succeeds but no files found after download."""
         path = "/data"  # Use directory path
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)

         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=True
@@ -299,10 +310,10 @@ async def test_download_files_download_success_but_no_files_found(self):
             "application_sdk.services.objectstore.ObjectStore.download_prefix",
             new_callable=AsyncMock,
         ), patch(
-            "application_sdk.inputs.get_object_store_prefix",
+            "application_sdk.activities.common.utils.get_object_store_prefix",
             return_value="data",
         ), patch(
-            "application_sdk.inputs.find_local_files_by_extension",
+            "application_sdk.io._utils.find_local_files_by_extension",
             side_effect=[
                 [],
                 [],
@@ -310,19 +321,23 @@ async def test_download_files_download_success_but_no_files_found(self):
         ):
             # Should raise error when no files found after download
             with pytest.raises(SDKIOError, match="ATLAN-IO-503-00"):
-                await input_instance.download_files()
+                await download_files(
+                    input_instance.path, ".parquet", input_instance.file_names
+                )

     @pytest.mark.asyncio
     async def test_download_files_recursive_glob_pattern(self):
         """Test that recursive glob pattern is used for directory search."""
         path = "/data"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)
         expected_files = ["/data/subdir/file1.parquet", "/data/file2.parquet"]

         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=True
         ), patch("glob.glob", return_value=expected_files) as mock_glob:
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

             # Should use recursive glob pattern (OS-specific path separators)
             expected_pattern = os.path.join("/data", "**", "*.parquet")
@@ -333,13 +348,15 @@ async def test_download_files_recursive_glob_pattern(self):
     async def test_download_files_file_extension_filtering(self):
         """Test that only files with correct extension are returned."""
         path = "/data"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)
         expected_files = ["/data/file1.parquet", "/data/file3.parquet"]

         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=True
         ), patch("glob.glob", return_value=expected_files):
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

             assert result == expected_files

@@ -348,14 +365,16 @@ async def test_download_files_file_names_basename_matching(self):
         """Test file_names matching works with both full path and basename."""
         path = "/data"
         file_names = ["file1.parquet"]  # Just basename
-        input_instance = MockInput(path, file_names)
+        input_instance = MockReader(path, file_names)
         all_files = ["/data/subdir/file1.parquet", "/data/file2.parquet"]
         expected_files = ["/data/subdir/file1.parquet"]

         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=True
         ), patch("glob.glob", return_value=all_files):
-            result = await input_instance.download_files()
+            result = await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

             assert result == expected_files

@@ -363,12 +382,14 @@ async def test_download_files_file_names_basename_matching(self):
     async def test_download_files_logging_messages(self):
         """Test that appropriate logging messages are generated."""
         path = "/data/test.parquet"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)

         with patch("os.path.isfile", return_value=True), patch(
-            "application_sdk.inputs.logger"
+            "application_sdk.io._utils.logger"
         ) as mock_logger:
-            await input_instance.download_files()
+            await download_files(
+                input_instance.path, ".parquet", input_instance.file_names
+            )

             mock_logger.info.assert_called_with(
                 "Found 1 .parquet files locally at: /data/test.parquet"
@@ -378,7 +399,7 @@ async def test_download_files_logging_messages(self):
     async def test_download_files_logging_download_attempt(self):
         """Test logging when attempting download from object store."""
         path = "/data/test.parquet"
-        input_instance = MockInput(path)
+        input_instance = MockReader(path)

         with patch("os.path.isfile", return_value=False), patch(
             "os.path.isdir", return_value=False
@@ -387,11 +408,13 @@ async def test_download_files_logging_download_attempt(self):
             new_callable=AsyncMock,
             side_effect=Exception("Download failed"),
         ), patch(
-            "application_sdk.inputs.get_object_store_prefix",
+            "application_sdk.activities.common.utils.get_object_store_prefix",
             return_value="data/test.parquet",
-        ), patch("application_sdk.inputs.logger") as mock_logger:
+        ), patch("application_sdk.io._utils.logger") as mock_logger:
             with pytest.raises(SDKIOError):
-                await input_instance.download_files()
+                await download_files(
+                    input_instance.path, ".parquet", input_instance.file_names
+                )

             mock_logger.info.assert_any_call(
                 "No local .parquet files found at /data/test.parquet, checking object store..."
diff --git a/tests/unit/outputs/test_iceberg.py b/tests/unit/io/writers/test_iceberg_writer.py
similarity index 51%
rename from tests/unit/outputs/test_iceberg.py
rename to tests/unit/io/writers/test_iceberg_writer.py
index 9c8c68721..d7235465f 100644
--- a/tests/unit/outputs/test_iceberg.py
+++ b/tests/unit/io/writers/test_iceberg_writer.py
@@ -5,7 +5,8 @@
 from pyiceberg.catalog import Catalog
 from pyiceberg.table import Table

-from application_sdk.outputs.iceberg import IcebergOutput
+from application_sdk.common.types import DataframeType
+from application_sdk.io.iceberg import IcebergTableWriter


 @pytest.fixture
@@ -19,18 +20,19 @@ def mock_table() -> Table:


 @pytest.fixture
-def iceberg_output(mock_catalog: Catalog) -> IcebergOutput:
-    return IcebergOutput(
+def iceberg_output(mock_catalog: Catalog) -> IcebergTableWriter:
+    return IcebergTableWriter(
         iceberg_catalog=mock_catalog,
         iceberg_namespace="test_namespace",
         iceberg_table="test_table",
         mode="append",
+        dataframe_type=DataframeType.pandas,
     )


 def test_iceberg_output_initialization(mock_catalog: Catalog) -> None:
-    """Test IcebergOutput initialization with different parameters"""
-    output = IcebergOutput(
+    """Test IcebergTableWriter initialization with different parameters"""
+    output = IcebergTableWriter(
         iceberg_catalog=mock_catalog,
         iceberg_namespace="test_namespace",
         iceberg_table="test_table",
@@ -46,17 +48,17 @@ def test_iceberg_output_initialization(mock_catalog: Catalog) -> None:


 @pytest.mark.asyncio
-async def test_write_dataframe_empty(iceberg_output: IcebergOutput) -> None:
+async def test_write_empty(iceberg_output: IcebergTableWriter) -> None:
     """Test writing empty dataframe"""
     df = pd.DataFrame()
-    await iceberg_output.write_dataframe(df)
+    await iceberg_output.write(df)
     # Should return without doing anything for empty dataframe
     assert iceberg_output.total_record_count == 0
     assert iceberg_output.chunk_count == 0


 @pytest.mark.asyncio
-async def test_write_dataframe_with_data(iceberg_output: IcebergOutput) -> None:
+async def test_write_with_data(iceberg_output: IcebergTableWriter) -> None:
     """Test writing dataframe with data"""
     test_data = pd.DataFrame({"col1": [1, 2, 3], "col2": ["a", "b", "c"]})

@@ -66,73 +68,67 @@ async def test_write_dataframe_with_data(iceberg_output: IcebergOutput) -> None:
     with patch("daft.from_pandas") as mock_from_pandas:
         mock_from_pandas.return_value = mock_daft_df

-        await iceberg_output.write_dataframe(test_data)
+        await iceberg_output.write(test_data)
         mock_from_pandas.assert_called_once_with(test_data)


 @pytest.mark.asyncio
-async def test_write_daft_dataframe_existing_table(
-    iceberg_output: IcebergOutput, mock_table: Table
+async def test_write_existing_table(
+    iceberg_output: IcebergTableWriter, mock_table: Table
 ) -> None:
     """Test writing daft dataframe to existing table"""
-    mock_df = Mock()
-    mock_df.count_rows.return_value = 5
+    df = pd.DataFrame({"col1": [1, 2, 3, 4, 5]})
     iceberg_output.iceberg_table = mock_table

-    await iceberg_output.write_daft_dataframe(mock_df)
+    with patch("daft.from_pandas") as mock_from_pandas:
+        mock_daft_df = Mock()
+        mock_daft_df.count_rows.return_value = 5
+        mock_daft_df.write_iceberg.return_value = None
+        mock_from_pandas.return_value = mock_daft_df

-    assert iceberg_output.total_record_count == 5
-    assert iceberg_output.chunk_count == 1
-    mock_df.write_iceberg.assert_called_once_with(mock_table, mode="append")
+        await iceberg_output.write(df)
+
+        assert iceberg_output.total_record_count == 5
+        assert iceberg_output.chunk_count == 1
+        mock_daft_df.write_iceberg.assert_called_once_with(mock_table, mode="append")


 @pytest.mark.asyncio
-async def test_write_daft_dataframe_new_table(
-    iceberg_output: IcebergOutput, mock_table: Table
+async def test_write_new_table(
+    iceberg_output: IcebergTableWriter, mock_table: Table
 ) -> None:
     """Test writing daft dataframe creating new table"""
-    mock_df = Mock()
-    mock_df.count_rows.return_value = 3
+    df = pd.DataFrame({"col1": [1, 2, 3]})
     mock_arrow_schema = Mock()
-    mock_df.to_arrow.return_value.schema = mock_arrow_schema

     iceberg_output.iceberg_catalog.create_table_if_not_exists.return_value = mock_table

-    await iceberg_output.write_daft_dataframe(mock_df)
+    with patch("daft.from_pandas") as mock_from_pandas:
+        mock_daft_df = Mock()
+        mock_daft_df.count_rows.return_value = 3
+        mock_daft_df.write_iceberg.return_value = None
+        mock_daft_df.to_arrow.return_value.schema = mock_arrow_schema
+        mock_from_pandas.return_value = mock_daft_df

-    iceberg_output.iceberg_catalog.create_table_if_not_exists.assert_called_once_with(
-        "test_namespace.test_table", schema=mock_arrow_schema
-    )
-    assert iceberg_output.total_record_count == 3
-    assert iceberg_output.chunk_count == 1
-    mock_df.write_iceberg.assert_called_once_with(mock_table, mode="append")
+        await iceberg_output.write(df)
+
+        iceberg_output.iceberg_catalog.create_table_if_not_exists.assert_called_once_with(
+            "test_namespace.test_table", schema=mock_arrow_schema
+        )
+        assert iceberg_output.total_record_count == 3
+        assert iceberg_output.chunk_count == 1


 @pytest.mark.asyncio
-async def test_write_dataframe_error_handling(iceberg_output: IcebergOutput) -> None:
-    """Test error handling in write_dataframe"""
+async def test_write_error_handling(iceberg_output: IcebergTableWriter) -> None:
+    """Test error handling in write"""
     df = pd.DataFrame({"col1": [1, 2, 3]})

     with patch("daft.from_pandas") as mock_from_pandas:
         mock_from_pandas.side_effect = Exception("Test error")

         with pytest.raises(Exception, match="Test error"):
-            await iceberg_output.write_dataframe(df)
+            await iceberg_output.write(df)
         # Verify counts remain unchanged
         assert iceberg_output.total_record_count == 0
         assert iceberg_output.chunk_count == 0
-
-
-@pytest.mark.asyncio
-async def test_write_daft_dataframe_error_handling(
-    iceberg_output: IcebergOutput,
-) -> None:
-    """Test error handling in write_daft_dataframe"""
-    mock_df = Mock()
-    mock_df.count_rows.side_effect = Exception("Test error")
-
-    with pytest.raises(Exception, match="Test error"):
-        await iceberg_output.write_daft_dataframe(mock_df)
-    # Verify counts remain unchanged
-    assert iceberg_output.total_record_count == 0
-    assert iceberg_output.chunk_count == 0
diff --git a/tests/unit/outputs/test_json_output.py b/tests/unit/io/writers/test_json_writer.py
similarity index 72%
rename from tests/unit/outputs/test_json_output.py
rename to tests/unit/io/writers/test_json_writer.py
index d6bacb4c2..cc34ecf4e 100644
--- a/tests/unit/outputs/test_json_output.py
+++ b/tests/unit/io/writers/test_json_writer.py
@@ -7,7 +7,7 @@
 import pytest
 from hypothesis import HealthCheck, given, settings

-from application_sdk.outputs.json import JsonOutput
+from application_sdk.io.json import JsonFileWriter
 from application_sdk.test_utils.hypothesis.strategies.outputs.json_output import (
     chunk_size_strategy,
     dataframe_strategy,
@@ -31,25 +31,20 @@ def base_output_path(tmp_path_factory: pytest.TempPathFactory) -> str:
 async def test_init(base_output_path: str, config: Dict[str, Any]) -> None:
     # Create a safe output path by joining base_output_path with config's output_path
     safe_path = str(Path(base_output_path) / config["output_path"])
-    json_output = JsonOutput(  # type: ignore
+    json_output = JsonFileWriter(  # type: ignore
         output_path=safe_path,
-        output_suffix=config["output_suffix"],
-        output_prefix=config["output_prefix"],
         chunk_size=config["chunk_size"],
     )

-    assert json_output.output_path.endswith(config["output_suffix"])
-    assert json_output.output_prefix == config["output_prefix"]
+    assert json_output.output_path == safe_path
     assert json_output.chunk_size == config["chunk_size"]
     assert os.path.exists(json_output.output_path)


 @pytest.mark.asyncio
-async def test_write_dataframe_empty(base_output_path: str) -> None:
-    json_output = JsonOutput(  # type: ignore
-        output_suffix="tests/raw",
-        output_path=base_output_path,
-        output_prefix="test_prefix",
+async def test_write_empty(base_output_path: str) -> None:
+    json_output = JsonFileWriter(  # type: ignore
+        output_path=os.path.join(base_output_path, "tests", "raw"),
         chunk_size=100000,
         typename=None,
         chunk_count=0,
@@ -57,7 +52,7 @@ async def test_write_dataframe_empty(base_output_path: str) -> None:
         chunk_start=None,
     )
     dataframe = pd.DataFrame()
-    await json_output.write_dataframe(dataframe)
+    await json_output.write(dataframe)
     assert json_output.chunk_count == 0
     assert json_output.total_record_count == 0

@@ -68,16 +63,12 @@ async def test_write_dataframe_empty(base_output_path: str) -> None:
 @settings(suppress_health_check=[HealthCheck.function_scoped_fixture])
 @given(df=dataframe_strategy())  # type: ignore
 @pytest.mark.asyncio
-async def test_write_dataframe_single_chunk(
-    base_output_path: str, df: pd.DataFrame
-) -> None:
+async def test_write_single_chunk(base_output_path: str, df: pd.DataFrame) -> None:
     with patch(
         "application_sdk.services.objectstore.ObjectStore.upload_file"
     ) as mock_push:
-        json_output = JsonOutput(  # type: ignore
-            output_suffix="tests/raw",
-            output_path=base_output_path,
-            output_prefix="test_prefix",
+        json_output = JsonFileWriter(  # type: ignore
+            output_path=os.path.join(base_output_path, "tests", "raw"),
             chunk_size=len(df)
             + 1,  # Ensure single chunk by making chunk size larger than df
             typename=None,
@@ -85,7 +76,7 @@ async def test_write_dataframe_single_chunk(
             total_record_count=0,
             chunk_start=None,
         )
-        await json_output.write_dataframe(df)
+        await json_output.write(df)

         assert json_output.chunk_count == (1 if not df.empty else 0)
         assert json_output.total_record_count == len(df)
@@ -102,23 +93,21 @@ async def test_write_dataframe_single_chunk(
 @settings(suppress_health_check=[HealthCheck.function_scoped_fixture])
 @given(df=dataframe_strategy(), chunk_size=chunk_size_strategy)  # type: ignore
 @pytest.mark.asyncio
-async def test_write_dataframe_multiple_chunks(
+async def test_write_multiple_chunks(
     base_output_path: str, df: pd.DataFrame, chunk_size: int
 ) -> None:
     with patch(
         "application_sdk.services.objectstore.ObjectStore.upload_file"
     ) as mock_push:
-        json_output = JsonOutput(  # type: ignore
-            output_suffix="tests/raw",
-            output_path=base_output_path,
-            output_prefix="test_prefix",
+        json_output = JsonFileWriter(  # type: ignore
+            output_path=os.path.join(base_output_path, "tests", "raw"),
             chunk_size=chunk_size,
             typename=None,
             chunk_count=0,
             total_record_count=0,
             chunk_start=None,
         )
-        await json_output.write_dataframe(df)
+        await json_output.write(df)

         expected_chunks = (
             (len(df) + chunk_size - 1) // chunk_size if not df.empty else 0
@@ -134,11 +123,9 @@ async def test_write_dataframe_multiple_chunks(


 @pytest.mark.asyncio
-async def test_write_dataframe_error(base_output_path: str) -> None:
-    json_output = JsonOutput(  # type: ignore
-        output_suffix="tests/raw",
-        output_path=base_output_path,
-        output_prefix="test_prefix",
+async def test_write_error(base_output_path: str) -> None:
+    json_output = JsonFileWriter(  # type: ignore
+        output_path=os.path.join(base_output_path, "tests", "raw"),
         chunk_size=100000,
         typename=None,
         chunk_count=0,
@@ -147,7 +134,7 @@ async def test_write_dataframe_error(base_output_path: str) -> None:
     )
     invalid_df = "not_a_dataframe"
     with pytest.raises(AttributeError, match="'str' object has no attribute"):
-        await json_output.write_dataframe(invalid_df)  # type: ignore
+        await json_output.write(invalid_df)  # type: ignore
     # Verify counts remain unchanged after error
     assert json_output.chunk_count == 0
     assert json_output.total_record_count == 0
diff --git a/tests/unit/outputs/test_parquet_output.py b/tests/unit/io/writers/test_parquet_writer.py
similarity index 84%
rename from tests/unit/outputs/test_parquet_output.py
rename to tests/unit/io/writers/test_parquet_writer.py
index cddba4735..188fe9ef4 100644
--- a/tests/unit/outputs/test_parquet_output.py
+++ b/tests/unit/io/writers/test_parquet_writer.py
@@ -6,7 +6,8 @@
 import pandas as pd
 import pytest

-from application_sdk.outputs.parquet import ParquetOutput
+from application_sdk.common.types import DataframeType
+from application_sdk.io.parquet import ParquetFileWriter


 @pytest.fixture
@@ -95,16 +96,15 @@ def create_mock_result(paths: List[str]):
     return _create_mock_files


-class TestParquetOutputInit:
-    """Test ParquetOutput initialization."""
+class TestParquetFileWriterInit:
+    """Test ParquetFileWriter initialization."""

     def test_init_default_values(self, base_output_path: str):
-        """Test ParquetOutput initialization with default values."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        """Test ParquetFileWriter initialization with default values."""
+        parquet_output = ParquetFileWriter(output_path=base_output_path)

         # The output path gets modified by adding suffix, so check it ends with the base path
         assert base_output_path in parquet_output.output_path
-        assert parquet_output.output_suffix == ""
         assert parquet_output.typename is None

         assert parquet_output.chunk_size == 100000
@@ -116,10 +116,9 @@ def test_init_default_values(self, base_output_path: str):
         # partition_cols was removed from the implementation

     def test_init_custom_values(self, base_output_path: str):
-        """Test ParquetOutput initialization with custom values."""
-        parquet_output = ParquetOutput(
-            output_path=base_output_path,
-            output_suffix="test_suffix",
+        """Test ParquetFileWriter initialization with custom values."""
+        parquet_output = ParquetFileWriter(
+            output_path=os.path.join(base_output_path, "test_suffix"),
             typename="test_table",
             chunk_size=50000,
             total_record_count=100,
@@ -129,7 +128,6 @@ def test_init_custom_values(self, base_output_path: str):
             end_marker="end",
         )

-        assert parquet_output.output_suffix == "test_suffix"
         assert parquet_output.typename == "test_table"

         assert parquet_output.chunk_size == 50000
@@ -144,9 +142,8 @@ def test_init_custom_values(self, base_output_path: str):

     def test_init_creates_output_directory(self, base_output_path: str):
         """Test that initialization creates the output directory."""
-        parquet_output = ParquetOutput(
-            output_path=base_output_path,
-            output_suffix="test_dir",
+        parquet_output = ParquetFileWriter(
+            output_path=os.path.join(base_output_path, "test_dir"),
             typename="test_table",
         )

@@ -155,50 +152,52 @@ def test_init_creates_output_directory(self, base_output_path: str):
         assert parquet_output.output_path == expected_path


-class TestParquetOutputPathGen:
-    """Test ParquetOutput path generation."""
+class TestParquetFileWriterPathGen:
+    """Test ParquetFileWriter path generation."""

     def test_path_gen_with_markers(self, base_output_path: str):
         """Test path generation with start and end markers."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        from application_sdk.io._utils import path_gen

-        path = parquet_output.path_gen(start_marker="start_123", end_marker="end_456")
+        path = path_gen(
+            start_marker="start_123", end_marker="end_456", extension=".parquet"
+        )

         assert path == "start_123_end_456.parquet"

     def test_path_gen_without_chunk_start(self, base_output_path: str):
         """Test path generation without chunk count."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        from application_sdk.io._utils import path_gen

-        path = parquet_output.path_gen(chunk_part=5)
+        path = path_gen(chunk_part=5, extension=".parquet")

         assert path == "5.parquet"

     def test_path_gen_with_chunk_count(self, base_output_path: str):
         """Test path generation with chunk count."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        from application_sdk.io._utils import path_gen

-        path = parquet_output.path_gen(chunk_count=10, chunk_part=3)
+        path = path_gen(chunk_count=10, chunk_part=3, extension=".parquet")

         assert path == "chunk-10-part3.parquet"


-class TestParquetOutputWriteDataframe:
-    """Test ParquetOutput pandas DataFrame writing."""
+class TestParquetFileWriterWriteDataframe:
+    """Test ParquetFileWriter pandas DataFrame writing."""

     @pytest.mark.asyncio
     async def test_write_empty_dataframe(self, base_output_path: str):
         """Test writing an empty DataFrame."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        parquet_output = ParquetFileWriter(output_path=base_output_path)
         empty_df = pd.DataFrame()

-        await parquet_output.write_dataframe(empty_df)
+        await parquet_output.write(empty_df)

         assert parquet_output.chunk_count == 0
         assert parquet_output.total_record_count == 0

     @pytest.mark.asyncio
-    async def test_write_dataframe_success(
+    async def test_write_success(
         self, base_output_path: str, sample_dataframe: pd.DataFrame
     ):
         """Test successful DataFrame writing."""
@@ -207,20 +206,19 @@ async def test_write_dataframe_success(
         ) as mock_upload, patch(
             "pandas.DataFrame.to_parquet"
         ) as mock_to_parquet, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             mock_upload.return_value = AsyncMock()
             mock_prefix.return_value = "test/output/path"

-            parquet_output = ParquetOutput(
-                output_path=base_output_path,
-                output_suffix="test",
+            parquet_output = ParquetFileWriter(
+                output_path=os.path.join(base_output_path, "test"),
                 use_consolidation=False,
             )

             # Mock os.path.exists after initialization to return True for upload check
             with patch("os.path.exists", return_value=True):
-                await parquet_output.write_dataframe(sample_dataframe)
+                await parquet_output.write(sample_dataframe)

             assert parquet_output.chunk_count == 1

@@ -232,7 +230,7 @@ async def test_write_dataframe_success(
             # We can verify this by checking the chunk count and that to_parquet was called

     @pytest.mark.asyncio
-    async def test_write_dataframe_with_custom_path_gen(
+    async def test_write_with_custom_path_gen(
         self, base_output_path: str, sample_dataframe: pd.DataFrame
     ):
         """Test DataFrame writing with custom path generation."""
@@ -241,18 +239,18 @@ async def test_write_dataframe_with_custom_path_gen(
         ) as mock_upload, patch(
             "pandas.DataFrame.to_parquet"
         ) as mock_to_parquet, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             mock_upload.return_value = AsyncMock()
             mock_prefix.return_value = "test/output/path"

-            parquet_output = ParquetOutput(
+            parquet_output = ParquetFileWriter(
                 output_path=base_output_path,
                 start_marker="test_start",
                 end_marker="test_end",
             )

-            await parquet_output.write_dataframe(sample_dataframe)
+            await parquet_output.write(sample_dataframe)

             # Check that to_parquet was called
             mock_to_parquet.assert_called()
@@ -265,44 +263,44 @@ async def test_write_dataframe_with_custom_path_gen(
             assert "chunk-" in call_args and ".parquet" in call_args

     @pytest.mark.asyncio
-    async def test_write_dataframe_error_handling(
+    async def test_write_error_handling(
         self, base_output_path: str, sample_dataframe: pd.DataFrame
     ):
         """Test error handling during DataFrame writing."""
         with patch("pandas.DataFrame.to_parquet") as mock_to_parquet:
             mock_to_parquet.side_effect = Exception("Test error")

-            parquet_output = ParquetOutput(output_path=base_output_path)
+            parquet_output = ParquetFileWriter(output_path=base_output_path)

             with pytest.raises(Exception, match="Test error"):
-                await parquet_output.write_dataframe(sample_dataframe)
+                await parquet_output.write(sample_dataframe)


-class TestParquetOutputWriteDaftDataframe:
-    """Test ParquetOutput daft DataFrame writing."""
+class TestParquetFileWriterWriteDaftDataframe:
+    """Test ParquetFileWriter daft DataFrame writing."""

     @pytest.mark.asyncio
-    async def test_write_daft_dataframe_empty(self, base_output_path: str):
+    async def test_write_empty(self, base_output_path: str):
         """Test writing an empty daft DataFrame."""
         with patch("daft.from_pydict") as mock_daft:
             mock_df = MagicMock()
             mock_df.count_rows.return_value = 0
             mock_daft.return_value = mock_df

-            parquet_output = ParquetOutput(output_path=base_output_path)
+            parquet_output = ParquetFileWriter(output_path=base_output_path)

-            await parquet_output.write_daft_dataframe(mock_df)
+            await parquet_output.write(mock_df)

             assert parquet_output.chunk_count == 0
             assert parquet_output.total_record_count == 0

     @pytest.mark.asyncio
-    async def test_write_daft_dataframe_success(self, base_output_path: str):
+    async def test_write_success(self, base_output_path: str):
         """Test successful daft DataFrame writing."""
         with patch("daft.execution_config_ctx") as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             mock_upload.return_value = AsyncMock()
             mock_prefix.return_value = "test/output/path"
@@ -316,11 +314,12 @@ async def test_write_daft_dataframe_success(self, base_output_path: str):
             mock_result.to_pydict.return_value = {"path": ["test.parquet"]}
             mock_df.write_parquet.return_value = mock_result

-            parquet_output = ParquetOutput(
+            parquet_output = ParquetFileWriter(
                 output_path=base_output_path,
+                dataframe_type=DataframeType.daft,
             )

-            await parquet_output.write_daft_dataframe(mock_df)
+            await parquet_output.write(mock_df)

             assert parquet_output.chunk_count == 1
             assert parquet_output.total_record_count == 1000
@@ -336,16 +335,14 @@ async def test_write_daft_dataframe_success(self, base_output_path: str):
             mock_upload.assert_called_once()

     @pytest.mark.asyncio
-    async def test_write_daft_dataframe_with_parameter_overrides(
-        self, base_output_path: str
-    ):
+    async def test_write_with_parameter_overrides(self, base_output_path: str):
         """Test daft DataFrame writing with parameter overrides."""
         with patch("daft.execution_config_ctx") as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
             "application_sdk.services.objectstore.ObjectStore.delete_prefix"
         ) as mock_delete, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             mock_upload.return_value = AsyncMock()
             mock_delete.return_value = AsyncMock()
@@ -360,12 +357,13 @@ async def test_write_daft_dataframe_with_parameter_overrides(
             mock_result.to_pydict.return_value = {"path": ["test.parquet"]}
             mock_df.write_parquet.return_value = mock_result

-            parquet_output = ParquetOutput(
+            parquet_output = ParquetFileWriter(
                 output_path=base_output_path,
+                dataframe_type=DataframeType.daft,
             )

             # Override parameters in method call
-            await parquet_output.write_daft_dataframe(
+            await parquet_output.write(
                 mock_df, partition_cols=["department", "year"], write_mode="overwrite"
             )

@@ -380,14 +378,12 @@ async def test_write_daft_dataframe_with_parameter_overrides(
             mock_delete.assert_called_once_with(prefix="test/output/path")

     @pytest.mark.asyncio
-    async def test_write_daft_dataframe_with_default_parameters(
-        self, base_output_path: str
-    ):
+    async def test_write_with_default_parameters(self, base_output_path: str):
         """Test daft DataFrame writing with default parameters (uses method default write_mode='append')."""
         with patch("daft.execution_config_ctx") as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             mock_upload.return_value = AsyncMock()
             mock_prefix.return_value = "test/output/path"
@@ -401,12 +397,13 @@ async def test_write_daft_dataframe_with_default_parameters(
             mock_result.to_pydict.return_value = {"path": ["test.parquet"]}
             mock_df.write_parquet.return_value = mock_result

-            parquet_output = ParquetOutput(
+            parquet_output = ParquetFileWriter(
                 output_path=base_output_path,
+                dataframe_type=DataframeType.daft,
             )

             # Use default parameters
-            await parquet_output.write_daft_dataframe(mock_df)
+            await parquet_output.write(mock_df)

             # Check that default method parameters were used
             mock_df.write_parquet.assert_called_once_with(
@@ -416,14 +413,12 @@ async def test_write_daft_dataframe_with_default_parameters(
             )

     @pytest.mark.asyncio
-    async def test_write_daft_dataframe_with_execution_configuration(
-        self, base_output_path: str
-    ):
+    async def test_write_with_execution_configuration(self, base_output_path: str):
         """Test that DAPR limit is properly configured."""
         with patch("daft.execution_config_ctx") as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             mock_upload.return_value = AsyncMock()
             mock_prefix.return_value = "test/output/path"
@@ -437,9 +432,12 @@ async def test_write_daft_dataframe_with_execution_configuration(
             mock_result.to_pydict.return_value = {"path": ["test.parquet"]}
             mock_df.write_parquet.return_value = mock_result

-            parquet_output = ParquetOutput(output_path=base_output_path)
+            parquet_output = ParquetFileWriter(
+                output_path=base_output_path,
+                dataframe_type=DataframeType.daft,
+            )

-            await parquet_output.write_daft_dataframe(mock_df)
+            await parquet_output.write(mock_df)

             # Check that execution context was called (don't check exact value since DAPR_MAX_GRPC_MESSAGE_LENGTH is imported)
             mock_ctx.assert_called_once()
@@ -451,20 +449,23 @@ async def test_write_daft_dataframe_with_execution_configuration(
             assert call_args.kwargs["default_morsel_size"] > 0

     @pytest.mark.asyncio
-    async def test_write_daft_dataframe_error_handling(self, base_output_path: str):
+    async def test_write_error_handling(self, base_output_path: str):
         """Test error handling during daft DataFrame writing."""
         # Test that count_rows error is properly handled
         mock_df = MagicMock()
         mock_df.count_rows.side_effect = Exception("Count rows error")

-        parquet_output = ParquetOutput(output_path=base_output_path)
+        parquet_output = ParquetFileWriter(
+            output_path=base_output_path,
+            dataframe_type=DataframeType.daft,
+        )

         with pytest.raises(Exception, match="Count rows error"):
-            await parquet_output.write_daft_dataframe(mock_df)
+            await parquet_output.write(mock_df)


-class TestParquetOutputMetrics:
-    """Test ParquetOutput metrics recording."""
+class TestParquetFileWriterMetrics:
+    """Test ParquetFileWriter metrics recording."""

     @pytest.mark.asyncio
     async def test_pandas_write_metrics(
@@ -474,18 +475,18 @@ async def test_pandas_write_metrics(
         with patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_metrics"
+            "application_sdk.io.parquet.get_metrics"
         ) as mock_get_metrics, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             mock_upload.return_value = AsyncMock()
             mock_prefix.return_value = "test/output/path"
             mock_metrics = MagicMock()
             mock_get_metrics.return_value = mock_metrics

-            parquet_output = ParquetOutput(output_path=base_output_path)
+            parquet_output = ParquetFileWriter(output_path=base_output_path)

-            await parquet_output.write_dataframe(sample_dataframe)
+            await parquet_output.write(sample_dataframe)

             # Check that record metrics were called
             assert (
@@ -498,9 +499,9 @@ async def test_daft_write_metrics(self, base_output_path: str):
         with patch("daft.execution_config_ctx") as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_metrics"
+            "application_sdk.io.parquet.get_metrics"
         ) as mock_get_metrics, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             mock_upload.return_value = AsyncMock()
             mock_prefix.return_value = "test/output/path"
@@ -516,9 +517,12 @@ async def test_daft_write_metrics(self, base_output_path: str):
             mock_result.to_pydict.return_value = {"path": ["test.parquet"]}
             mock_df.write_parquet.return_value = mock_result

-            parquet_output = ParquetOutput(output_path=base_output_path)
+            parquet_output = ParquetFileWriter(
+                output_path=base_output_path,
+                dataframe_type=DataframeType.daft,
+            )

-            await parquet_output.write_daft_dataframe(mock_df)
+            await parquet_output.write(mock_df)

             # Check that record metrics were called with correct labels
             assert (
@@ -533,12 +537,12 @@ async def test_daft_write_metrics(self, base_output_path: str):
                 assert labels["type"] == "daft"


-class TestParquetOutputConsolidation:
-    """Test ParquetOutput consolidation functionality."""
+class TestParquetFileWriterConsolidation:
+    """Test ParquetFileWriter consolidation functionality."""

     def test_consolidation_init_attributes(self, base_output_path: str):
         """Test that consolidation attributes are properly initialized."""
-        parquet_output = ParquetOutput(
+        parquet_output = ParquetFileWriter(
             output_path=base_output_path,
             chunk_size=1000,
             buffer_size=200,
@@ -555,7 +559,7 @@ def test_consolidation_init_attributes(self, base_output_path: str):

     def test_consolidation_init_with_none_chunk_size(self, base_output_path: str):
         """Test consolidation threshold when chunk_size is None."""
-        parquet_output = ParquetOutput(
+        parquet_output = ParquetFileWriter(
             output_path=base_output_path, chunk_size=None, buffer_size=200
         )

@@ -564,9 +568,8 @@ def test_consolidation_init_with_none_chunk_size(self, base_output_path: str):

     def test_temp_folder_path_generation(self, base_output_path: str):
         """Test temp folder path generation."""
-        parquet_output = ParquetOutput(
-            output_path=base_output_path,
-            output_suffix="test_suffix",
+        parquet_output = ParquetFileWriter(
+            output_path=os.path.join(base_output_path, "test_suffix"),
             typename="test_type",
         )

@@ -583,9 +586,8 @@ def test_temp_folder_path_generation(self, base_output_path: str):

     def test_consolidated_file_path_generation(self, base_output_path: str):
         """Test consolidated file path generation."""
-        parquet_output = ParquetOutput(
-            output_path=base_output_path,
-            output_suffix="test_suffix",
+        parquet_output = ParquetFileWriter(
+            output_path=os.path.join(base_output_path, "test_suffix"),
             typename="test_type",
         )

@@ -600,7 +602,7 @@ def test_consolidated_file_path_generation(self, base_output_path: str):

     def test_start_new_temp_folder(self, base_output_path: str):
         """Test starting a new temp folder."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        parquet_output = ParquetFileWriter(output_path=base_output_path)

         # Initially no temp folder
         assert parquet_output.current_temp_folder_path is None
@@ -629,7 +631,7 @@ async def test_write_chunk_to_temp_folder(
         self, base_output_path: str, sample_dataframe: pd.DataFrame
     ):
         """Test writing chunk to temp folder."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        parquet_output = ParquetFileWriter(output_path=base_output_path)

         # Start temp folder first
         parquet_output._start_new_temp_folder()
@@ -656,7 +658,7 @@ async def test_write_chunk_to_temp_folder_no_path(
         self, base_output_path: str, sample_dataframe: pd.DataFrame
     ):
         """Test writing chunk to temp folder when no path is set."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        parquet_output = ParquetFileWriter(output_path=base_output_path)

         # Should raise error when no temp folder path is set
         with pytest.raises(ValueError, match="No temp folder path available"):
@@ -672,7 +674,7 @@ async def test_consolidate_current_folder(
         ) as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             # Setup mocks
             mock_upload.return_value = AsyncMock()
@@ -684,7 +686,7 @@ async def test_consolidate_current_folder(
             mock_df = MagicMock()
             mock_read.return_value = mock_df

-            parquet_output = ParquetOutput(output_path=base_output_path)
+            parquet_output = ParquetFileWriter(output_path=base_output_path)
             parquet_output._start_new_temp_folder()
             parquet_output.current_folder_records = 500  # Simulate some records

@@ -718,7 +720,7 @@ async def test_consolidate_current_folder(
     @pytest.mark.asyncio
     async def test_consolidate_empty_folder(self, base_output_path: str):
         """Test consolidating when folder is empty."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        parquet_output = ParquetFileWriter(output_path=base_output_path)
         parquet_output.current_folder_records = 0
         parquet_output.current_temp_folder_path = None

@@ -731,7 +733,7 @@ async def test_consolidate_empty_folder(self, base_output_path: str):
     @pytest.mark.asyncio
     async def test_cleanup_temp_folders(self, base_output_path: str):
         """Test cleanup of temp folders."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+        parquet_output = ParquetFileWriter(output_path=base_output_path)

         # Create multiple temp folders
         parquet_output._start_new_temp_folder()
@@ -759,16 +761,16 @@ async def test_cleanup_temp_folders(self, base_output_path: str):
         assert parquet_output.current_folder_records == 0

     @pytest.mark.asyncio
-    async def test_write_batched_dataframe_with_consolidation(
+    async def test_write_batches_with_consolidation(
         self, base_output_path: str, mock_consolidation_files
     ):
-        """Test write_batched_dataframe with consolidation enabled."""
+        """Test write_batches with consolidation enabled."""
         with patch("daft.read_parquet") as mock_read, patch(
             "daft.execution_config_ctx"
         ) as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             # Setup mocks
             mock_upload.return_value = AsyncMock()
@@ -783,7 +785,7 @@ async def test_write_batched_dataframe_with_consolidation(
             mock_result.to_pydict.return_value = {"path": ["test_file.parquet"]}
             mock_df.write_parquet.return_value = mock_result

-            parquet_output = ParquetOutput(
+            parquet_output = ParquetFileWriter(
                 output_path=base_output_path,
                 chunk_size=500,  # Small threshold for testing
                 buffer_size=100,  # Small buffer for testing
@@ -808,7 +810,7 @@ def create_test_dataframes():
             ):
                 mock_df.write_parquet.return_value = create_mock_result(file_paths)

-                await parquet_output.write_batched_dataframe(create_test_dataframes())
+                await parquet_output.write_batches(create_test_dataframes())

                 # Should have triggered consolidation (600 records > 500 threshold)
                 assert parquet_output.total_record_count == 600
@@ -821,11 +823,9 @@ def create_test_dataframes():
                 assert not os.path.exists(temp_base) or len(os.listdir(temp_base)) == 0

     @pytest.mark.asyncio
-    async def test_write_batched_dataframe_without_consolidation(
-        self, base_output_path: str
-    ):
-        """Test write_batched_dataframe with consolidation disabled."""
-        parquet_output = ParquetOutput(output_path=base_output_path)
+    async def test_write_batches_without_consolidation(self, base_output_path: str):
+        """Test write_batches with consolidation disabled."""
+        parquet_output = ParquetFileWriter(output_path=base_output_path)
         parquet_output.use_consolidation = False

         def create_test_dataframes():
@@ -833,12 +833,10 @@ def create_test_dataframes():
             yield df

         # Mock the super() call to avoid actual file operations
-        with patch(
-            "application_sdk.outputs.Output.write_batched_dataframe"
-        ) as mock_base_method:
+        with patch("application_sdk.io.Writer.write_batches") as mock_base_method:
             mock_base_method.return_value = AsyncMock()

-            await parquet_output.write_batched_dataframe(create_test_dataframes())
+            await parquet_output.write_batches(create_test_dataframes())

             # Should have called base class method
             mock_base_method.assert_called_once()
@@ -846,7 +844,7 @@ def create_test_dataframes():
     @pytest.mark.asyncio
     async def test_accumulate_dataframe(self, base_output_path: str):
         """Test accumulating DataFrame into temp folders."""
-        parquet_output = ParquetOutput(
+        parquet_output = ParquetFileWriter(
             output_path=base_output_path,
             chunk_size=500,  # This sets consolidation_threshold internally
             buffer_size=100,
@@ -867,10 +865,10 @@ async def test_accumulate_dataframe(self, base_output_path: str):
             "_start_new_temp_folder",
             wraps=parquet_output._start_new_temp_folder,
         ) as mock_start_folder, patch.object(
-            parquet_output, "write_chunk"
-        ) as mock_write_chunk:
+            parquet_output, "_write_chunk"
+        ) as mock__write_chunk:
             mock_consolidate.return_value = AsyncMock()
-            mock_write_chunk.return_value = AsyncMock()
+            mock__write_chunk.return_value = AsyncMock()

             await parquet_output._accumulate_dataframe(large_df)

@@ -881,7 +879,7 @@ async def test_accumulate_dataframe(self, base_output_path: str):
     @pytest.mark.asyncio
     async def test_consolidation_error_handling(self, base_output_path: str):
         """Test error handling in consolidation with cleanup."""
-        parquet_output = ParquetOutput(
+        parquet_output = ParquetFileWriter(
             output_path=base_output_path, use_consolidation=True
         )

@@ -900,14 +898,14 @@ def create_test_dataframes():

             # Should raise the exception and call cleanup
             with pytest.raises(Exception, match="Test error"):
-                await parquet_output.write_batched_dataframe(create_test_dataframes())
+                await parquet_output.write_batches(create_test_dataframes())

             mock_cleanup.assert_called_once()

     @pytest.mark.asyncio
     async def test_async_generator_support(self, base_output_path: str):
         """Test that consolidation works with async generators."""
-        parquet_output = ParquetOutput(
+        parquet_output = ParquetFileWriter(
             output_path=base_output_path, use_consolidation=True
         )

@@ -929,7 +927,7 @@ async def create_async_dataframes():
             mock_accumulate.return_value = AsyncMock()
             mock_cleanup.return_value = AsyncMock()

-            await parquet_output.write_batched_dataframe(create_async_dataframes())
+            await parquet_output.write_batches(create_async_dataframes())

             # Should have called accumulate for each DataFrame
             assert mock_accumulate.call_count == 2
@@ -939,10 +937,10 @@ async def create_async_dataframes():
     async def test_multiple_write_batched_calls_with_consolidation(
         self, base_output_path: str
     ):
-        """Test multiple calls to write_batched_dataframe with consolidation enabled.
+        """Test multiple calls to write_batches with consolidation enabled.

         This test verifies that:
-        1. Multiple calls to write_batched_dataframe work correctly
+        1. Multiple calls to write_batches work correctly
         2. Each call generates separate consolidated files
         3. Chunk counts accumulate across calls
         4. Low thresholds trigger multiple consolidations within a single call
@@ -952,7 +950,7 @@ async def test_multiple_write_batched_calls_with_consolidation(
         ) as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             # Setup mocks
             mock_upload.return_value = AsyncMock()
@@ -985,8 +983,8 @@ def mock_write_parquet(*args, **kwargs):

             mock_df.write_parquet.side_effect = mock_write_parquet

-            # Create ParquetOutput with very low thresholds to trigger multiple consolidations
-            parquet_output = ParquetOutput(
+            # Create ParquetFileWriter with very low thresholds to trigger multiple consolidations
+            parquet_output = ParquetFileWriter(
                 output_path=base_output_path,
                 chunk_size=100,  # Very small consolidation threshold
                 buffer_size=50,  # Very small buffer size
@@ -1009,8 +1007,8 @@ def create_first_batch():
             # Files are now created by the mock_write_parquet function

             try:
-                # First call to write_batched_dataframe
-                await parquet_output.write_batched_dataframe(create_first_batch())
+                # First call to write_batches
+                await parquet_output.write_batches(create_first_batch())

                 # Verify first call results
                 first_call_total = parquet_output.total_record_count
@@ -1035,8 +1033,8 @@ def create_second_batch():
                         )
                         yield df

-                # Second call to write_batched_dataframe on the same instance
-                await parquet_output.write_batched_dataframe(create_second_batch())
+                # Second call to write_batches on the same instance
+                await parquet_output.write_batches(create_second_batch())

                 # Verify accumulated results across both calls
                 total_records = parquet_output.total_record_count
@@ -1083,7 +1081,7 @@ async def test_consolidation_with_very_small_buffer_multiple_chunks(
         ) as mock_ctx, patch(
             "application_sdk.services.objectstore.ObjectStore.upload_file"
         ) as mock_upload, patch(
-            "application_sdk.outputs.parquet.get_object_store_prefix"
+            "application_sdk.io.parquet.get_object_store_prefix"
         ) as mock_prefix:
             # Setup mocks
             mock_upload.return_value = AsyncMock()
@@ -1101,7 +1099,7 @@ async def test_consolidation_with_very_small_buffer_multiple_chunks(

                 # Extreme settings: buffer_size=10, consolidation_threshold=200
                 # This should create many small chunk files before consolidation
-                parquet_output = ParquetOutput(
+                parquet_output = ParquetFileWriter(
                     output_path=base_output_path,
                     chunk_size=200,  # consolidation_threshold
                     buffer_size=10,  # Very small buffer - each dataframe chunk becomes a file
@@ -1124,7 +1122,7 @@ def create_large_batch():
                     yield df

                 # Files are created by the fixture automatically
-                await parquet_output.write_batched_dataframe(create_large_batch())
+                await parquet_output.write_batches(create_large_batch())

                 # Verify results
                 assert parquet_output.total_record_count == 250
diff --git a/tests/unit/outputs/test_output.py b/tests/unit/outputs/test_output.py
deleted file mode 100644
index bea3fa4ef..000000000
--- a/tests/unit/outputs/test_output.py
+++ /dev/null
@@ -1,173 +0,0 @@
-"""Unit tests for output interface."""
-
-import os
-from typing import Any
-from unittest.mock import AsyncMock, mock_open, patch
-
-import pandas as pd
-import pytest
-
-from application_sdk.common.dataframe_utils import is_empty_dataframe
-from application_sdk.outputs import Output
-
-
-def test_is_empty_dataframe_pandas():
-    """Test is_empty_dataframe with pandas DataFrame."""
-    # Test with empty DataFrame
-    empty_df = pd.DataFrame()
-    assert is_empty_dataframe(empty_df) is True
-
-    # Test with non-empty DataFrame
-    non_empty_df = pd.DataFrame({"col": [1, 2, 3]})
-    assert is_empty_dataframe(non_empty_df) is False
-
-
-@pytest.mark.asyncio
-class TestOutput:
-    class ConcreteOutput(Output):
-        """Concrete implementation of Output for testing."""
-
-        def __init__(self, output_path: str, output_prefix: str):
-            self.output_path = output_path
-            self.output_prefix = output_prefix
-            self.total_record_count = 0
-            self.chunk_count = 0
-            self.partitions = []  # Initialize partitions attribute
-            self.buffer_size = 5000
-            self.max_file_size_bytes = 1024 * 1024 * 10  # 10MB
-            self.current_buffer_size = 0
-            self.current_buffer_size_bytes = 0
-
-        async def write_dataframe(self, dataframe: pd.DataFrame):
-            """Implement abstract method."""
-            self.total_record_count += len(dataframe)
-            self.chunk_count += 1
-
-        async def write_daft_dataframe(self, dataframe: Any):  # type: ignore
-            """Implement abstract method."""
-            self.total_record_count += 1  # Mock implementation
-            self.chunk_count += 1
-
-    def setup_method(self):
-        """Set up test fixtures."""
-        self.output = self.ConcreteOutput("/test/path", "/test/prefix")
-
-    @pytest.mark.asyncio
-    async def test_write_batched_dataframe_sync(self):
-        """Test write_batched_dataframe with sync generator."""
-
-        def generate_dataframes():
-            yield pd.DataFrame({"col": [1, 2]})
-            yield pd.DataFrame({"col": [3, 4]})
-
-        await self.output.write_batched_dataframe(generate_dataframes())
-        assert self.output.total_record_count == 4
-        assert self.output.chunk_count == 2
-
-    @pytest.mark.asyncio
-    async def test_write_batched_dataframe_async(self):
-        """Test write_batched_dataframe with async generator."""
-
-        async def generate_dataframes():
-            yield pd.DataFrame({"col": [1, 2]})
-            yield pd.DataFrame({"col": [3, 4]})
-
-        await self.output.write_batched_dataframe(generate_dataframes())
-        assert self.output.total_record_count == 4
-        assert self.output.chunk_count == 2
-
-    @pytest.mark.asyncio
-    async def test_write_batched_dataframe_empty(self):
-        """Test write_batched_dataframe with empty DataFrame."""
-
-        def generate_empty_dataframes():
-            yield pd.DataFrame()
-
-        await self.output.write_batched_dataframe(generate_empty_dataframes())
-        assert self.output.total_record_count == 0
-        assert self.output.chunk_count == 0
-
-    @pytest.mark.asyncio
-    async def test_write_batched_dataframe_error(self):
-        """Test write_batched_dataframe error handling."""
-
-        def generate_error():
-            yield pd.DataFrame({"col": [1]})
-            raise Exception("Test error")
-
-        with patch("application_sdk.outputs.logger.error") as mock_logger:
-            # According to workspace rules, exceptions should be re-raised after logging
-            with pytest.raises(Exception, match="Test error"):
-                await self.output.write_batched_dataframe(generate_error())
-            mock_logger.assert_called_once()
-            assert "Error writing batched dataframe" in mock_logger.call_args[0][0]
-
-    @pytest.mark.asyncio
-    async def test_get_statistics_error(self):
-        """Test get_statistics error handling."""
-        with patch.object(self.output, "write_statistics") as mock_write:
-            mock_write.side_effect = Exception("Test error")
-            with pytest.raises(Exception):
-                await self.output.get_statistics()
-
-    @pytest.mark.asyncio
-    async def test_write_statistics_success(self):
-        """Test write_statistics successful case."""
-        self.output.total_record_count = 100
-        self.output.chunk_count = 5
-        self.output.partitions = [1, 2, 1, 2, 1]
-
-        # Mock the open function, orjson.dumps, os.makedirs, and object store upload
-        with patch("builtins.open", mock_open()) as mock_file, patch(
-            "orjson.dumps",
-            return_value=b'{"total_record_count": 100, "chunk_count": 5, "partitions": [1,2,1,2,1]}',
-        ) as mock_orjson, patch(
-            "application_sdk.outputs.os.makedirs",
-        ) as mock_makedirs, patch(
-            "application_sdk.outputs.get_object_store_prefix",
-            return_value="path/statistics/statistics.json.ignore",
-        ), patch(
-            "application_sdk.services.objectstore.ObjectStore.upload_file",
-            new_callable=AsyncMock,
-        ) as mock_push:
-            # Call the method
-            stats = await self.output.write_statistics()
-
-            # Assertions
-            assert stats == {
-                "total_record_count": 100,
-                "chunk_count": 5,  # This is len(self.partitions) which is 5
-                "partitions": [1, 2, 1, 2, 1],
-            }
-            expected_stats_dir = os.path.join("/test/path", "statistics")
-            mock_makedirs.assert_called_once_with(expected_stats_dir, exist_ok=True)
-            expected_file_path = os.path.join(
-                expected_stats_dir, "statistics.json.ignore"
-            )
-            mock_file.assert_called_once_with(expected_file_path, "wb")
-            mock_orjson.assert_called_once_with(
-                {
-                    "total_record_count": 100,
-                    "chunk_count": 5,
-                    "partitions": [1, 2, 1, 2, 1],
-                }
-            )
-            # Verify the upload call
-            mock_push.assert_awaited_once()
-            upload_kwargs = mock_push.await_args.kwargs  # type: ignore[attr-defined]
-            assert upload_kwargs["source"] == expected_file_path
-            assert (
-                upload_kwargs["destination"] == "path/statistics/statistics.json.ignore"
-            )
-
-    @pytest.mark.asyncio
-    async def test_write_statistics_error(self):
-        """Test write_statistics error handling."""
-        with patch("pandas.DataFrame.to_json") as mock_to_json:
-            mock_to_json.side_effect = Exception("Test error")
-
-            with patch("application_sdk.outputs.logger.error") as mock_logger:
-                result = await self.output.write_statistics()
-                assert result is None
-                mock_logger.assert_called_once()
-                assert "Error writing statistics" in mock_logger.call_args[0][0]
diff --git a/tests/unit/services/test_eventstore.py b/tests/unit/services/test_eventstore.py
index d2853800e..6b5fc7d6b 100644
--- a/tests/unit/services/test_eventstore.py
+++ b/tests/unit/services/test_eventstore.py
@@ -5,7 +5,7 @@
 import pytest

 from application_sdk.constants import EVENT_STORE_NAME
-from application_sdk.events.models import (
+from application_sdk.interceptors.models import (
     ApplicationEventNames,
     Event,
     EventMetadata,
